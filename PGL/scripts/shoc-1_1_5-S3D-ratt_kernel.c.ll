	  %a = alloca [16384 x float], align 16
	  %b = alloca [16384 x float], align 16
	  %c = alloca float, align 4
	  %1 = bitcast [16384 x float]* %a to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* bitcast ([16384 x float]* @main.a to i8*), i64 65536, i32 16, i1 false)
	  %4 = bitcast [16384 x float]* %b to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %4, i8* bitcast ([16384 x float]* @main.b to i8*), i64 65536, i32 16, i1 false)
	  %9 = load float, float* %c, align 4
	  %8 = getelementptr inbounds [16384 x float], [16384 x float]* %b, i32 0, i32 0
	  %7 = getelementptr inbounds [16384 x float], [16384 x float]* %a, i32 0, i32 0
	store float* %7, float** %a, align 8
	store  float* %8, float** %b, align 8
	store  float %9, float* %c, align 8
	  store float 1.000000e+00, float* %c, align 4
	  call void @A(float* %7, float* %8, float %9)
	  %11 = load float, float* %d, align 4
	  %9 = load float, float* %3, align 4
	  %8 = load float, float* %7, align 4
	  %7 = getelementptr inbounds float, float* %6, i64 %5
	  %6 = load float*, float** %1, align 8
	  %4 = load i32, i32* %z, align 4
	  %1 = alloca float*, align 8
	  %2 = alloca float*, align 8
	  %3 = alloca float, align 4
	  %z = alloca i32, align 4
	  %d = alloca float, align 4
	  %e = alloca float, align 4
	  %f = alloca float, align 4
	  %g = alloca float, align 4
	  %h = alloca float, align 4
	  store float* %a, float** %1, align 8
	  store float* %b, float** %2, align 8
	  store float %c, float* %3, align 4
	  store i32 0, i32* %z, align 4
	  %5 = sext i32 %4 to i64
	  %10 = fmul float %8, %9
	  store float %10, float* %d, align 4
	  %12 = fpext float %11 to double
	  %14 = call double @log(double %12) #3
	  %24 = load float, float* %f, align 4
	  %22 = load float, float* %f, align 4
	  %21 = load float, float* %f, align 4
	  %17 = load float, float* %d, align 4
	  %16 = fptrunc double %14 to float
	  store float %16, float* %e, align 4
	  %18 = fpext float %17 to double
	  %19 = fdiv double 1.000000e+00, %18
	  %20 = fptrunc double %19 to float
	  store float %20, float* %f, align 4
	  %23 = fmul float %21, %22
	  store float %23, float* %g, align 4
	  %25 = fmul float 0x40BC54DCA0000000, %24
	  %26 = fsub float 0x40400661E0000000, %25
	  %27 = fpext float %26 to double
	  %29 = call double @exp(double %27) #4
	  %41 = load float, float* %f, align 4
	  %38 = load float, float* %e, align 4
	  %37 = getelementptr inbounds float, float* %36, i64 %35
	  %36 = load float*, float** %2, align 8
	  %32 = load i32, i32* %z, align 4
	  %31 = fptrunc double %29 to float
	  %33 = add nsw i32 0, %32
	  %34 = srem i32 %33, 128
	  %35 = sext i32 %34 to i64
	  store float %31, float* %37, align 4
	  %39 = fmul float 0x40055C2900000000, %38
	  %40 = fadd float 0x4025A3BA00000000, %39
	  %42 = fmul float 0x40A8BA7740000000, %41
	  %43 = fsub float %40, %42
	  %44 = fpext float %43 to double
	  %46 = call double @exp(double %44) #4
	  %58 = load float, float* %f, align 4
	  %55 = load float, float* %e, align 4
	  %54 = getelementptr inbounds float, float* %53, i64 %52
	  %53 = load float*, float** %2, align 8
	  %49 = load i32, i32* %z, align 4
	  %48 = fptrunc double %46 to float
	  %50 = add nsw i32 8, %49
	  %51 = srem i32 %50, 128
	  %52 = sext i32 %51 to i64
	  store float %48, float* %54, align 4
	  %56 = fmul float 0x3FF828F5C0000000, %55
	  %57 = fadd float 0x403330D780000000, %56
	  %59 = fmul float 0x409AF82200000000, %58
	  %60 = fsub float %57, %59
	  %61 = fpext float %60 to double
	  %63 = call double @exp(double %61) #4
	  %75 = load float, float* %f, align 4
	  %72 = load float, float* %e, align 4
	  %71 = getelementptr inbounds float, float* %70, i64 %69
	  %70 = load float*, float** %2, align 8
	  %66 = load i32, i32* %z, align 4
	  %65 = fptrunc double %63 to float
	  %67 = add nsw i32 16, %66
	  %68 = srem i32 %67, 128
	  %69 = sext i32 %68 to i64
	  store float %65, float* %71, align 4
	  %73 = fmul float 0x4003333340000000, %72
	  %74 = fadd float 0x4024F73F80000000, %73
	  %76 = fmul float 0x4090972600000000, %75
	  %77 = fadd float %74, %76
	  %78 = fpext float %77 to double
	  %80 = call double @exp(double %78) #4
	  %99 = load float, float* %e, align 4
	  %98 = getelementptr inbounds float, float* %97, i64 %96
	  %97 = load float*, float** %2, align 8
	  %93 = load i32, i32* %z, align 4
	  %89 = load float, float* %f, align 4
	  %88 = getelementptr inbounds float, float* %87, i64 %86
	  %87 = load float*, float** %2, align 8
	  %83 = load i32, i32* %z, align 4
	  %82 = fptrunc double %80 to float
	  %84 = add nsw i32 24, %83
	  %85 = srem i32 %84, 128
	  %86 = sext i32 %85 to i64
	  store float %82, float* %88, align 4
	  %90 = fpext float %89 to double
	  %91 = fmul double 1.000000e+18, %90
	  %92 = fptrunc double %91 to float
	  %94 = add nsw i32 32, %93
	  %95 = srem i32 %94, 128
	  %96 = sext i32 %95 to i64
	  store float %92, float* %98, align 4
	  %100 = fmul float 0x3FE3333340000000, %99
	  %101 = fsub float 0x404384F060000000, %100
	  %102 = fpext float %101 to double
	  %104 = call double @exp(double %102) #4
	  %113 = load float, float* %e, align 4
	  %112 = getelementptr inbounds float, float* %111, i64 %110
	  %111 = load float*, float** %2, align 8
	  %107 = load i32, i32* %z, align 4
	  %106 = fptrunc double %104 to float
	  %108 = add nsw i32 40, %107
	  %109 = srem i32 %108, 128
	  %110 = sext i32 %109 to i64
	  store float %106, float* %112, align 4
	  %114 = fmul float 1.250000e+00, %113
	  %115 = fsub float 0x4046C53B60000000, %114
	  %116 = fpext float %115 to double
	  %118 = call double @exp(double %116) #4
	  %167 = load float, float* %e, align 4
	  %166 = getelementptr inbounds float, float* %165, i64 %164
	  %165 = load float*, float** %2, align 8
	  %161 = load i32, i32* %z, align 4
	  %157 = load float, float* %f, align 4
	  %156 = getelementptr inbounds float, float* %155, i64 %154
	  %155 = load float*, float** %2, align 8
	  %151 = load i32, i32* %z, align 4
	  %147 = load float, float* %f, align 4
	  %146 = getelementptr inbounds float, float* %145, i64 %144
	  %145 = load float*, float** %2, align 8
	  %141 = load i32, i32* %z, align 4
	  %137 = load float, float* %g, align 4
	  %136 = getelementptr inbounds float, float* %135, i64 %134
	  %135 = load float*, float** %2, align 8
	  %131 = load i32, i32* %z, align 4
	  %127 = load float, float* %g, align 4
	  %126 = getelementptr inbounds float, float* %125, i64 %124
	  %125 = load float*, float** %2, align 8
	  %121 = load i32, i32* %z, align 4
	  %120 = fptrunc double %118 to float
	  %122 = add nsw i32 48, %121
	  %123 = srem i32 %122, 128
	  %124 = sext i32 %123 to i64
	  store float %120, float* %126, align 4
	  %128 = fpext float %127 to double
	  %129 = fmul double 5.500000e+20, %128
	  %130 = fptrunc double %129 to float
	  %132 = add nsw i32 56, %131
	  %133 = srem i32 %132, 128
	  %134 = sext i32 %133 to i64
	  store float %130, float* %136, align 4
	  %138 = fpext float %137 to double
	  %139 = fmul double 2.200000e+22, %138
	  %140 = fptrunc double %139 to float
	  %142 = add nsw i32 64, %141
	  %143 = srem i32 %142, 128
	  %144 = sext i32 %143 to i64
	  store float %140, float* %146, align 4
	  %148 = fpext float %147 to double
	  %149 = fmul double 5.000000e+17, %148
	  %150 = fptrunc double %149 to float
	  %152 = add nsw i32 72, %151
	  %153 = srem i32 %152, 128
	  %154 = sext i32 %153 to i64
	  store float %150, float* %156, align 4
	  %158 = fpext float %157 to double
	  %159 = fmul double 1.200000e+17, %158
	  %160 = fptrunc double %159 to float
	  %162 = add nsw i32 80, %161
	  %163 = srem i32 %162, 128
	  %164 = sext i32 %163 to i64
	  store float %160, float* %166, align 4
	  %168 = fmul float 0x3FEB851EC0000000, %167
	  %169 = fsub float 0x40453CF280000000, %168
	  %170 = fpext float %169 to double
	  %172 = call double @exp(double %170) #4
	  %181 = load float, float* %e, align 4
	  %180 = getelementptr inbounds float, float* %179, i64 %178
	  %179 = load float*, float** %2, align 8
	  %175 = load i32, i32* %z, align 4
	  %174 = fptrunc double %172 to float
	  %176 = add nsw i32 88, %175
	  %177 = srem i32 %176, 128
	  %178 = sext i32 %177 to i64
	  store float %174, float* %180, align 4
	  %182 = fmul float 0x3FFB851EC0000000, %181
	  %183 = fsub float 0x4047933D80000000, %182
	  %184 = fpext float %183 to double
	  %186 = call double @exp(double %184) #4
	  %195 = load float, float* %e, align 4
	  %194 = getelementptr inbounds float, float* %193, i64 %192
	  %193 = load float*, float** %2, align 8
	  %189 = load i32, i32* %z, align 4
	  %188 = fptrunc double %186 to float
	  %190 = add nsw i32 96, %189
	  %191 = srem i32 %190, 128
	  %192 = sext i32 %191 to i64
	  store float %188, float* %194, align 4
	  %196 = fmul float 0x3FE851EB80000000, %195
	  %197 = fsub float 0x4046202420000000, %196
	  %198 = fpext float %197 to double
	  %200 = call double @exp(double %198) #4
	  %209 = load float, float* %e, align 4
	  %208 = getelementptr inbounds float, float* %207, i64 %206
	  %207 = load float*, float** %2, align 8
	  %203 = load i32, i32* %z, align 4
	  %202 = fptrunc double %200 to float
	  %204 = add nsw i32 104, %203
	  %205 = srem i32 %204, 128
	  %206 = sext i32 %205 to i64
	  store float %202, float* %208, align 4
	  %210 = fmul float 0x3FF3D70A40000000, %209
	  %211 = fsub float 0x40465A3140000000, %210
	  %212 = fpext float %211 to double
	  %214 = call double @exp(double %212) #4
	  %223 = load float, float* %e, align 4
	  %222 = getelementptr inbounds float, float* %221, i64 %220
	  %221 = load float*, float** %2, align 8
	  %217 = load i32, i32* %z, align 4
	  %216 = fptrunc double %214 to float
	  %218 = add nsw i32 112, %217
	  %219 = srem i32 %218, 128
	  %220 = sext i32 %219 to i64
	  store float %216, float* %222, align 4
	  %224 = fmul float 0x3FD7AE1480000000, %223
	  %225 = fsub float 0x403FEF61C0000000, %224
	  %226 = fpext float %225 to double
	  %228 = call double @exp(double %226) #4
	  %237 = load float, float* %f, align 4
	  %236 = getelementptr inbounds float, float* %235, i64 %234
	  %235 = load float*, float** %2, align 8
	  %231 = load i32, i32* %z, align 4
	  %230 = fptrunc double %228 to float
	  %232 = add nsw i32 120, %231
	  %233 = srem i32 %232, 128
	  %234 = sext i32 %233 to i64
	  store float %230, float* %236, align 4
	  %238 = fmul float 0x40751A88C0000000, %237
	  %239 = fsub float 0x403D028160000000, %238
	  %240 = fpext float %239 to double
	  %242 = call double @exp(double %240) #4
	  %251 = load float, float* %f, align 4
	  %250 = getelementptr inbounds float, float* %249, i64 %248
	  %249 = load float*, float** %2, align 8
	  %245 = load i32, i32* %z, align 4
	  %244 = fptrunc double %242 to float
	  %246 = add nsw i32 128, %245
	  %247 = srem i32 %246, 128
	  %248 = sext i32 %247 to i64
	  store float %244, float* %250, align 4
	  %252 = fmul float 0x4079CA33E0000000, %251
	  %253 = fsub float 0x403E70BFA0000000, %252
	  %254 = fpext float %253 to double
	  %256 = call double @exp(double %254) #4
	  %265 = load float, float* %f, align 4
	  %264 = getelementptr inbounds float, float* %263, i64 %262
	  %263 = load float*, float** %2, align 8
	  %259 = load i32, i32* %z, align 4
	  %258 = fptrunc double %256 to float
	  %260 = add nsw i32 136, %259
	  %261 = srem i32 %260, 128
	  %262 = sext i32 %261 to i64
	  store float %258, float* %264, align 4
	  %266 = fmul float 0x4062DEE140000000, %265
	  %267 = fsub float 0x403FE410C0000000, %266
	  %268 = fpext float %267 to double
	  %270 = call double @exp(double %268) #4
	  %285 = load float, float* %f, align 4
	  %284 = getelementptr inbounds float, float* %283, i64 %282
	  %283 = load float*, float** %2, align 8
	  %279 = load i32, i32* %z, align 4
	  %278 = getelementptr inbounds float, float* %277, i64 %276
	  %277 = load float*, float** %2, align 8
	  %273 = load i32, i32* %z, align 4
	  %272 = fptrunc double %270 to float
	  %274 = add nsw i32 144, %273
	  %275 = srem i32 %274, 128
	  %276 = sext i32 %275 to i64
	  store float %272, float* %278, align 4
	  %280 = add nsw i32 152, %279
	  %281 = srem i32 %280, 128
	  %282 = sext i32 %281 to i64
	  store float 0x42B2309CE0000000, float* %284, align 4
	  %286 = fmul float 0x406F737780000000, %285
	  %287 = fadd float 0x403F77E3E0000000, %286
	  %288 = fpext float %287 to double
	  %290 = call double @exp(double %288) #4
	  %299 = load float, float* %f, align 4
	  %298 = getelementptr inbounds float, float* %297, i64 %296
	  %297 = load float*, float** %2, align 8
	  %293 = load i32, i32* %z, align 4
	  %292 = fptrunc double %290 to float
	  %294 = add nsw i32 160, %293
	  %295 = srem i32 %294, 128
	  %296 = sext i32 %295 to i64
	  store float %292, float* %298, align 4
	  %300 = fmul float 0x4089A1F200000000, %299
	  %301 = fadd float 0x4039973EC0000000, %300
	  %302 = fpext float %301 to double
	  %304 = call double @exp(double %302) #4
	  %313 = load float, float* %f, align 4
	  %312 = getelementptr inbounds float, float* %311, i64 %310
	  %311 = load float*, float** %2, align 8
	  %307 = load i32, i32* %z, align 4
	  %306 = fptrunc double %304 to float
	  %308 = add nsw i32 168, %307
	  %309 = srem i32 %308, 128
	  %310 = sext i32 %309 to i64
	  store float %306, float* %312, align 4
	  %314 = fmul float 0x40B79699A0000000, %313
	  %315 = fsub float 0x4040D5EC60000000, %314
	  %316 = fpext float %315 to double
	  %318 = call double @exp(double %316) #4
	  %330 = load float, float* %f, align 4
	  %327 = load float, float* %e, align 4
	  %326 = getelementptr inbounds float, float* %325, i64 %324
	  %325 = load float*, float** %2, align 8
	  %321 = load i32, i32* %z, align 4
	  %320 = fptrunc double %318 to float
	  %322 = add nsw i32 176, %321
	  %323 = srem i32 %322, 128
	  %324 = sext i32 %323 to i64
	  store float %320, float* %326, align 4
	  %328 = fmul float 2.000000e+00, %327
	  %329 = fadd float 0x40304F0800000000, %328
	  %331 = fmul float 0x40A4717400000000, %330
	  %332 = fsub float %329, %331
	  %333 = fpext float %332 to double
	  %335 = call double @exp(double %333) #4
	  %344 = load float, float* %f, align 4
	  %343 = getelementptr inbounds float, float* %342, i64 %341
	  %342 = load float*, float** %2, align 8
	  %338 = load i32, i32* %z, align 4
	  %337 = fptrunc double %335 to float
	  %339 = add nsw i32 184, %338
	  %340 = srem i32 %339, 128
	  %341 = sext i32 %340 to i64
	  store float %337, float* %343, align 4
	  %345 = fmul float 0x409C4E51E0000000, %344
	  %346 = fsub float 0x403DEF00E0000000, %345
	  %347 = fpext float %346 to double
	  %349 = call double @exp(double %347) #4
	  %361 = load float, float* %f, align 4
	  %358 = load float, float* %e, align 4
	  %357 = getelementptr inbounds float, float* %356, i64 %355
	  %356 = load float*, float** %2, align 8
	  %352 = load i32, i32* %z, align 4
	  %351 = fptrunc double %349 to float
	  %353 = add nsw i32 192, %352
	  %354 = srem i32 %353, 128
	  %355 = sext i32 %354 to i64
	  store float %351, float* %357, align 4
	  %359 = fmul float 2.000000e+00, %358
	  %360 = fadd float 0x40301494C0000000, %359
	  %362 = fmul float 0x409F737780000000, %361
	  %363 = fsub float %360, %362
	  %364 = fpext float %363 to double
	  %366 = call double @exp(double %364) #4
	  %375 = load float, float* %f, align 4
	  %374 = getelementptr inbounds float, float* %373, i64 %372
	  %373 = load float*, float** %2, align 8
	  %369 = load i32, i32* %z, align 4
	  %368 = fptrunc double %366 to float
	  %370 = add nsw i32 200, %369
	  %371 = srem i32 %370, 128
	  %372 = sext i32 %371 to i64
	  store float %368, float* %374, align 4
	  %376 = fmul float 0x406420F040000000, %375
	  %377 = fsub float 0x403C30CDA0000000, %376
	  %378 = fpext float %377 to double
	  %380 = call double @exp(double %378) #4
	  %389 = load float, float* %f, align 4
	  %388 = getelementptr inbounds float, float* %387, i64 %386
	  %387 = load float*, float** %2, align 8
	  %383 = load i32, i32* %z, align 4
	  %382 = fptrunc double %380 to float
	  %384 = add nsw i32 208, %383
	  %385 = srem i32 %384, 128
	  %386 = sext i32 %385 to i64
	  store float %382, float* %388, align 4
	  %390 = fmul float 0x40B2CAC060000000, %389
	  %391 = fsub float 0x4040FF3D00000000, %390
	  %392 = fpext float %391 to double
	  %394 = call double @exp(double %392) #4
	  %403 = load float, float* %f, align 4
	  %402 = getelementptr inbounds float, float* %401, i64 %400
	  %401 = load float*, float** %2, align 8
	  %397 = load i32, i32* %z, align 4
	  %396 = fptrunc double %394 to float
	  %398 = add nsw i32 216, %397
	  %399 = srem i32 %398, 128
	  %400 = sext i32 %399 to i64
	  store float %396, float* %402, align 4
	  %404 = fmul float 0x40979699A0000000, %403
	  %405 = fsub float 0x40410400E0000000, %404
	  %406 = fpext float %405 to double
	  %408 = call double @exp(double %406) #4
	  %420 = load float, float* %f, align 4
	  %417 = load float, float* %e, align 4
	  %416 = getelementptr inbounds float, float* %415, i64 %414
	  %415 = load float*, float** %2, align 8
	  %411 = load i32, i32* %z, align 4
	  %410 = fptrunc double %408 to float
	  %412 = add nsw i32 224, %411
	  %413 = srem i32 %412, 128
	  %414 = sext i32 %413 to i64
	  store float %410, float* %416, align 4
	  %418 = fmul float 0x3FF3A5E360000000, %417
	  %419 = fadd float 0x4031ADA7E0000000, %418
	  %421 = fmul float 0x40419CD240000000, %420
	  %422 = fsub float %419, %421
	  %423 = fpext float %422 to double
	  %425 = call double @exp(double %423) #4
	  %437 = load float, float* %f, align 4
	  %434 = load float, float* %e, align 4
	  %433 = getelementptr inbounds float, float* %432, i64 %431
	  %432 = load float*, float** %2, align 8
	  %428 = load i32, i32* %z, align 4
	  %427 = fptrunc double %425 to float
	  %429 = add nsw i32 232, %428
	  %430 = srem i32 %429, 128
	  %431 = sext i32 %430 to i64
	  store float %427, float* %433, align 4
	  %435 = fmul float 1.500000e+00, %434
	  %436 = fadd float 0x403193A340000000, %435
	  %438 = fmul float 0x40E38F0180000000, %437
	  %439 = fsub float %436, %438
	  %440 = fpext float %439 to double
	  %442 = call double @exp(double %440) #4
	  %451 = load float, float* %f, align 4
	  %450 = getelementptr inbounds float, float* %449, i64 %448
	  %449 = load float*, float** %2, align 8
	  %445 = load i32, i32* %z, align 4
	  %444 = fptrunc double %442 to float
	  %446 = add nsw i32 240, %445
	  %447 = srem i32 %446, 128
	  %448 = sext i32 %447 to i64
	  store float %444, float* %450, align 4
	  %452 = fmul float 0x40D77D7060000000, %451
	  %453 = fsub float 0x403C8C1CA0000000, %452
	  %454 = fpext float %453 to double
	  %456 = call double @exp(double %454) #4
	  %465 = load float, float* %f, align 4
	  %464 = getelementptr inbounds float, float* %463, i64 %462
	  %463 = load float*, float** %2, align 8
	  %459 = load i32, i32* %z, align 4
	  %458 = fptrunc double %456 to float
	  %460 = add nsw i32 248, %459
	  %461 = srem i32 %460, 128
	  %462 = sext i32 %461 to i64
	  store float %458, float* %464, align 4
	  %466 = fmul float 0x40C731F4E0000000, %465
	  %467 = fsub float 0x40405221C0000000, %466
	  %468 = fpext float %467 to double
	  %470 = call double @exp(double %468) #4
	  %494 = load float, float* %f, align 4
	  %491 = load float, float* %e, align 4
	  %490 = getelementptr inbounds float, float* %489, i64 %488
	  %489 = load float*, float** %2, align 8
	  %485 = load i32, i32* %z, align 4
	  %484 = getelementptr inbounds float, float* %483, i64 %482
	  %483 = load float*, float** %2, align 8
	  %479 = load i32, i32* %z, align 4
	  %478 = getelementptr inbounds float, float* %477, i64 %476
	  %477 = load float*, float** %2, align 8
	  %473 = load i32, i32* %z, align 4
	  %472 = fptrunc double %470 to float
	  %474 = add nsw i32 256, %473
	  %475 = srem i32 %474, 128
	  %476 = sext i32 %475 to i64
	  store float %472, float* %478, align 4
	  %480 = add nsw i32 264, %479
	  %481 = srem i32 %480, 128
	  %482 = sext i32 %481 to i64
	  store float 0x42C9EBAC60000000, float* %484, align 4
	  %486 = add nsw i32 272, %485
	  %487 = srem i32 %486, 128
	  %488 = sext i32 %487 to i64
	  store float 0x42BB48EB60000000, float* %490, align 4
	  %492 = fmul float 0x3FFCA3D700000000, %491
	  %493 = fadd float 0x403285B7C0000000, %492
	  %495 = fmul float 0x408A42F980000000, %494
	  %496 = fsub float %493, %495
	  %497 = fpext float %496 to double
	  %499 = call double @exp(double %497) #4
	  %508 = load float, float* %f, align 4
	  %507 = getelementptr inbounds float, float* %506, i64 %505
	  %506 = load float*, float** %2, align 8
	  %502 = load i32, i32* %z, align 4
	  %501 = fptrunc double %499 to float
	  %503 = add nsw i32 280, %502
	  %504 = srem i32 %503, 128
	  %505 = sext i32 %504 to i64
	  store float %501, float* %507, align 4
	  %509 = fmul float 0x4077BEDB80000000, %508
	  %510 = fadd float 0x403D5F8CA0000000, %509
	  %511 = fpext float %510 to double
	  %513 = call double @exp(double %511) #4
	  %534 = load float, float* %f, align 4
	  %533 = getelementptr inbounds float, float* %532, i64 %531
	  %532 = load float*, float** %2, align 8
	  %528 = load i32, i32* %z, align 4
	  %527 = getelementptr inbounds float, float* %526, i64 %525
	  %526 = load float*, float** %2, align 8
	  %522 = load i32, i32* %z, align 4
	  %521 = getelementptr inbounds float, float* %520, i64 %519
	  %520 = load float*, float** %2, align 8
	  %516 = load i32, i32* %z, align 4
	  %515 = fptrunc double %513 to float
	  %517 = add nsw i32 288, %516
	  %518 = srem i32 %517, 128
	  %519 = sext i32 %518 to i64
	  store float %515, float* %521, align 4
	  %523 = add nsw i32 296, %522
	  %524 = srem i32 %523, 128
	  %525 = sext i32 %524 to i64
	  store float 0x42BE036940000000, float* %527, align 4
	  %529 = add nsw i32 304, %528
	  %530 = srem i32 %529, 128
	  %531 = sext i32 %530 to i64
	  store float 0x42C6BCC420000000, float* %533, align 4
	  %535 = fmul float 0x4075B38320000000, %534
	  %536 = fsub float 0x403CDAD400000000, %535
	  %537 = fpext float %536 to double
	  %539 = call double @exp(double %537) #4
	  %551 = load float, float* %f, align 4
	  %548 = load float, float* %e, align 4
	  %547 = getelementptr inbounds float, float* %546, i64 %545
	  %546 = load float*, float** %2, align 8
	  %542 = load i32, i32* %z, align 4
	  %541 = fptrunc double %539 to float
	  %543 = add nsw i32 312, %542
	  %544 = srem i32 %543, 128
	  %545 = sext i32 %544 to i64
	  store float %541, float* %547, align 4
	  %549 = fmul float 0x3FDEB851E0000000, %548
	  %550 = fadd float 0x403BB79A60000000, %549
	  %552 = fmul float 0x40605AC340000000, %551
	  %553 = fadd float %550, %552
	  %554 = fpext float %553 to double
	  %556 = call double @exp(double %554) #4
	  %592 = load float, float* %f, align 4
	  %589 = load float, float* %e, align 4
	  %588 = getelementptr inbounds float, float* %587, i64 %586
	  %587 = load float*, float** %2, align 8
	  %583 = load i32, i32* %z, align 4
	  %582 = getelementptr inbounds float, float* %581, i64 %580
	  %581 = load float*, float** %2, align 8
	  %577 = load i32, i32* %z, align 4
	  %576 = getelementptr inbounds float, float* %575, i64 %574
	  %575 = load float*, float** %2, align 8
	  %571 = load i32, i32* %z, align 4
	  %570 = getelementptr inbounds float, float* %569, i64 %568
	  %569 = load float*, float** %2, align 8
	  %565 = load i32, i32* %z, align 4
	  %564 = getelementptr inbounds float, float* %563, i64 %562
	  %563 = load float*, float** %2, align 8
	  %559 = load i32, i32* %z, align 4
	  %558 = fptrunc double %556 to float
	  %560 = add nsw i32 320, %559
	  %561 = srem i32 %560, 128
	  %562 = sext i32 %561 to i64
	  store float %558, float* %564, align 4
	  %566 = add nsw i32 328, %565
	  %567 = srem i32 %566, 128
	  %568 = sext i32 %567 to i64
	  store float 0x42D0B07140000000, float* %570, align 4
	  %572 = add nsw i32 336, %571
	  %573 = srem i32 %572, 128
	  %574 = sext i32 %573 to i64
	  store float 0x42BB48EB60000000, float* %576, align 4
	  %578 = add nsw i32 344, %577
	  %579 = srem i32 %578, 128
	  %580 = sext i32 %579 to i64
	  store float 0x42BB48EB60000000, float* %582, align 4
	  %584 = add nsw i32 352, %583
	  %585 = srem i32 %584, 128
	  %586 = sext i32 %585 to i64
	  store float 0x42C6BCC420000000, float* %588, align 4
	  %590 = fmul float 1.000000e+00, %589
	  %591 = fsub float 0x4043E28BA0000000, %590
	  %593 = fmul float 0x40C0B55780000000, %592
	  %594 = fsub float %591, %593
	  %595 = fpext float %594 to double
	  %597 = call double @exp(double %595) #4
	  %606 = load float, float* %f, align 4
	  %605 = getelementptr inbounds float, float* %604, i64 %603
	  %604 = load float*, float** %2, align 8
	  %600 = load i32, i32* %z, align 4
	  %599 = fptrunc double %597 to float
	  %601 = add nsw i32 360, %600
	  %602 = srem i32 %601, 128
	  %603 = sext i32 %602 to i64
	  store float %599, float* %605, align 4
	  %607 = fmul float 0x4069292C60000000, %606
	  %608 = fsub float 0x403DA8BF60000000, %607
	  %609 = fpext float %608 to double
	  %611 = call double @exp(double %609) #4
	  %620 = load float, float* %e, align 4
	  %619 = getelementptr inbounds float, float* %618, i64 %617
	  %618 = load float*, float** %2, align 8
	  %614 = load i32, i32* %z, align 4
	  %613 = fptrunc double %611 to float
	  %615 = add nsw i32 368, %614
	  %616 = srem i32 %615, 128
	  %617 = sext i32 %616 to i64
	  store float %613, float* %619, align 4
	  %621 = fmul float 0x3FE99999A0000000, %620
	  %622 = fsub float 0x4042E0FAC0000000, %621
	  %623 = fpext float %622 to double
	  %625 = call double @exp(double %623) #4
	  %637 = load float, float* %f, align 4
	  %634 = load float, float* %e, align 4
	  %633 = getelementptr inbounds float, float* %632, i64 %631
	  %632 = load float*, float** %2, align 8
	  %628 = load i32, i32* %z, align 4
	  %627 = fptrunc double %625 to float
	  %629 = add nsw i32 376, %628
	  %630 = srem i32 %629, 128
	  %631 = sext i32 %630 to i64
	  store float %627, float* %633, align 4
	  %635 = fmul float 2.000000e+00, %634
	  %636 = fadd float 0x402A3EA660000000, %635
	  %638 = fmul float 0x40AC6C8360000000, %637
	  %639 = fsub float %636, %638
	  %640 = fpext float %639 to double
	  %642 = call double @exp(double %640) #4
	  %657 = load float, float* %f, align 4
	  %656 = getelementptr inbounds float, float* %655, i64 %654
	  %655 = load float*, float** %2, align 8
	  %651 = load i32, i32* %z, align 4
	  %650 = getelementptr inbounds float, float* %649, i64 %648
	  %649 = load float*, float** %2, align 8
	  %645 = load i32, i32* %z, align 4
	  %644 = fptrunc double %642 to float
	  %646 = add nsw i32 384, %645
	  %647 = srem i32 %646, 128
	  %648 = sext i32 %647 to i64
	  store float %644, float* %650, align 4
	  %652 = add nsw i32 392, %651
	  %653 = srem i32 %652, 128
	  %654 = sext i32 %653 to i64
	  store float 0x42D2309CE0000000, float* %656, align 4
	  %658 = fmul float 0xC0879699A0000000, %657
	  %659 = fpext float %658 to double
	  %661 = call double @exp(double %659) #4
	  %693 = load float, float* %f, align 4
	  %690 = load float, float* %e, align 4
	  %689 = getelementptr inbounds float, float* %688, i64 %687
	  %688 = load float*, float** %2, align 8
	  %684 = load i32, i32* %z, align 4
	  %683 = getelementptr inbounds float, float* %682, i64 %681
	  %682 = load float*, float** %2, align 8
	  %678 = load i32, i32* %z, align 4
	  %674 = load float, float* %h, align 4
	  %673 = getelementptr inbounds float, float* %672, i64 %671
	  %672 = load float*, float** %2, align 8
	  %668 = load i32, i32* %z, align 4
	  %664 = load float, float* %h, align 4
	  %663 = fptrunc double %661 to float
	  store float %663, float* %h, align 4
	  %665 = fpext float %664 to double
	  %666 = fmul double 1.056000e+13, %665
	  %667 = fptrunc double %666 to float
	  %669 = add nsw i32 400, %668
	  %670 = srem i32 %669, 128
	  %671 = sext i32 %670 to i64
	  store float %667, float* %673, align 4
	  %675 = fpext float %674 to double
	  %676 = fmul double 2.640000e+12, %675
	  %677 = fptrunc double %676 to float
	  %679 = add nsw i32 408, %678
	  %680 = srem i32 %679, 128
	  %681 = sext i32 %680 to i64
	  store float %677, float* %683, align 4
	  %685 = add nsw i32 416, %684
	  %686 = srem i32 %685, 128
	  %687 = sext i32 %686 to i64
	  store float 0x42B2309CE0000000, float* %689, align 4
	  %691 = fmul float 2.000000e+00, %690
	  %692 = fadd float 0x40303D8520000000, %691
	  %694 = fmul float 0x40979699A0000000, %693
	  %695 = fsub float %692, %694
	  %696 = fpext float %695 to double
	  %698 = call double @exp(double %696) #4
	  %716 = load float, float* %f, align 4
	  %713 = load float, float* %e, align 4
	  %712 = getelementptr inbounds float, float* %711, i64 %710
	  %711 = load float*, float** %2, align 8
	  %707 = load i32, i32* %z, align 4
	  %706 = getelementptr inbounds float, float* %705, i64 %704
	  %705 = load float*, float** %2, align 8
	  %701 = load i32, i32* %z, align 4
	  %700 = fptrunc double %698 to float
	  %702 = add nsw i32 424, %701
	  %703 = srem i32 %702, 128
	  %704 = sext i32 %703 to i64
	  store float %700, float* %706, align 4
	  %708 = add nsw i32 432, %707
	  %709 = srem i32 %708, 128
	  %710 = sext i32 %709 to i64
	  store float 0x42B2309CE0000000, float* %712, align 4
	  %714 = fmul float 5.000000e-01, %713
	  %715 = fadd float 0x403B6B98C0000000, %714
	  %717 = fmul float 0x40A1BB03A0000000, %716
	  %718 = fsub float %715, %717
	  %719 = fpext float %718 to double
	  %721 = call double @exp(double %719) #4
	  %742 = load float, float* %f, align 4
	  %741 = getelementptr inbounds float, float* %740, i64 %739
	  %740 = load float*, float** %2, align 8
	  %736 = load i32, i32* %z, align 4
	  %735 = getelementptr inbounds float, float* %734, i64 %733
	  %734 = load float*, float** %2, align 8
	  %730 = load i32, i32* %z, align 4
	  %729 = getelementptr inbounds float, float* %728, i64 %727
	  %728 = load float*, float** %2, align 8
	  %724 = load i32, i32* %z, align 4
	  %723 = fptrunc double %721 to float
	  %725 = add nsw i32 440, %724
	  %726 = srem i32 %725, 128
	  %727 = sext i32 %726 to i64
	  store float %723, float* %729, align 4
	  %731 = add nsw i32 448, %730
	  %732 = srem i32 %731, 128
	  %733 = sext i32 %732 to i64
	  store float 0x42C2309CE0000000, float* %735, align 4
	  %737 = add nsw i32 456, %736
	  %738 = srem i32 %737, 128
	  %739 = sext i32 %738 to i64
	  store float 0x42BD1A94A0000000, float* %741, align 4
	  %743 = fmul float 0x4072DEE140000000, %742
	  %744 = fsub float 0x403E56CD60000000, %743
	  %745 = fpext float %744 to double
	  %747 = call double @exp(double %745) #4
	  %825 = load float, float* %f, align 4
	  %822 = load float, float* %e, align 4
	  %821 = getelementptr inbounds float, float* %820, i64 %819
	  %820 = load float*, float** %2, align 8
	  %816 = load i32, i32* %z, align 4
	  %815 = getelementptr inbounds float, float* %814, i64 %813
	  %814 = load float*, float** %2, align 8
	  %810 = load i32, i32* %z, align 4
	  %809 = getelementptr inbounds float, float* %808, i64 %807
	  %808 = load float*, float** %2, align 8
	  %804 = load i32, i32* %z, align 4
	  %803 = getelementptr inbounds float, float* %802, i64 %801
	  %802 = load float*, float** %2, align 8
	  %798 = load i32, i32* %z, align 4
	  %797 = getelementptr inbounds float, float* %796, i64 %795
	  %796 = load float*, float** %2, align 8
	  %792 = load i32, i32* %z, align 4
	  %791 = getelementptr inbounds float, float* %790, i64 %789
	  %790 = load float*, float** %2, align 8
	  %786 = load i32, i32* %z, align 4
	  %785 = getelementptr inbounds float, float* %784, i64 %783
	  %784 = load float*, float** %2, align 8
	  %780 = load i32, i32* %z, align 4
	  %779 = getelementptr inbounds float, float* %778, i64 %777
	  %778 = load float*, float** %2, align 8
	  %774 = load i32, i32* %z, align 4
	  %773 = getelementptr inbounds float, float* %772, i64 %771
	  %772 = load float*, float** %2, align 8
	  %768 = load i32, i32* %z, align 4
	  %767 = getelementptr inbounds float, float* %766, i64 %765
	  %766 = load float*, float** %2, align 8
	  %762 = load i32, i32* %z, align 4
	  %761 = getelementptr inbounds float, float* %760, i64 %759
	  %760 = load float*, float** %2, align 8
	  %756 = load i32, i32* %z, align 4
	  %755 = getelementptr inbounds float, float* %754, i64 %753
	  %754 = load float*, float** %2, align 8
	  %750 = load i32, i32* %z, align 4
	  %749 = fptrunc double %747 to float
	  %751 = add nsw i32 464, %750
	  %752 = srem i32 %751, 128
	  %753 = sext i32 %752 to i64
	  store float %749, float* %755, align 4
	  %757 = add nsw i32 472, %756
	  %758 = srem i32 %757, 128
	  %759 = sext i32 %758 to i64
	  store float 0x42BB48EB60000000, float* %761, align 4
	  %763 = add nsw i32 480, %762
	  %764 = srem i32 %763, 128
	  %765 = sext i32 %764 to i64
	  store float 0x42AB48EB60000000, float* %767, align 4
	  %769 = add nsw i32 488, %768
	  %770 = srem i32 %769, 128
	  %771 = sext i32 %770 to i64
	  store float 0x42AB48EB60000000, float* %773, align 4
	  %775 = add nsw i32 496, %774
	  %776 = srem i32 %775, 128
	  %777 = sext i32 %776 to i64
	  store float 0x42BB48EB60000000, float* %779, align 4
	  %781 = add nsw i32 504, %780
	  %782 = srem i32 %781, 128
	  %783 = sext i32 %782 to i64
	  store float 0x42CFD512A0000000, float* %785, align 4
	  %787 = add nsw i32 512, %786
	  %788 = srem i32 %787, 128
	  %789 = sext i32 %788 to i64
	  store float 0x42B9774200000000, float* %791, align 4
	  %793 = add nsw i32 520, %792
	  %794 = srem i32 %793, 128
	  %795 = sext i32 %794 to i64
	  store float 0x42A5D3EF80000000, float* %797, align 4
	  %799 = add nsw i32 528, %798
	  %800 = srem i32 %799, 128
	  %801 = sext i32 %800 to i64
	  store float 0x42BB48EB60000000, float* %803, align 4
	  %805 = add nsw i32 536, %804
	  %806 = srem i32 %805, 128
	  %807 = sext i32 %806 to i64
	  store float 0x42A05EF3A0000000, float* %809, align 4
	  %811 = add nsw i32 544, %810
	  %812 = srem i32 %811, 128
	  %813 = sext i32 %812 to i64
	  store float 0x4299774200000000, float* %815, align 4
	  %817 = add nsw i32 552, %816
	  %818 = srem i32 %817, 128
	  %819 = sext i32 %818 to i64
	  store float 0x42A9774200000000, float* %821, align 4
	  %823 = fmul float 0x3FDD0E5600000000, %822
	  %824 = fadd float 0x403B03CC40000000, %823
	  %826 = fmul float 0x4094717400000000, %825
	  %827 = fsub float %824, %826
	  %828 = fpext float %827 to double
	  %830 = call double @exp(double %828) #4
	  %842 = load float, float* %f, align 4
	  %839 = load float, float* %e, align 4
	  %838 = getelementptr inbounds float, float* %837, i64 %836
	  %837 = load float*, float** %2, align 8
	  %833 = load i32, i32* %z, align 4
	  %832 = fptrunc double %830 to float
	  %834 = add nsw i32 560, %833
	  %835 = srem i32 %834, 128
	  %836 = sext i32 %835 to i64
	  store float %832, float* %838, align 4
	  %840 = fmul float 0x3FF0CCCCC0000000, %839
	  %841 = fadd float 0x4037DBD7C0000000, %840
	  %843 = fmul float 0x4099C02360000000, %842
	  %844 = fsub float %841, %843
	  %845 = fpext float %844 to double
	  %847 = call double @exp(double %845) #4
	  %856 = load float, float* %f, align 4
	  %855 = getelementptr inbounds float, float* %854, i64 %853
	  %854 = load float*, float** %2, align 8
	  %850 = load i32, i32* %z, align 4
	  %849 = fptrunc double %847 to float
	  %851 = add nsw i32 568, %850
	  %852 = srem i32 %851, 128
	  %853 = sext i32 %852 to i64
	  store float %849, float* %855, align 4
	  %857 = fmul float 0x409BD58C40000000, %856
	  %858 = fsub float 0x403F4B69C0000000, %857
	  %859 = fpext float %858 to double
	  %861 = call double @exp(double %859) #4
	  %873 = load float, float* %f, align 4
	  %870 = load float, float* %e, align 4
	  %869 = getelementptr inbounds float, float* %868, i64 %867
	  %868 = load float*, float** %2, align 8
	  %864 = load i32, i32* %z, align 4
	  %863 = fptrunc double %861 to float
	  %865 = add nsw i32 576, %864
	  %866 = srem i32 %865, 128
	  %867 = sext i32 %866 to i64
	  store float %863, float* %869, align 4
	  %871 = fmul float 0x3FF2E147A0000000, %870
	  %872 = fadd float 0x4035F4B100000000, %871
	  %874 = fmul float 0x406C1E02E0000000, %873
	  %875 = fadd float %872, %874
	  %876 = fpext float %875 to double
	  %878 = call double @exp(double %876) #4
	  %887 = load float, float* %f, align 4
	  %886 = getelementptr inbounds float, float* %885, i64 %884
	  %885 = load float*, float** %2, align 8
	  %881 = load i32, i32* %z, align 4
	  %880 = fptrunc double %878 to float
	  %882 = add nsw i32 584, %881
	  %883 = srem i32 %882, 128
	  %884 = sext i32 %883 to i64
	  store float %880, float* %886, align 4
	  %888 = fmul float 0x40D3A82AA0000000, %887
	  %889 = fsub float 0x40401E3B80000000, %888
	  %890 = fpext float %889 to double
	  %892 = call double @exp(double %890) #4
	  %901 = load float, float* %f, align 4
	  %900 = getelementptr inbounds float, float* %899, i64 %898
	  %899 = load float*, float** %2, align 8
	  %895 = load i32, i32* %z, align 4
	  %894 = fptrunc double %892 to float
	  %896 = add nsw i32 592, %895
	  %897 = srem i32 %896, 128
	  %898 = sext i32 %897 to i64
	  store float %894, float* %900, align 4
	  %902 = fmul float 0xC0AF737780000000, %901
	  %903 = fpext float %902 to double
	  %905 = call double @exp(double %903) #4
	  %938 = load float, float* %f, align 4
	  %937 = getelementptr inbounds float, float* %936, i64 %935
	  %936 = load float*, float** %2, align 8
	  %932 = load i32, i32* %z, align 4
	  %928 = load float, float* %h, align 4
	  %927 = getelementptr inbounds float, float* %926, i64 %925
	  %926 = load float*, float** %2, align 8
	  %922 = load i32, i32* %z, align 4
	  %918 = load float, float* %h, align 4
	  %917 = getelementptr inbounds float, float* %916, i64 %915
	  %916 = load float*, float** %2, align 8
	  %912 = load i32, i32* %z, align 4
	  %908 = load float, float* %h, align 4
	  %907 = fptrunc double %905 to float
	  store float %907, float* %h, align 4
	  %909 = fpext float %908 to double
	  %910 = fmul double 1.000000e+12, %909
	  %911 = fptrunc double %910 to float
	  %913 = add nsw i32 600, %912
	  %914 = srem i32 %913, 128
	  %915 = sext i32 %914 to i64
	  store float %911, float* %917, align 4
	  %919 = fpext float %918 to double
	  %920 = fmul double 5.000000e+13, %919
	  %921 = fptrunc double %920 to float
	  %923 = add nsw i32 1008, %922
	  %924 = srem i32 %923, 128
	  %925 = sext i32 %924 to i64
	  store float %921, float* %927, align 4
	  %929 = fpext float %928 to double
	  %930 = fmul double 8.000000e+08, %929
	  %931 = fptrunc double %930 to float
	  %933 = add nsw i32 0, %932
	  %934 = srem i32 %933, 128
	  %935 = sext i32 %934 to i64
	  store float %931, float* %937, align 4
	  %939 = fmul float 0x4070328160000000, %938
	  %940 = fadd float 0x4040172080000000, %939
	  %941 = fpext float %940 to double
	  %943 = call double @exp(double %941) #4
	  %955 = load float, float* %f, align 4
	  %952 = load float, float* %e, align 4
	  %951 = getelementptr inbounds float, float* %950, i64 %949
	  %950 = load float*, float** %2, align 8
	  %946 = load i32, i32* %z, align 4
	  %945 = fptrunc double %943 to float
	  %947 = add nsw i32 608, %946
	  %948 = srem i32 %947, 128
	  %949 = sext i32 %948 to i64
	  store float %945, float* %951, align 4
	  %953 = fmul float 0x3FE428F5C0000000, %952
	  %954 = fsub float 0x40428A49E0000000, %953
	  %956 = fmul float 0x4068176C60000000, %955
	  %957 = fsub float %954, %956
	  %958 = fpext float %957 to double
	  %960 = call double @exp(double %958) #4
	  %978 = load float, float* %f, align 4
	  %975 = load float, float* %e, align 4
	  %974 = getelementptr inbounds float, float* %973, i64 %972
	  %973 = load float*, float** %2, align 8
	  %969 = load i32, i32* %z, align 4
	  %968 = getelementptr inbounds float, float* %967, i64 %966
	  %967 = load float*, float** %2, align 8
	  %963 = load i32, i32* %z, align 4
	  %962 = fptrunc double %960 to float
	  %964 = add nsw i32 616, %963
	  %965 = srem i32 %964, 128
	  %966 = sext i32 %965 to i64
	  store float %962, float* %968, align 4
	  %970 = add nsw i32 624, %969
	  %971 = srem i32 %970, 128
	  %972 = sext i32 %971 to i64
	  store float 0x42D32AE7E0000000, float* %974, align 4
	  %976 = fmul float 0x3FF99999A0000000, %975
	  %977 = fadd float 0x4031D742C0000000, %976
	  %979 = fmul float 0x40A54EDE60000000, %978
	  %980 = fsub float %977, %979
	  %981 = fpext float %980 to double
	  %983 = call double @exp(double %981) #4
	  %998 = load float, float* %f, align 4
	  %997 = getelementptr inbounds float, float* %996, i64 %995
	  %996 = load float*, float** %2, align 8
	  %992 = load i32, i32* %z, align 4
	  %991 = getelementptr inbounds float, float* %990, i64 %989
	  %990 = load float*, float** %2, align 8
	  %986 = load i32, i32* %z, align 4
	  %985 = fptrunc double %983 to float
	  %987 = add nsw i32 632, %986
	  %988 = srem i32 %987, 128
	  %989 = sext i32 %988 to i64
	  store float %985, float* %991, align 4
	  %993 = add nsw i32 640, %992
	  %994 = srem i32 %993, 128
	  %995 = sext i32 %994 to i64
	  store float 0x42B6BF1820000000, float* %997, align 4
	  %999 = fmul float 0x40CC4E51E0000000, %998
	  %1000 = fsub float 0x403F0F3C00000000, %999
	  %1001 = fpext float %1000 to double
	  %1003 = call double @exp(double %1001) #4
	  %1012 = load float, float* %f, align 4
	  %1011 = getelementptr inbounds float, float* %1010, i64 %1009
	  %1010 = load float*, float** %2, align 8
	  %1006 = load i32, i32* %z, align 4
	  %1005 = fptrunc double %1003 to float
	  %1007 = add nsw i32 648, %1006
	  %1008 = srem i32 %1007, 128
	  %1009 = sext i32 %1008 to i64
	  store float %1005, float* %1011, align 4
	  %1013 = fmul float 0x40B192C1C0000000, %1012
	  %1014 = fsub float 0x40384E8980000000, %1013
	  %1015 = fpext float %1014 to double
	  %1017 = call double @exp(double %1015) #4
	  %1041 = load float, float* %f, align 4
	  %1038 = load float, float* %e, align 4
	  %1037 = getelementptr inbounds float, float* %1036, i64 %1035
	  %1036 = load float*, float** %2, align 8
	  %1032 = load i32, i32* %z, align 4
	  %1031 = getelementptr inbounds float, float* %1030, i64 %1029
	  %1030 = load float*, float** %2, align 8
	  %1026 = load i32, i32* %z, align 4
	  %1025 = getelementptr inbounds float, float* %1024, i64 %1023
	  %1024 = load float*, float** %2, align 8
	  %1020 = load i32, i32* %z, align 4
	  %1019 = fptrunc double %1017 to float
	  %1021 = add nsw i32 656, %1020
	  %1022 = srem i32 %1021, 128
	  %1023 = sext i32 %1022 to i64
	  store float %1019, float* %1025, align 4
	  %1027 = add nsw i32 664, %1026
	  %1028 = srem i32 %1027, 128
	  %1029 = sext i32 %1028 to i64
	  store float 0x426D1A94A0000000, float* %1031, align 4
	  %1033 = add nsw i32 672, %1032
	  %1034 = srem i32 %1033, 128
	  %1035 = sext i32 %1034 to i64
	  store float 0x42A85FDC80000000, float* %1037, align 4
	  %1039 = fmul float 0x4003C28F60000000, %1038
	  %1040 = fadd float 0x4024367DC0000000, %1039
	  %1042 = fmul float 0x40A45D5320000000, %1041
	  %1043 = fsub float %1040, %1042
	  %1044 = fpext float %1043 to double
	  %1046 = call double @exp(double %1044) #4
	  %1076 = load float, float* %f, align 4
	  %1073 = load float, float* %e, align 4
	  %1072 = getelementptr inbounds float, float* %1071, i64 %1070
	  %1071 = load float*, float** %2, align 8
	  %1067 = load i32, i32* %z, align 4
	  %1066 = getelementptr inbounds float, float* %1065, i64 %1064
	  %1065 = load float*, float** %2, align 8
	  %1061 = load i32, i32* %z, align 4
	  %1060 = getelementptr inbounds float, float* %1059, i64 %1058
	  %1059 = load float*, float** %2, align 8
	  %1055 = load i32, i32* %z, align 4
	  %1054 = getelementptr inbounds float, float* %1053, i64 %1052
	  %1053 = load float*, float** %2, align 8
	  %1049 = load i32, i32* %z, align 4
	  %1048 = fptrunc double %1046 to float
	  %1050 = add nsw i32 680, %1049
	  %1051 = srem i32 %1050, 128
	  %1052 = sext i32 %1051 to i64
	  store float %1048, float* %1054, align 4
	  %1056 = add nsw i32 688, %1055
	  %1057 = srem i32 %1056, 128
	  %1058 = sext i32 %1057 to i64
	  store float 0x42BB48EB60000000, float* %1060, align 4
	  %1062 = add nsw i32 696, %1061
	  %1063 = srem i32 %1062, 128
	  %1064 = sext i32 %1063 to i64
	  store float 0x429ED99D80000000, float* %1066, align 4
	  %1068 = add nsw i32 704, %1067
	  %1069 = srem i32 %1068, 128
	  %1070 = sext i32 %1069 to i64
	  store float 0x42B05EF3A0000000, float* %1072, align 4
	  %1074 = fmul float 0x40067AE140000000, %1073
	  %1075 = fadd float 0x4020372720000000, %1074
	  %1077 = fmul float 0x40A709B300000000, %1076
	  %1078 = fsub float %1075, %1077
	  %1079 = fpext float %1078 to double
	  %1081 = call double @exp(double %1079) #4
	  %1096 = load float, float* %f, align 4
	  %1095 = getelementptr inbounds float, float* %1094, i64 %1093
	  %1094 = load float*, float** %2, align 8
	  %1090 = load i32, i32* %z, align 4
	  %1089 = getelementptr inbounds float, float* %1088, i64 %1087
	  %1088 = load float*, float** %2, align 8
	  %1084 = load i32, i32* %z, align 4
	  %1083 = fptrunc double %1081 to float
	  %1085 = add nsw i32 712, %1084
	  %1086 = srem i32 %1085, 128
	  %1087 = sext i32 %1086 to i64
	  store float %1083, float* %1089, align 4
	  %1091 = add nsw i32 720, %1090
	  %1092 = srem i32 %1091, 128
	  %1093 = sext i32 %1092 to i64
	  store float 0x42C2309CE0000000, float* %1095, align 4
	  %1097 = fmul float 0x4071ED5600000000, %1096
	  %1098 = fpext float %1097 to double
	  %1100 = call double @exp(double %1098) #4
	  %1126 = load float, float* %f, align 4
	  %1123 = load float, float* %e, align 4
	  %1122 = getelementptr inbounds float, float* %1121, i64 %1120
	  %1121 = load float*, float** %2, align 8
	  %1117 = load i32, i32* %z, align 4
	  %1113 = load float, float* %h, align 4
	  %1112 = getelementptr inbounds float, float* %1111, i64 %1110
	  %1111 = load float*, float** %2, align 8
	  %1107 = load i32, i32* %z, align 4
	  %1103 = load float, float* %h, align 4
	  %1102 = fptrunc double %1100 to float
	  store float %1102, float* %h, align 4
	  %1104 = fpext float %1103 to double
	  %1105 = fmul double 1.200000e+13, %1104
	  %1106 = fptrunc double %1105 to float
	  %1108 = add nsw i32 728, %1107
	  %1109 = srem i32 %1108, 128
	  %1110 = sext i32 %1109 to i64
	  store float %1106, float* %1112, align 4
	  %1114 = fpext float %1113 to double
	  %1115 = fmul double 1.600000e+13, %1114
	  %1116 = fptrunc double %1115 to float
	  %1118 = add nsw i32 848, %1117
	  %1119 = srem i32 %1118, 128
	  %1120 = sext i32 %1119 to i64
	  store float %1116, float* %1122, align 4
	  %1124 = fmul float 0x3FEF0A3D80000000, %1123
	  %1125 = fsub float 0x4042CBE020000000, %1124
	  %1127 = fmul float 0x40737FE8C0000000, %1126
	  %1128 = fsub float %1125, %1127
	  %1129 = fpext float %1128 to double
	  %1131 = call double @exp(double %1129) #4
	  %1143 = load float, float* %f, align 4
	  %1140 = load float, float* %e, align 4
	  %1139 = getelementptr inbounds float, float* %1138, i64 %1137
	  %1138 = load float*, float** %2, align 8
	  %1134 = load i32, i32* %z, align 4
	  %1133 = fptrunc double %1131 to float
	  %1135 = add nsw i32 736, %1134
	  %1136 = srem i32 %1135, 128
	  %1137 = sext i32 %1136 to i64
	  store float %1133, float* %1139, align 4
	  %1141 = fmul float 0x3FB99999A0000000, %1140
	  %1142 = fadd float 0x403D3D0B80000000, %1141
	  %1144 = fmul float 0x40B4D618C0000000, %1143
	  %1145 = fsub float %1142, %1144
	  %1146 = fpext float %1145 to double
	  %1148 = call double @exp(double %1146) #4
	  %1196 = load float, float* %f, align 4
	  %1193 = load float, float* %e, align 4
	  %1192 = getelementptr inbounds float, float* %1191, i64 %1190
	  %1191 = load float*, float** %2, align 8
	  %1187 = load i32, i32* %z, align 4
	  %1186 = getelementptr inbounds float, float* %1185, i64 %1184
	  %1185 = load float*, float** %2, align 8
	  %1181 = load i32, i32* %z, align 4
	  %1180 = getelementptr inbounds float, float* %1179, i64 %1178
	  %1179 = load float*, float** %2, align 8
	  %1175 = load i32, i32* %z, align 4
	  %1174 = getelementptr inbounds float, float* %1173, i64 %1172
	  %1173 = load float*, float** %2, align 8
	  %1169 = load i32, i32* %z, align 4
	  %1168 = getelementptr inbounds float, float* %1167, i64 %1166
	  %1167 = load float*, float** %2, align 8
	  %1163 = load i32, i32* %z, align 4
	  %1162 = getelementptr inbounds float, float* %1161, i64 %1160
	  %1161 = load float*, float** %2, align 8
	  %1157 = load i32, i32* %z, align 4
	  %1156 = getelementptr inbounds float, float* %1155, i64 %1154
	  %1155 = load float*, float** %2, align 8
	  %1151 = load i32, i32* %z, align 4
	  %1150 = fptrunc double %1148 to float
	  %1152 = add nsw i32 744, %1151
	  %1153 = srem i32 %1152, 128
	  %1154 = sext i32 %1153 to i64
	  store float %1150, float* %1156, align 4
	  %1158 = add nsw i32 752, %1157
	  %1159 = srem i32 %1158, 128
	  %1160 = sext i32 %1159 to i64
	  store float 0x42C6BCC420000000, float* %1162, align 4
	  %1164 = add nsw i32 760, %1163
	  %1165 = srem i32 %1164, 128
	  %1166 = sext i32 %1165 to i64
	  store float 0x42B2309CE0000000, float* %1168, align 4
	  %1170 = add nsw i32 768, %1169
	  %1171 = srem i32 %1170, 128
	  %1172 = sext i32 %1171 to i64
	  store float 0x42BD1A94A0000000, float* %1174, align 4
	  %1176 = add nsw i32 776, %1175
	  %1177 = srem i32 %1176, 128
	  %1178 = sext i32 %1177 to i64
	  store float 0x42AD1A94A0000000, float* %1180, align 4
	  %1182 = add nsw i32 784, %1181
	  %1183 = srem i32 %1182, 128
	  %1184 = sext i32 %1183 to i64
	  store float 0x42A2309CE0000000, float* %1186, align 4
	  %1188 = add nsw i32 792, %1187
	  %1189 = srem i32 %1188, 128
	  %1190 = sext i32 %1189 to i64
	  store float 0x4292309CE0000000, float* %1192, align 4
	  %1194 = fmul float 0x401E666660000000, %1193
	  %1195 = fadd float 0xC03C7ACA80000000, %1194
	  %1197 = fmul float 0x409BC16B60000000, %1196
	  %1198 = fadd float %1195, %1197
	  %1199 = fpext float %1198 to double
	  %1201 = call double @exp(double %1199) #4
	  %1213 = load float, float* %f, align 4
	  %1210 = load float, float* %e, align 4
	  %1209 = getelementptr inbounds float, float* %1208, i64 %1207
	  %1208 = load float*, float** %2, align 8
	  %1204 = load i32, i32* %z, align 4
	  %1203 = fptrunc double %1201 to float
	  %1205 = add nsw i32 800, %1204
	  %1206 = srem i32 %1205, 128
	  %1207 = sext i32 %1206 to i64
	  store float %1203, float* %1209, align 4
	  %1211 = fmul float 0x3FF9EB8520000000, %1210
	  %1212 = fadd float 0x40344EC8C0000000, %1211
	  %1214 = fmul float 0x40B54EDE60000000, %1213
	  %1215 = fsub float %1212, %1214
	  %1216 = fpext float %1215 to double
	  %1218 = call double @exp(double %1216) #4
	  %1230 = load float, float* %f, align 4
	  %1227 = load float, float* %e, align 4
	  %1226 = getelementptr inbounds float, float* %1225, i64 %1224
	  %1225 = load float*, float** %2, align 8
	  %1221 = load i32, i32* %z, align 4
	  %1220 = fptrunc double %1218 to float
	  %1222 = add nsw i32 808, %1221
	  %1223 = srem i32 %1222, 128
	  %1224 = sext i32 %1223 to i64
	  store float %1220, float* %1226, align 4
	  %1228 = fmul float 1.500000e+00, %1227
	  %1229 = fadd float 0x4034BE39C0000000, %1228
	  %1231 = fmul float 0x40B0E7A9E0000000, %1230
	  %1232 = fsub float %1229, %1231
	  %1233 = fpext float %1232 to double
	  %1235 = call double @exp(double %1233) #4
	  %1247 = load float, float* %f, align 4
	  %1244 = load float, float* %e, align 4
	  %1243 = getelementptr inbounds float, float* %1242, i64 %1241
	  %1242 = load float*, float** %2, align 8
	  %1238 = load i32, i32* %z, align 4
	  %1237 = fptrunc double %1235 to float
	  %1239 = add nsw i32 816, %1238
	  %1240 = srem i32 %1239, 128
	  %1241 = sext i32 %1240 to i64
	  store float %1237, float* %1243, align 4
	  %1245 = fmul float 0x3FF99999A0000000, %1244
	  %1246 = fadd float 0x40326BB1C0000000, %1245
	  %1248 = fmul float 0x40988824E0000000, %1247
	  %1249 = fsub float %1246, %1248
	  %1250 = fpext float %1249 to double
	  %1252 = call double @exp(double %1250) #4
	  %1270 = load float, float* %f, align 4
	  %1267 = load float, float* %e, align 4
	  %1266 = getelementptr inbounds float, float* %1265, i64 %1264
	  %1265 = load float*, float** %2, align 8
	  %1261 = load i32, i32* %z, align 4
	  %1260 = getelementptr inbounds float, float* %1259, i64 %1258
	  %1259 = load float*, float** %2, align 8
	  %1255 = load i32, i32* %z, align 4
	  %1254 = fptrunc double %1252 to float
	  %1256 = add nsw i32 824, %1255
	  %1257 = srem i32 %1256, 128
	  %1258 = sext i32 %1257 to i64
	  store float %1254, float* %1260, align 4
	  %1262 = add nsw i32 832, %1261
	  %1263 = srem i32 %1262, 128
	  %1264 = sext i32 %1263 to i64
	  store float 0x42CB48EB60000000, float* %1266, align 4
	  %1268 = fmul float 2.000000e+00, %1267
	  %1269 = fadd float 0x402D6E6C80000000, %1268
	  %1271 = fmul float 0x40B0419A20000000, %1270
	  %1272 = fsub float %1269, %1271
	  %1273 = fpext float %1272 to double
	  %1275 = call double @exp(double %1273) #4
	  %1296 = load float, float* %f, align 4
	  %1295 = getelementptr inbounds float, float* %1294, i64 %1293
	  %1294 = load float*, float** %2, align 8
	  %1290 = load i32, i32* %z, align 4
	  %1289 = getelementptr inbounds float, float* %1288, i64 %1287
	  %1288 = load float*, float** %2, align 8
	  %1284 = load i32, i32* %z, align 4
	  %1283 = getelementptr inbounds float, float* %1282, i64 %1281
	  %1282 = load float*, float** %2, align 8
	  %1278 = load i32, i32* %z, align 4
	  %1277 = fptrunc double %1275 to float
	  %1279 = add nsw i32 840, %1278
	  %1280 = srem i32 %1279, 128
	  %1281 = sext i32 %1280 to i64
	  store float %1277, float* %1283, align 4
	  %1285 = add nsw i32 856, %1284
	  %1286 = srem i32 %1285, 128
	  %1287 = sext i32 %1286 to i64
	  store float 0x42D6BCC420000000, float* %1289, align 4
	  %1291 = add nsw i32 864, %1290
	  %1292 = srem i32 %1291, 128
	  %1293 = sext i32 %1292 to i64
	  store float 0x42D6BCC420000000, float* %1295, align 4
	  %1297 = fmul float 0x407ADBF3E0000000, %1296
	  %1298 = fsub float 0x403C19DCC0000000, %1297
	  %1299 = fpext float %1298 to double
	  %1301 = call double @exp(double %1299) #4
	  %1331 = load float, float* %f, align 4
	  %1328 = load float, float* %e, align 4
	  %1327 = getelementptr inbounds float, float* %1326, i64 %1325
	  %1326 = load float*, float** %2, align 8
	  %1322 = load i32, i32* %z, align 4
	  %1321 = getelementptr inbounds float, float* %1320, i64 %1319
	  %1320 = load float*, float** %2, align 8
	  %1316 = load i32, i32* %z, align 4
	  %1315 = getelementptr inbounds float, float* %1314, i64 %1313
	  %1314 = load float*, float** %2, align 8
	  %1310 = load i32, i32* %z, align 4
	  %1309 = getelementptr inbounds float, float* %1308, i64 %1307
	  %1308 = load float*, float** %2, align 8
	  %1304 = load i32, i32* %z, align 4
	  %1303 = fptrunc double %1301 to float
	  %1305 = add nsw i32 872, %1304
	  %1306 = srem i32 %1305, 128
	  %1307 = sext i32 %1306 to i64
	  store float %1303, float* %1309, align 4
	  %1311 = add nsw i32 880, %1310
	  %1312 = srem i32 %1311, 128
	  %1313 = sext i32 %1312 to i64
	  store float 0x42C6BCC420000000, float* %1315, align 4
	  %1317 = add nsw i32 888, %1316
	  %1318 = srem i32 %1317, 128
	  %1319 = sext i32 %1318 to i64
	  store float 0x42BB48EB60000000, float* %1321, align 4
	  %1323 = add nsw i32 896, %1322
	  %1324 = srem i32 %1323, 128
	  %1325 = sext i32 %1324 to i64
	  store float 0x42A2309CE0000000, float* %1327, align 4
	  %1329 = fmul float 0x3FE0A3D700000000, %1328
	  %1330 = fsub float 0x40412866A0000000, %1329
	  %1332 = fmul float 0x40D8F08FC0000000, %1331
	  %1333 = fsub float %1330, %1332
	  %1334 = fpext float %1333 to double
	  %1336 = call double @exp(double %1334) #4
	  %1348 = load float, float* %f, align 4
	  %1345 = load float, float* %e, align 4
	  %1344 = getelementptr inbounds float, float* %1343, i64 %1342
	  %1343 = load float*, float** %2, align 8
	  %1339 = load i32, i32* %z, align 4
	  %1338 = fptrunc double %1336 to float
	  %1340 = add nsw i32 904, %1339
	  %1341 = srem i32 %1340, 128
	  %1342 = sext i32 %1341 to i64
	  store float %1338, float* %1344, align 4
	  %1346 = fmul float 0x3FF9EB8520000000, %1345
	  %1347 = fadd float 0x4033C57700000000, %1346
	  %1349 = fmul float 0x40D234D200000000, %1348
	  %1350 = fsub float %1347, %1349
	  %1351 = fpext float %1350 to double
	  %1353 = call double @exp(double %1351) #4
	  %1364 = load float, float* %f, align 4
	  %1362 = load float, float* %e, align 4
	  %1361 = getelementptr inbounds float, float* %1360, i64 %1359
	  %1360 = load float*, float** %2, align 8
	  %1356 = load i32, i32* %z, align 4
	  %1355 = fptrunc double %1353 to float
	  %1357 = add nsw i32 912, %1356
	  %1358 = srem i32 %1357, 128
	  %1359 = sext i32 %1358 to i64
	  store float %1355, float* %1361, align 4
	  %1363 = fmul float 2.000000e+00, %1362
	  %1365 = fmul float 0x408DE0E4C0000000, %1364
	  %1366 = fsub float %1363, %1365
	  %1367 = fpext float %1366 to double
	  %1369 = call double @exp(double %1367) #4
	  %1395 = load float, float* %f, align 4
	  %1392 = load float, float* %e, align 4
	  %1391 = getelementptr inbounds float, float* %1390, i64 %1389
	  %1390 = load float*, float** %2, align 8
	  %1386 = load i32, i32* %z, align 4
	  %1382 = load float, float* %h, align 4
	  %1381 = getelementptr inbounds float, float* %1380, i64 %1379
	  %1380 = load float*, float** %2, align 8
	  %1376 = load i32, i32* %z, align 4
	  %1372 = load float, float* %h, align 4
	  %1371 = fptrunc double %1369 to float
	  store float %1371, float* %h, align 4
	  %1373 = fpext float %1372 to double
	  %1374 = fmul double 1.632000e+07, %1373
	  %1375 = fptrunc double %1374 to float
	  %1377 = add nsw i32 920, %1376
	  %1378 = srem i32 %1377, 128
	  %1379 = sext i32 %1378 to i64
	  store float %1375, float* %1381, align 4
	  %1383 = fpext float %1382 to double
	  %1384 = fmul double 4.080000e+06, %1383
	  %1385 = fptrunc double %1384 to float
	  %1387 = add nsw i32 928, %1386
	  %1388 = srem i32 %1387, 128
	  %1389 = sext i32 %1388 to i64
	  store float %1385, float* %1391, align 4
	  %1393 = fmul float 4.500000e+00, %1392
	  %1394 = fadd float 0xC020DCAE20000000, %1393
	  %1396 = fmul float 0x407F737780000000, %1395
	  %1397 = fadd float %1394, %1396
	  %1398 = fpext float %1397 to double
	  %1400 = call double @exp(double %1398) #4
	  %1412 = load float, float* %f, align 4
	  %1409 = load float, float* %e, align 4
	  %1408 = getelementptr inbounds float, float* %1407, i64 %1406
	  %1407 = load float*, float** %2, align 8
	  %1403 = load i32, i32* %z, align 4
	  %1402 = fptrunc double %1400 to float
	  %1404 = add nsw i32 936, %1403
	  %1405 = srem i32 %1404, 128
	  %1406 = sext i32 %1405 to i64
	  store float %1402, float* %1408, align 4
	  %1410 = fmul float 4.000000e+00, %1409
	  %1411 = fadd float 0xC01E8ABEE0000000, %1410
	  %1413 = fmul float 0x408F737780000000, %1412
	  %1414 = fadd float %1411, %1413
	  %1415 = fpext float %1414 to double
	  %1417 = call double @exp(double %1415) #4
	  %1429 = load float, float* %f, align 4
	  %1426 = load float, float* %e, align 4
	  %1425 = getelementptr inbounds float, float* %1424, i64 %1423
	  %1424 = load float*, float** %2, align 8
	  %1420 = load i32, i32* %z, align 4
	  %1419 = fptrunc double %1417 to float
	  %1421 = add nsw i32 944, %1420
	  %1422 = srem i32 %1421, 128
	  %1423 = sext i32 %1422 to i64
	  store float %1419, float* %1425, align 4
	  %1427 = fmul float 2.000000e+00, %1426
	  %1428 = fadd float 0x40301E3B80000000, %1427
	  %1430 = fmul float 0x40A79699A0000000, %1429
	  %1431 = fsub float %1428, %1430
	  %1432 = fpext float %1431 to double
	  %1434 = call double @exp(double %1432) #4
	  %1446 = load float, float* %f, align 4
	  %1443 = load float, float* %e, align 4
	  %1442 = getelementptr inbounds float, float* %1441, i64 %1440
	  %1441 = load float*, float** %2, align 8
	  %1437 = load i32, i32* %z, align 4
	  %1436 = fptrunc double %1434 to float
	  %1438 = add nsw i32 952, %1437
	  %1439 = srem i32 %1438, 128
	  %1440 = sext i32 %1439 to i64
	  store float %1436, float* %1442, align 4
	  %1444 = fmul float 0x4027A3D700000000, %1443
	  %1445 = fsub float 0x405FDB8F80000000, %1444
	  %1447 = fmul float 0x40D18EFBA0000000, %1446
	  %1448 = fsub float %1445, %1447
	  %1449 = fpext float %1448 to double
	  %1451 = call double @exp(double %1449) #4
	  %1487 = load float, float* %f, align 4
	  %1484 = load float, float* %e, align 4
	  %1483 = getelementptr inbounds float, float* %1482, i64 %1481
	  %1482 = load float*, float** %2, align 8
	  %1478 = load i32, i32* %z, align 4
	  %1477 = getelementptr inbounds float, float* %1476, i64 %1475
	  %1476 = load float*, float** %2, align 8
	  %1472 = load i32, i32* %z, align 4
	  %1471 = getelementptr inbounds float, float* %1470, i64 %1469
	  %1470 = load float*, float** %2, align 8
	  %1466 = load i32, i32* %z, align 4
	  %1465 = getelementptr inbounds float, float* %1464, i64 %1463
	  %1464 = load float*, float** %2, align 8
	  %1460 = load i32, i32* %z, align 4
	  %1459 = getelementptr inbounds float, float* %1458, i64 %1457
	  %1458 = load float*, float** %2, align 8
	  %1454 = load i32, i32* %z, align 4
	  %1453 = fptrunc double %1451 to float
	  %1455 = add nsw i32 960, %1454
	  %1456 = srem i32 %1455, 128
	  %1457 = sext i32 %1456 to i64
	  store float %1453, float* %1459, align 4
	  %1461 = add nsw i32 968, %1460
	  %1462 = srem i32 %1461, 128
	  %1463 = sext i32 %1462 to i64
	  store float 0x42D6BCC420000000, float* %1465, align 4
	  %1467 = add nsw i32 976, %1466
	  %1468 = srem i32 %1467, 128
	  %1469 = sext i32 %1468 to i64
	  store float 0x42D6BCC420000000, float* %1471, align 4
	  %1473 = add nsw i32 984, %1472
	  %1474 = srem i32 %1473, 128
	  %1475 = sext i32 %1474 to i64
	  store float 0x42B2309CE0000000, float* %1477, align 4
	  %1479 = add nsw i32 992, %1478
	  %1480 = srem i32 %1479, 128
	  %1481 = sext i32 %1480 to i64
	  store float 0x42A2309CE0000000, float* %1483, align 4
	  %1485 = fmul float 0x3FAEB851E0000000, %1484
	  %1486 = fsub float 0x4040B70E00000000, %1485
	  %1488 = fmul float 0x40B0B55780000000, %1487
	  %1489 = fsub float %1486, %1488
	  %1490 = fpext float %1489 to double
	  %1492 = call double @exp(double %1490) #4
	  %1504 = load float, float* %f, align 4
	  %1501 = load float, float* %e, align 4
	  %1500 = getelementptr inbounds float, float* %1499, i64 %1498
	  %1499 = load float*, float** %2, align 8
	  %1495 = load i32, i32* %z, align 4
	  %1494 = fptrunc double %1492 to float
	  %1496 = add nsw i32 1000, %1495
	  %1497 = srem i32 %1496, 128
	  %1498 = sext i32 %1497 to i64
	  store float %1494, float* %1500, align 4
	  %1502 = fmul float 8.800000e+08, %1501
	  %1503 = fadd float 8.800000e+08, %1502
	  %1505 = fmul float 8.800000e+08, %1504
	  %1506 = fsub float %1503, %1505
	  %1507 = fpext float %1506 to double
	  %1509 = call double @exp(double %1507) #4
	  %1518 = load float, float* %f, align 4
	  %1517 = getelementptr inbounds float, float* %1516, i64 %1515
	  %1516 = load float*, float** %2, align 8
	  %1512 = load i32, i32* %z, align 4
	  %1511 = fptrunc double %1509 to float
	  %1513 = add nsw i32 0, %1512
	  %1514 = srem i32 %1513, 128
	  %1515 = sext i32 %1514 to i64
	  store float %1511, float* %1517, align 4
	  %1519 = fmul float 8.800000e+08, %1518
	  %1520 = fsub float 8.800000e+08, %1519
	  %1521 = fpext float %1520 to double
	  %1523 = call double @exp(double %1521) #4
	  %1532 = load float, float* %f, align 4
	  %1531 = getelementptr inbounds float, float* %1530, i64 %1529
	  %1530 = load float*, float** %2, align 8
	  %1526 = load i32, i32* %z, align 4
	  %1525 = fptrunc double %1523 to float
	  %1527 = add nsw i32 0, %1526
	  %1528 = srem i32 %1527, 128
	  %1529 = sext i32 %1528 to i64
	  store float %1525, float* %1531, align 4
	  %1533 = fmul float 0xC08F737780000000, %1532
	  %1534 = fpext float %1533 to double
	  %1536 = call double @exp(double %1534) #4
	  %1572 = load float, float* %f, align 4
	  %1569 = load float, float* %e, align 4
	  %1568 = getelementptr inbounds float, float* %1567, i64 %1566
	  %1567 = load float*, float** %2, align 8
	  %1563 = load i32, i32* %z, align 4
	  %1559 = load float, float* %h, align 4
	  %1558 = getelementptr inbounds float, float* %1557, i64 %1556
	  %1557 = load float*, float** %2, align 8
	  %1553 = load i32, i32* %z, align 4
	  %1549 = load float, float* %h, align 4
	  %1548 = getelementptr inbounds float, float* %1547, i64 %1546
	  %1547 = load float*, float** %2, align 8
	  %1543 = load i32, i32* %z, align 4
	  %1539 = load float, float* %h, align 4
	  %1538 = fptrunc double %1536 to float
	  store float %1538, float* %h, align 4
	  %1540 = fpext float %1539 to double
	  %1541 = fmul double 8.800000e+08, %1540
	  %1542 = fptrunc double %1541 to float
	  %1544 = add nsw i32 0, %1543
	  %1545 = srem i32 %1544, 128
	  %1546 = sext i32 %1545 to i64
	  store float %1542, float* %1548, align 4
	  %1550 = fpext float %1549 to double
	  %1551 = fmul double 8.000000e+08, %1550
	  %1552 = fptrunc double %1551 to float
	  %1554 = add nsw i32 0, %1553
	  %1555 = srem i32 %1554, 128
	  %1556 = sext i32 %1555 to i64
	  store float %1552, float* %1558, align 4
	  %1560 = fpext float %1559 to double
	  %1561 = fmul double 8.000000e+08, %1560
	  %1562 = fptrunc double %1561 to float
	  %1564 = add nsw i32 0, %1563
	  %1565 = srem i32 %1564, 128
	  %1566 = sext i32 %1565 to i64
	  store float %1562, float* %1568, align 4
	  %1570 = fmul float 0x3E779F5060000000, %1569
	  %1571 = fadd float 8.800000e+08, %1570
	  %1573 = fmul float 8.800000e+08, %1572
	  %1574 = fsub float %1571, %1573
	  %1575 = fpext float %1574 to double
	  %1577 = call double @exp(double %1575) #4
	  %1619 = load float, float* %f, align 4
	  %1616 = load float, float* %e, align 4
	  %1615 = getelementptr inbounds float, float* %1614, i64 %1613
	  %1614 = load float*, float** %2, align 8
	  %1610 = load i32, i32* %z, align 4
	  %1609 = getelementptr inbounds float, float* %1608, i64 %1607
	  %1608 = load float*, float** %2, align 8
	  %1604 = load i32, i32* %z, align 4
	  %1603 = getelementptr inbounds float, float* %1602, i64 %1601
	  %1602 = load float*, float** %2, align 8
	  %1598 = load i32, i32* %z, align 4
	  %1597 = getelementptr inbounds float, float* %1596, i64 %1595
	  %1596 = load float*, float** %2, align 8
	  %1592 = load i32, i32* %z, align 4
	  %1591 = getelementptr inbounds float, float* %1590, i64 %1589
	  %1590 = load float*, float** %2, align 8
	  %1586 = load i32, i32* %z, align 4
	  %1585 = getelementptr inbounds float, float* %1584, i64 %1583
	  %1584 = load float*, float** %2, align 8
	  %1580 = load i32, i32* %z, align 4
	  %1579 = fptrunc double %1577 to float
	  %1581 = add nsw i32 0, %1580
	  %1582 = srem i32 %1581, 128
	  %1583 = sext i32 %1582 to i64
	  store float %1579, float* %1585, align 4
	  %1587 = add nsw i32 0, %1586
	  %1588 = srem i32 %1587, 128
	  %1589 = sext i32 %1588 to i64
	  store float 8.000000e+08, float* %1591, align 4
	  %1593 = add nsw i32 0, %1592
	  %1594 = srem i32 %1593, 128
	  %1595 = sext i32 %1594 to i64
	  store float 8.000000e+08, float* %1597, align 4
	  %1599 = add nsw i32 0, %1598
	  %1600 = srem i32 %1599, 128
	  %1601 = sext i32 %1600 to i64
	  store float 8.800000e+08, float* %1603, align 4
	  %1605 = add nsw i32 0, %1604
	  %1606 = srem i32 %1605, 128
	  %1607 = sext i32 %1606 to i64
	  store float 8.800000e+08, float* %1609, align 4
	  %1611 = add nsw i32 0, %1610
	  %1612 = srem i32 %1611, 128
	  %1613 = sext i32 %1612 to i64
	  store float 8.800000e+08, float* %1615, align 4
	  %1617 = fmul float 8.800000e+08, %1616
	  %1618 = fadd float 8.800000e+08, %1617
	  %1620 = fmul float 8.800000e+08, %1619
	  %1621 = fadd float %1618, %1620
	  %1622 = fpext float %1621 to double
	  %1624 = call double @exp(double %1622) #4
	  %1636 = load float, float* %f, align 4
	  %1633 = load float, float* %e, align 4
	  %1632 = getelementptr inbounds float, float* %1631, i64 %1630
	  %1631 = load float*, float** %2, align 8
	  %1627 = load i32, i32* %z, align 4
	  %1626 = fptrunc double %1624 to float
	  %1628 = add nsw i32 0, %1627
	  %1629 = srem i32 %1628, 128
	  %1630 = sext i32 %1629 to i64
	  store float %1626, float* %1632, align 4
	  %1634 = fmul float 0x3E779F5060000000, %1633
	  %1635 = fadd float 8.800000e+08, %1634
	  %1637 = fmul float 8.800000e+08, %1636
	  %1638 = fsub float %1635, %1637
	  %1639 = fpext float %1638 to double
	  %1641 = call double @exp(double %1639) #4
	  %1653 = load float, float* %f, align 4
	  %1650 = load float, float* %e, align 4
	  %1649 = getelementptr inbounds float, float* %1648, i64 %1647
	  %1648 = load float*, float** %2, align 8
	  %1644 = load i32, i32* %z, align 4
	  %1643 = fptrunc double %1641 to float
	  %1645 = add nsw i32 0, %1644
	  %1646 = srem i32 %1645, 128
	  %1647 = sext i32 %1646 to i64
	  store float %1643, float* %1649, align 4
	  %1651 = fmul float 8.800000e+08, %1650
	  %1652 = fsub float 8.800000e+08, %1651
	  %1654 = fmul float 8.800000e+08, %1653
	  %1655 = fsub float %1652, %1654
	  %1656 = fpext float %1655 to double
	  %1658 = call double @exp(double %1656) #4
	  %1673 = load float, float* %f, align 4
	  %1672 = getelementptr inbounds float, float* %1671, i64 %1670
	  %1671 = load float*, float** %2, align 8
	  %1667 = load i32, i32* %z, align 4
	  %1666 = getelementptr inbounds float, float* %1665, i64 %1664
	  %1665 = load float*, float** %2, align 8
	  %1661 = load i32, i32* %z, align 4
	  %1660 = fptrunc double %1658 to float
	  %1662 = add nsw i32 0, %1661
	  %1663 = srem i32 %1662, 128
	  %1664 = sext i32 %1663 to i64
	  store float %1660, float* %1666, align 4
	  %1668 = add nsw i32 0, %1667
	  %1669 = srem i32 %1668, 128
	  %1670 = sext i32 %1669 to i64
	  store float 8.000000e+08, float* %1672, align 4
	  %1674 = fmul float 8.800000e+08, %1673
	  %1675 = fadd float 8.800000e+08, %1674
	  %1676 = fpext float %1675 to double
	  %1678 = call double @exp(double %1676) #4
	  %1708 = load float, float* %f, align 4
	  %1705 = load float, float* %e, align 4
	  %1704 = getelementptr inbounds float, float* %1703, i64 %1702
	  %1703 = load float*, float** %2, align 8
	  %1699 = load i32, i32* %z, align 4
	  %1698 = getelementptr inbounds float, float* %1697, i64 %1696
	  %1697 = load float*, float** %2, align 8
	  %1693 = load i32, i32* %z, align 4
	  %1692 = getelementptr inbounds float, float* %1691, i64 %1690
	  %1691 = load float*, float** %2, align 8
	  %1687 = load i32, i32* %z, align 4
	  %1686 = getelementptr inbounds float, float* %1685, i64 %1684
	  %1685 = load float*, float** %2, align 8
	  %1681 = load i32, i32* %z, align 4
	  %1680 = fptrunc double %1678 to float
	  %1682 = add nsw i32 0, %1681
	  %1683 = srem i32 %1682, 128
	  %1684 = sext i32 %1683 to i64
	  store float %1680, float* %1686, align 4
	  %1688 = add nsw i32 0, %1687
	  %1689 = srem i32 %1688, 128
	  %1690 = sext i32 %1689 to i64
	  store float 8.800000e+08, float* %1692, align 4
	  %1694 = add nsw i32 0, %1693
	  %1695 = srem i32 %1694, 128
	  %1696 = sext i32 %1695 to i64
	  store float 8.800000e+08, float* %1698, align 4
	  %1700 = add nsw i32 0, %1699
	  %1701 = srem i32 %1700, 128
	  %1702 = sext i32 %1701 to i64
	  store float 8.800000e+08, float* %1704, align 4
	  %1706 = fmul float 8.800000e+08, %1705
	  %1707 = fsub float 8.800000e+08, %1706
	  %1709 = fmul float 8.800000e+08, %1708
	  %1710 = fsub float %1707, %1709
	  %1711 = fpext float %1710 to double
	  %1713 = call double @exp(double %1711) #4
	  %1725 = load float, float* %f, align 4
	  %1722 = load float, float* %e, align 4
	  %1721 = getelementptr inbounds float, float* %1720, i64 %1719
	  %1720 = load float*, float** %2, align 8
	  %1716 = load i32, i32* %z, align 4
	  %1715 = fptrunc double %1713 to float
	  %1717 = add nsw i32 0, %1716
	  %1718 = srem i32 %1717, 128
	  %1719 = sext i32 %1718 to i64
	  store float %1715, float* %1721, align 4
	  %1723 = fmul float 8.800000e+08, %1722
	  %1724 = fsub float 8.800000e+08, %1723
	  %1726 = fmul float 8.800000e+08, %1725
	  %1727 = fsub float %1724, %1726
	  %1728 = fpext float %1727 to double
	  %1730 = call double @exp(double %1728) #4
	  %1751 = load float, float* %f, align 4
	  %1750 = getelementptr inbounds float, float* %1749, i64 %1748
	  %1749 = load float*, float** %2, align 8
	  %1745 = load i32, i32* %z, align 4
	  %1744 = getelementptr inbounds float, float* %1743, i64 %1742
	  %1743 = load float*, float** %2, align 8
	  %1739 = load i32, i32* %z, align 4
	  %1738 = getelementptr inbounds float, float* %1737, i64 %1736
	  %1737 = load float*, float** %2, align 8
	  %1733 = load i32, i32* %z, align 4
	  %1732 = fptrunc double %1730 to float
	  %1734 = add nsw i32 0, %1733
	  %1735 = srem i32 %1734, 128
	  %1736 = sext i32 %1735 to i64
	  store float %1732, float* %1738, align 4
	  %1740 = add nsw i32 0, %1739
	  %1741 = srem i32 %1740, 128
	  %1742 = sext i32 %1741 to i64
	  store float 8.000000e+08, float* %1744, align 4
	  %1746 = add nsw i32 0, %1745
	  %1747 = srem i32 %1746, 128
	  %1748 = sext i32 %1747 to i64
	  store float 8.000000e+08, float* %1750, align 4
	  %1752 = fmul float 0xC09F737780000000, %1751
	  %1753 = fpext float %1752 to double
	  %1755 = call double @exp(double %1753) #4
	  %1793 = load float, float* %f, align 4
	  %1790 = load float, float* %e, align 4
	  %1789 = getelementptr inbounds float, float* %1788, i64 %1787
	  %1788 = load float*, float** %2, align 8
	  %1784 = load i32, i32* %z, align 4
	  %1783 = getelementptr inbounds float, float* %1782, i64 %1781
	  %1782 = load float*, float** %2, align 8
	  %1778 = load i32, i32* %z, align 4
	  %1777 = getelementptr inbounds float, float* %1776, i64 %1775
	  %1776 = load float*, float** %2, align 8
	  %1772 = load i32, i32* %z, align 4
	  %1768 = load float, float* %h, align 4
	  %1767 = getelementptr inbounds float, float* %1766, i64 %1765
	  %1766 = load float*, float** %2, align 8
	  %1762 = load i32, i32* %z, align 4
	  %1758 = load float, float* %h, align 4
	  %1757 = fptrunc double %1755 to float
	  store float %1757, float* %h, align 4
	  %1759 = fpext float %1758 to double
	  %1760 = fmul double 8.000000e+08, %1759
	  %1761 = fptrunc double %1760 to float
	  %1763 = add nsw i32 0, %1762
	  %1764 = srem i32 %1763, 128
	  %1765 = sext i32 %1764 to i64
	  store float %1761, float* %1767, align 4
	  %1769 = fpext float %1768 to double
	  %1770 = fmul double 8.000000e+08, %1769
	  %1771 = fptrunc double %1770 to float
	  %1773 = add nsw i32 0, %1772
	  %1774 = srem i32 %1773, 128
	  %1775 = sext i32 %1774 to i64
	  store float %1771, float* %1777, align 4
	  %1779 = add nsw i32 0, %1778
	  %1780 = srem i32 %1779, 128
	  %1781 = sext i32 %1780 to i64
	  store float 8.800000e+08, float* %1783, align 4
	  %1785 = add nsw i32 0, %1784
	  %1786 = srem i32 %1785, 128
	  %1787 = sext i32 %1786 to i64
	  store float 8.800000e+08, float* %1789, align 4
	  %1791 = fmul float 0x3E779F5060000000, %1790
	  %1792 = fadd float 8.800000e+08, %1791
	  %1794 = fmul float 8.800000e+08, %1793
	  %1795 = fsub float %1792, %1794
	  %1796 = fpext float %1795 to double
	  %1798 = call double @exp(double %1796) #4
	  %1810 = load float, float* %f, align 4
	  %1807 = load float, float* %e, align 4
	  %1806 = getelementptr inbounds float, float* %1805, i64 %1804
	  %1805 = load float*, float** %2, align 8
	  %1801 = load i32, i32* %z, align 4
	  %1800 = fptrunc double %1798 to float
	  %1802 = add nsw i32 0, %1801
	  %1803 = srem i32 %1802, 128
	  %1804 = sext i32 %1803 to i64
	  store float %1800, float* %1806, align 4
	  %1808 = fmul float 0x3E779F5060000000, %1807
	  %1809 = fadd float 8.800000e+08, %1808
	  %1811 = fmul float 8.800000e+08, %1810
	  %1812 = fsub float %1809, %1811
	  %1813 = fpext float %1812 to double
	  %1815 = call double @exp(double %1813) #4
	  %1827 = load float, float* %f, align 4
	  %1824 = load float, float* %e, align 4
	  %1823 = getelementptr inbounds float, float* %1822, i64 %1821
	  %1822 = load float*, float** %2, align 8
	  %1818 = load i32, i32* %z, align 4
	  %1817 = fptrunc double %1815 to float
	  %1819 = add nsw i32 0, %1818
	  %1820 = srem i32 %1819, 128
	  %1821 = sext i32 %1820 to i64
	  store float %1817, float* %1823, align 4
	  %1825 = fmul float 8.800000e+08, %1824
	  %1826 = fadd float 8.800000e+08, %1825
	  %1828 = fmul float 8.800000e+08, %1827
	  %1829 = fsub float %1826, %1828
	  %1830 = fpext float %1829 to double
	  %1832 = call double @exp(double %1830) #4
	  %1844 = load float, float* %f, align 4
	  %1841 = load float, float* %e, align 4
	  %1840 = getelementptr inbounds float, float* %1839, i64 %1838
	  %1839 = load float*, float** %2, align 8
	  %1835 = load i32, i32* %z, align 4
	  %1834 = fptrunc double %1832 to float
	  %1836 = add nsw i32 0, %1835
	  %1837 = srem i32 %1836, 128
	  %1838 = sext i32 %1837 to i64
	  store float %1834, float* %1840, align 4
	  %1842 = fmul float 8.800000e+08, %1841
	  %1843 = fadd float 8.800000e+08, %1842
	  %1845 = fmul float 8.800000e+08, %1844
	  %1846 = fsub float %1843, %1845
	  %1847 = fpext float %1846 to double
	  %1849 = call double @exp(double %1847) #4
	  %1860 = load float, float* %f, align 4
	  %1858 = load float, float* %e, align 4
	  %1857 = getelementptr inbounds float, float* %1856, i64 %1855
	  %1856 = load float*, float** %2, align 8
	  %1852 = load i32, i32* %z, align 4
	  %1851 = fptrunc double %1849 to float
	  %1853 = add nsw i32 0, %1852
	  %1854 = srem i32 %1853, 128
	  %1855 = sext i32 %1854 to i64
	  store float %1851, float* %1857, align 4
	  %1859 = fmul float 0x3FFD47AE20000000, %1858
	  %1861 = fmul float 0x405BAD4A60000000, %1860
	  %1862 = fsub float %1859, %1861
	  %1863 = fpext float %1862 to double
	  %1865 = call double @exp(double %1863) #4
	  %1891 = load float, float* %f, align 4
	  %1888 = load float, float* %e, align 4
	  %1887 = getelementptr inbounds float, float* %1886, i64 %1885
	  %1886 = load float*, float** %2, align 8
	  %1882 = load i32, i32* %z, align 4
	  %1878 = load float, float* %h, align 4
	  %1877 = getelementptr inbounds float, float* %1876, i64 %1875
	  %1876 = load float*, float** %2, align 8
	  %1872 = load i32, i32* %z, align 4
	  %1868 = load float, float* %h, align 4
	  %1867 = fptrunc double %1865 to float
	  store float %1867, float* %h, align 4
	  %1869 = fpext float %1868 to double
	  %1870 = fmul double 8.800000e+08, %1869
	  %1871 = fptrunc double %1870 to float
	  %1873 = add nsw i32 0, %1872
	  %1874 = srem i32 %1873, 128
	  %1875 = sext i32 %1874 to i64
	  store float %1871, float* %1877, align 4
	  %1879 = fpext float %1878 to double
	  %1880 = fmul double 8.800000e+08, %1879
	  %1881 = fptrunc double %1880 to float
	  %1883 = add nsw i32 0, %1882
	  %1884 = srem i32 %1883, 128
	  %1885 = sext i32 %1884 to i64
	  store float %1881, float* %1887, align 4
	  %1889 = fmul float 8.000000e+08, %1888
	  %1890 = fadd float 8.800000e+08, %1889
	  %1892 = fmul float 8.800000e+08, %1891
	  %1893 = fsub float %1890, %1892
	  %1894 = fpext float %1893 to double
	  %1896 = call double @exp(double %1894) #4
	  %1905 = load float, float* %f, align 4
	  %1904 = getelementptr inbounds float, float* %1903, i64 %1902
	  %1903 = load float*, float** %2, align 8
	  %1899 = load i32, i32* %z, align 4
	  %1898 = fptrunc double %1896 to float
	  %1900 = add nsw i32 0, %1899
	  %1901 = srem i32 %1900, 128
	  %1902 = sext i32 %1901 to i64
	  store float %1898, float* %1904, align 4
	  %1906 = fmul float 8.800000e+08, %1905
	  %1907 = fsub float 8.800000e+08, %1906
	  %1908 = fpext float %1907 to double
	  %1910 = call double @exp(double %1908) #4
	  %1919 = load float, float* %f, align 4
	  %1918 = getelementptr inbounds float, float* %1917, i64 %1916
	  %1917 = load float*, float** %2, align 8
	  %1913 = load i32, i32* %z, align 4
	  %1912 = fptrunc double %1910 to float
	  %1914 = add nsw i32 0, %1913
	  %1915 = srem i32 %1914, 128
	  %1916 = sext i32 %1915 to i64
	  store float %1912, float* %1918, align 4
	  %1920 = fmul float 8.800000e+08, %1919
	  %1921 = fsub float 8.800000e+08, %1920
	  %1922 = fpext float %1921 to double
	  %1924 = call double @exp(double %1922) #4
	  %1936 = load float, float* %f, align 4
	  %1933 = load float, float* %e, align 4
	  %1932 = getelementptr inbounds float, float* %1931, i64 %1930
	  %1931 = load float*, float** %2, align 8
	  %1927 = load i32, i32* %z, align 4
	  %1926 = fptrunc double %1924 to float
	  %1928 = add nsw i32 0, %1927
	  %1929 = srem i32 %1928, 128
	  %1930 = sext i32 %1929 to i64
	  store float %1926, float* %1932, align 4
	  %1934 = fmul float 8.000000e+08, %1933
	  %1935 = fadd float 8.800000e+08, %1934
	  %1937 = fmul float 8.800000e+08, %1936
	  %1938 = fsub float %1935, %1937
	  %1939 = fpext float %1938 to double
	  %1941 = call double @exp(double %1939) #4
	  %1950 = load float, float* %f, align 4
	  %1949 = getelementptr inbounds float, float* %1948, i64 %1947
	  %1948 = load float*, float** %2, align 8
	  %1944 = load i32, i32* %z, align 4
	  %1943 = fptrunc double %1941 to float
	  %1945 = add nsw i32 0, %1944
	  %1946 = srem i32 %1945, 128
	  %1947 = sext i32 %1946 to i64
	  store float %1943, float* %1949, align 4
	  %1951 = fmul float 8.800000e+08, %1950
	  %1952 = fsub float 8.800000e+08, %1951
	  %1953 = fpext float %1952 to double
	  %1955 = call double @exp(double %1953) #4
	  %1979 = load float, float* %f, align 4
	  %1976 = load float, float* %e, align 4
	  %1975 = getelementptr inbounds float, float* %1974, i64 %1973
	  %1974 = load float*, float** %2, align 8
	  %1970 = load i32, i32* %z, align 4
	  %1969 = getelementptr inbounds float, float* %1968, i64 %1967
	  %1968 = load float*, float** %2, align 8
	  %1964 = load i32, i32* %z, align 4
	  %1963 = getelementptr inbounds float, float* %1962, i64 %1961
	  %1962 = load float*, float** %2, align 8
	  %1958 = load i32, i32* %z, align 4
	  %1957 = fptrunc double %1955 to float
	  %1959 = add nsw i32 0, %1958
	  %1960 = srem i32 %1959, 128
	  %1961 = sext i32 %1960 to i64
	  store float %1957, float* %1963, align 4
	  %1965 = add nsw i32 0, %1964
	  %1966 = srem i32 %1965, 128
	  %1967 = sext i32 %1966 to i64
	  store float 8.000000e+08, float* %1969, align 4
	  %1971 = add nsw i32 0, %1970
	  %1972 = srem i32 %1971, 128
	  %1973 = sext i32 %1972 to i64
	  store float 8.000000e+08, float* %1975, align 4
	  %1977 = fmul float 8.000000e+08, %1976
	  %1978 = fadd float 8.800000e+08, %1977
	  %1980 = fmul float 8.800000e+08, %1979
	  %1981 = fsub float %1978, %1980
	  %1982 = fpext float %1981 to double
	  %1984 = call double @exp(double %1982) #4
	  %1993 = load float, float* %f, align 4
	  %1992 = getelementptr inbounds float, float* %1991, i64 %1990
	  %1991 = load float*, float** %2, align 8
	  %1987 = load i32, i32* %z, align 4
	  %1986 = fptrunc double %1984 to float
	  %1988 = add nsw i32 0, %1987
	  %1989 = srem i32 %1988, 128
	  %1990 = sext i32 %1989 to i64
	  store float %1986, float* %1992, align 4
	  %1994 = fmul float 8.800000e+08, %1993
	  %1995 = fsub float 8.800000e+08, %1994
	  %1996 = fpext float %1995 to double
	  %1998 = call double @exp(double %1996) #4
	  %2010 = load float, float* %f, align 4
	  %2007 = load float, float* %e, align 4
	  %2006 = getelementptr inbounds float, float* %2005, i64 %2004
	  %2005 = load float*, float** %2, align 8
	  %2001 = load i32, i32* %z, align 4
	  %2000 = fptrunc double %1998 to float
	  %2002 = add nsw i32 0, %2001
	  %2003 = srem i32 %2002, 128
	  %2004 = sext i32 %2003 to i64
	  store float %2000, float* %2006, align 4
	  %2008 = fmul float 0x3E779F5060000000, %2007
	  %2009 = fsub float 8.800000e+08, %2008
	  %2011 = fmul float 8.800000e+08, %2010
	  %2012 = fsub float %2009, %2011
	  %2013 = fpext float %2012 to double
	  %2015 = call double @exp(double %2013) #4
	  %2066 = load float, float* %f, align 4
	  %2065 = getelementptr inbounds float, float* %2064, i64 %2063
	  %2064 = load float*, float** %2, align 8
	  %2060 = load i32, i32* %z, align 4
	  %2059 = getelementptr inbounds float, float* %2058, i64 %2057
	  %2058 = load float*, float** %2, align 8
	  %2054 = load i32, i32* %z, align 4
	  %2053 = getelementptr inbounds float, float* %2052, i64 %2051
	  %2052 = load float*, float** %2, align 8
	  %2048 = load i32, i32* %z, align 4
	  %2047 = getelementptr inbounds float, float* %2046, i64 %2045
	  %2046 = load float*, float** %2, align 8
	  %2042 = load i32, i32* %z, align 4
	  %2041 = getelementptr inbounds float, float* %2040, i64 %2039
	  %2040 = load float*, float** %2, align 8
	  %2036 = load i32, i32* %z, align 4
	  %2035 = getelementptr inbounds float, float* %2034, i64 %2033
	  %2034 = load float*, float** %2, align 8
	  %2030 = load i32, i32* %z, align 4
	  %2029 = getelementptr inbounds float, float* %2028, i64 %2027
	  %2028 = load float*, float** %2, align 8
	  %2024 = load i32, i32* %z, align 4
	  %2023 = getelementptr inbounds float, float* %2022, i64 %2021
	  %2022 = load float*, float** %2, align 8
	  %2018 = load i32, i32* %z, align 4
	  %2017 = fptrunc double %2015 to float
	  %2019 = add nsw i32 0, %2018
	  %2020 = srem i32 %2019, 128
	  %2021 = sext i32 %2020 to i64
	  store float %2017, float* %2023, align 4
	  %2025 = add nsw i32 0, %2024
	  %2026 = srem i32 %2025, 128
	  %2027 = sext i32 %2026 to i64
	  store float 8.000000e+08, float* %2029, align 4
	  %2031 = add nsw i32 0, %2030
	  %2032 = srem i32 %2031, 128
	  %2033 = sext i32 %2032 to i64
	  store float 8.800000e+08, float* %2035, align 4
	  %2037 = add nsw i32 0, %2036
	  %2038 = srem i32 %2037, 128
	  %2039 = sext i32 %2038 to i64
	  store float 8.800000e+08, float* %2041, align 4
	  %2043 = add nsw i32 0, %2042
	  %2044 = srem i32 %2043, 128
	  %2045 = sext i32 %2044 to i64
	  store float 8.000000e+08, float* %2047, align 4
	  %2049 = add nsw i32 0, %2048
	  %2050 = srem i32 %2049, 128
	  %2051 = sext i32 %2050 to i64
	  store float 8.000000e+08, float* %2053, align 4
	  %2055 = add nsw i32 0, %2054
	  %2056 = srem i32 %2055, 128
	  %2057 = sext i32 %2056 to i64
	  store float 8.000000e+08, float* %2059, align 4
	  %2061 = add nsw i32 0, %2060
	  %2062 = srem i32 %2061, 128
	  %2063 = sext i32 %2062 to i64
	  store float 8.800000e+08, float* %2065, align 4
	  %2067 = fmul float 8.800000e+08, %2066
	  %2068 = fsub float 8.800000e+08, %2067
	  %2069 = fpext float %2068 to double
	  %2071 = call double @exp(double %2069) #4
	  %2089 = load float, float* %f, align 4
	  %2086 = load float, float* %e, align 4
	  %2085 = getelementptr inbounds float, float* %2084, i64 %2083
	  %2084 = load float*, float** %2, align 8
	  %2080 = load i32, i32* %z, align 4
	  %2079 = getelementptr inbounds float, float* %2078, i64 %2077
	  %2078 = load float*, float** %2, align 8
	  %2074 = load i32, i32* %z, align 4
	  %2073 = fptrunc double %2071 to float
	  %2075 = add nsw i32 0, %2074
	  %2076 = srem i32 %2075, 128
	  %2077 = sext i32 %2076 to i64
	  store float %2073, float* %2079, align 4
	  %2081 = add nsw i32 0, %2080
	  %2082 = srem i32 %2081, 128
	  %2083 = sext i32 %2082 to i64
	  store float 8.800000e+08, float* %2085, align 4
	  %2087 = fmul float 8.800000e+08, %2086
	  %2088 = fadd float 8.800000e+08, %2087
	  %2090 = fmul float 8.800000e+08, %2089
	  %2091 = fsub float %2088, %2090
	  %2092 = fpext float %2091 to double
	  %2094 = call double @exp(double %2092) #4
	  %2106 = load float, float* %f, align 4
	  %2103 = load float, float* %e, align 4
	  %2102 = getelementptr inbounds float, float* %2101, i64 %2100
	  %2101 = load float*, float** %2, align 8
	  %2097 = load i32, i32* %z, align 4
	  %2096 = fptrunc double %2094 to float
	  %2098 = add nsw i32 0, %2097
	  %2099 = srem i32 %2098, 128
	  %2100 = sext i32 %2099 to i64
	  store float %2096, float* %2102, align 4
	  %2104 = fmul float 8.800000e+08, %2103
	  %2105 = fadd float 8.800000e+08, %2104
	  %2107 = fmul float 8.800000e+08, %2106
	  %2108 = fsub float %2105, %2107
	  %2109 = fpext float %2108 to double
	  %2111 = call double @exp(double %2109) #4
	  %2123 = load float, float* %f, align 4
	  %2120 = load float, float* %e, align 4
	  %2119 = getelementptr inbounds float, float* %2118, i64 %2117
	  %2118 = load float*, float** %2, align 8
	  %2114 = load i32, i32* %z, align 4
	  %2113 = fptrunc double %2111 to float
	  %2115 = add nsw i32 0, %2114
	  %2116 = srem i32 %2115, 128
	  %2117 = sext i32 %2116 to i64
	  store float %2113, float* %2119, align 4
	  %2121 = fmul float 8.800000e+08, %2120
	  %2122 = fadd float 8.800000e+08, %2121
	  %2124 = fmul float 8.800000e+08, %2123
	  %2125 = fsub float %2122, %2124
	  %2126 = fpext float %2125 to double
	  %2128 = call double @exp(double %2126) #4
	  %2137 = load float, float* %f, align 4
	  %2136 = getelementptr inbounds float, float* %2135, i64 %2134
	  %2135 = load float*, float** %2, align 8
	  %2131 = load i32, i32* %z, align 4
	  %2130 = fptrunc double %2128 to float
	  %2132 = add nsw i32 0, %2131
	  %2133 = srem i32 %2132, 128
	  %2134 = sext i32 %2133 to i64
	  store float %2130, float* %2136, align 4
	  %2138 = fmul float 8.800000e+08, %2137
	  %2139 = fadd float 8.800000e+08, %2138
	  %2140 = fpext float %2139 to double
	  %2142 = call double @exp(double %2140) #4
	  %2154 = load float, float* %f, align 4
	  %2151 = load float, float* %e, align 4
	  %2150 = getelementptr inbounds float, float* %2149, i64 %2148
	  %2149 = load float*, float** %2, align 8
	  %2145 = load i32, i32* %z, align 4
	  %2144 = fptrunc double %2142 to float
	  %2146 = add nsw i32 0, %2145
	  %2147 = srem i32 %2146, 128
	  %2148 = sext i32 %2147 to i64
	  store float %2144, float* %2150, align 4
	  %2152 = fmul float 8.800000e+08, %2151
	  %2153 = fadd float 8.800000e+08, %2152
	  %2155 = fmul float 8.800000e+08, %2154
	  %2156 = fsub float %2153, %2155
	  %2157 = fpext float %2156 to double
	  %2159 = call double @exp(double %2157) #4
	  %2192 = load float, float* %f, align 4
	  %2191 = getelementptr inbounds float, float* %2190, i64 %2189
	  %2190 = load float*, float** %2, align 8
	  %2186 = load i32, i32* %z, align 4
	  %2185 = getelementptr inbounds float, float* %2184, i64 %2183
	  %2184 = load float*, float** %2, align 8
	  %2180 = load i32, i32* %z, align 4
	  %2179 = getelementptr inbounds float, float* %2178, i64 %2177
	  %2178 = load float*, float** %2, align 8
	  %2174 = load i32, i32* %z, align 4
	  %2173 = getelementptr inbounds float, float* %2172, i64 %2171
	  %2172 = load float*, float** %2, align 8
	  %2168 = load i32, i32* %z, align 4
	  %2167 = getelementptr inbounds float, float* %2166, i64 %2165
	  %2166 = load float*, float** %2, align 8
	  %2162 = load i32, i32* %z, align 4
	  %2161 = fptrunc double %2159 to float
	  %2163 = add nsw i32 0, %2162
	  %2164 = srem i32 %2163, 128
	  %2165 = sext i32 %2164 to i64
	  store float %2161, float* %2167, align 4
	  %2169 = add nsw i32 0, %2168
	  %2170 = srem i32 %2169, 128
	  %2171 = sext i32 %2170 to i64
	  store float 8.000000e+08, float* %2173, align 4
	  %2175 = add nsw i32 0, %2174
	  %2176 = srem i32 %2175, 128
	  %2177 = sext i32 %2176 to i64
	  store float 8.800000e+08, float* %2179, align 4
	  %2181 = add nsw i32 0, %2180
	  %2182 = srem i32 %2181, 128
	  %2183 = sext i32 %2182 to i64
	  store float 8.800000e+08, float* %2185, align 4
	  %2187 = add nsw i32 0, %2186
	  %2188 = srem i32 %2187, 128
	  %2189 = sext i32 %2188 to i64
	  store float 8.000000e+08, float* %2191, align 4
	  %2193 = fmul float 8.800000e+08, %2192
	  %2194 = fsub float 8.800000e+08, %2193
	  %2195 = fpext float %2194 to double
	  %2197 = call double @exp(double %2195) #4
	  %2209 = load float, float* %f, align 4
	  %2206 = load float, float* %e, align 4
	  %2205 = getelementptr inbounds float, float* %2204, i64 %2203
	  %2204 = load float*, float** %2, align 8
	  %2200 = load i32, i32* %z, align 4
	  %2199 = fptrunc double %2197 to float
	  %2201 = add nsw i32 0, %2200
	  %2202 = srem i32 %2201, 128
	  %2203 = sext i32 %2202 to i64
	  store float %2199, float* %2205, align 4
	  %2207 = fmul float 8.800000e+08, %2206
	  %2208 = fsub float 8.800000e+08, %2207
	  %2210 = fmul float 8.800000e+08, %2209
	  %2211 = fsub float %2208, %2210
	  %2212 = fpext float %2211 to double
	  %2214 = call double @exp(double %2212) #4
	  %2226 = load float, float* %f, align 4
	  %2223 = load float, float* %e, align 4
	  %2222 = getelementptr inbounds float, float* %2221, i64 %2220
	  %2221 = load float*, float** %2, align 8
	  %2217 = load i32, i32* %z, align 4
	  %2216 = fptrunc double %2214 to float
	  %2218 = add nsw i32 0, %2217
	  %2219 = srem i32 %2218, 128
	  %2220 = sext i32 %2219 to i64
	  store float %2216, float* %2222, align 4
	  %2224 = fmul float 8.800000e+08, %2223
	  %2225 = fadd float 8.800000e+08, %2224
	  %2227 = fmul float 8.800000e+08, %2226
	  %2228 = fsub float %2225, %2227
	  %2229 = fpext float %2228 to double
	  %2231 = call double @exp(double %2229) #4
	  %2243 = load float, float* %f, align 4
	  %2240 = load float, float* %e, align 4
	  %2239 = getelementptr inbounds float, float* %2238, i64 %2237
	  %2238 = load float*, float** %2, align 8
	  %2234 = load i32, i32* %z, align 4
	  %2233 = fptrunc double %2231 to float
	  %2235 = add nsw i32 0, %2234
	  %2236 = srem i32 %2235, 128
	  %2237 = sext i32 %2236 to i64
	  store float %2233, float* %2239, align 4
	  %2241 = fmul float 8.800000e+08, %2240
	  %2242 = fadd float 8.800000e+08, %2241
	  %2244 = fmul float 8.800000e+08, %2243
	  %2245 = fsub float %2242, %2244
	  %2246 = fpext float %2245 to double
	  %2248 = call double @exp(double %2246) #4
	  %2260 = load float, float* %f, align 4
	  %2257 = load float, float* %e, align 4
	  %2256 = getelementptr inbounds float, float* %2255, i64 %2254
	  %2255 = load float*, float** %2, align 8
	  %2251 = load i32, i32* %z, align 4
	  %2250 = fptrunc double %2248 to float
	  %2252 = add nsw i32 0, %2251
	  %2253 = srem i32 %2252, 128
	  %2254 = sext i32 %2253 to i64
	  store float %2250, float* %2256, align 4
	  %2258 = fmul float 8.800000e+08, %2257
	  %2259 = fadd float 8.800000e+08, %2258
	  %2261 = fmul float 8.800000e+08, %2260
	  %2262 = fadd float %2259, %2261
	  %2263 = fpext float %2262 to double
	  %2265 = call double @exp(double %2263) #4
	  %2277 = load float, float* %f, align 4
	  %2274 = load float, float* %e, align 4
	  %2273 = getelementptr inbounds float, float* %2272, i64 %2271
	  %2272 = load float*, float** %2, align 8
	  %2268 = load i32, i32* %z, align 4
	  %2267 = fptrunc double %2265 to float
	  %2269 = add nsw i32 0, %2268
	  %2270 = srem i32 %2269, 128
	  %2271 = sext i32 %2270 to i64
	  store float %2267, float* %2273, align 4
	  %2275 = fmul float 0x3E75798EE0000000, %2274
	  %2276 = fadd float 8.800000e+08, %2275
	  %2278 = fmul float 8.800000e+08, %2277
	  %2279 = fsub float %2276, %2278
	  %2280 = fpext float %2279 to double
	  %2282 = call double @exp(double %2280) #4
	  %2294 = load float, float* %f, align 4
	  %2291 = load float, float* %e, align 4
	  %2290 = getelementptr inbounds float, float* %2289, i64 %2288
	  %2289 = load float*, float** %2, align 8
	  %2285 = load i32, i32* %z, align 4
	  %2284 = fptrunc double %2282 to float
	  %2286 = add nsw i32 0, %2285
	  %2287 = srem i32 %2286, 128
	  %2288 = sext i32 %2287 to i64
	  store float %2284, float* %2290, align 4
	  %2292 = fmul float 8.000000e+08, %2291
	  %2293 = fadd float 8.800000e+08, %2292
	  %2295 = fmul float 8.800000e+08, %2294
	  %2296 = fadd float %2293, %2295
	  %2297 = fpext float %2296 to double
	  %2299 = call double @exp(double %2297) #4
	  %2311 = load float, float* %f, align 4
	  %2308 = load float, float* %e, align 4
	  %2307 = getelementptr inbounds float, float* %2306, i64 %2305
	  %2306 = load float*, float** %2, align 8
	  %2302 = load i32, i32* %z, align 4
	  %2301 = fptrunc double %2299 to float
	  %2303 = add nsw i32 0, %2302
	  %2304 = srem i32 %2303, 128
	  %2305 = sext i32 %2304 to i64
	  store float %2301, float* %2307, align 4
	  %2309 = fmul float 8.800000e+08, %2308
	  %2310 = fadd float 8.800000e+08, %2309
	  %2312 = fmul float 8.800000e+08, %2311
	  %2313 = fsub float %2310, %2312
	  %2314 = fpext float %2313 to double
	  %2316 = call double @exp(double %2314) #4
	  %2328 = load float, float* %f, align 4
	  %2325 = load float, float* %e, align 4
	  %2324 = getelementptr inbounds float, float* %2323, i64 %2322
	  %2323 = load float*, float** %2, align 8
	  %2319 = load i32, i32* %z, align 4
	  %2318 = fptrunc double %2316 to float
	  %2320 = add nsw i32 0, %2319
	  %2321 = srem i32 %2320, 128
	  %2322 = sext i32 %2321 to i64
	  store float %2318, float* %2324, align 4
	  %2326 = fmul float 8.800000e+08, %2325
	  %2327 = fadd float 0x3E779F5060000000, %2326
	  %2329 = fmul float 8.800000e+08, %2328
	  %2330 = fsub float %2327, %2329
	  %2331 = fpext float %2330 to double
	  %2333 = call double @exp(double %2331) #4
	  %2345 = load float, float* %f, align 4
	  %2342 = load float, float* %e, align 4
	  %2341 = getelementptr inbounds float, float* %2340, i64 %2339
	  %2340 = load float*, float** %2, align 8
	  %2336 = load i32, i32* %z, align 4
	  %2335 = fptrunc double %2333 to float
	  %2337 = add nsw i32 0, %2336
	  %2338 = srem i32 %2337, 128
	  %2339 = sext i32 %2338 to i64
	  store float %2335, float* %2341, align 4
	  %2343 = fmul float 8.800000e+08, %2342
	  %2344 = fsub float 8.800000e+08, %2343
	  %2346 = fmul float 8.800000e+08, %2345
	  %2347 = fsub float %2344, %2346
	  %2348 = fpext float %2347 to double
	  %2350 = call double @exp(double %2348) #4
	  %2398 = load float, float* %f, align 4
	  %2395 = load float, float* %e, align 4
	  %2394 = getelementptr inbounds float, float* %2393, i64 %2392
	  %2393 = load float*, float** %2, align 8
	  %2389 = load i32, i32* %z, align 4
	  %2388 = getelementptr inbounds float, float* %2387, i64 %2386
	  %2387 = load float*, float** %2, align 8
	  %2383 = load i32, i32* %z, align 4
	  %2382 = getelementptr inbounds float, float* %2381, i64 %2380
	  %2381 = load float*, float** %2, align 8
	  %2377 = load i32, i32* %z, align 4
	  %2376 = getelementptr inbounds float, float* %2375, i64 %2374
	  %2375 = load float*, float** %2, align 8
	  %2371 = load i32, i32* %z, align 4
	  %2370 = getelementptr inbounds float, float* %2369, i64 %2368
	  %2369 = load float*, float** %2, align 8
	  %2365 = load i32, i32* %z, align 4
	  %2364 = getelementptr inbounds float, float* %2363, i64 %2362
	  %2363 = load float*, float** %2, align 8
	  %2359 = load i32, i32* %z, align 4
	  %2358 = getelementptr inbounds float, float* %2357, i64 %2356
	  %2357 = load float*, float** %2, align 8
	  %2353 = load i32, i32* %z, align 4
	  %2352 = fptrunc double %2350 to float
	  %2354 = add nsw i32 0, %2353
	  %2355 = srem i32 %2354, 128
	  %2356 = sext i32 %2355 to i64
	  store float %2352, float* %2358, align 4
	  %2360 = add nsw i32 0, %2359
	  %2361 = srem i32 %2360, 128
	  %2362 = sext i32 %2361 to i64
	  store float 8.800000e+08, float* %2364, align 4
	  %2366 = add nsw i32 0, %2365
	  %2367 = srem i32 %2366, 128
	  %2368 = sext i32 %2367 to i64
	  store float 8.800000e+08, float* %2370, align 4
	  %2372 = add nsw i32 0, %2371
	  %2373 = srem i32 %2372, 128
	  %2374 = sext i32 %2373 to i64
	  store float 8.800000e+08, float* %2376, align 4
	  %2378 = add nsw i32 0, %2377
	  %2379 = srem i32 %2378, 128
	  %2380 = sext i32 %2379 to i64
	  store float 8.000000e+08, float* %2382, align 4
	  %2384 = add nsw i32 0, %2383
	  %2385 = srem i32 %2384, 128
	  %2386 = sext i32 %2385 to i64
	  store float 8.800000e+08, float* %2388, align 4
	  %2390 = add nsw i32 0, %2389
	  %2391 = srem i32 %2390, 128
	  %2392 = sext i32 %2391 to i64
	  store float 8.800000e+08, float* %2394, align 4
	  %2396 = fmul float 8.800000e+08, %2395
	  %2397 = fsub float 8.800000e+08, %2396
	  %2399 = fmul float 8.800000e+08, %2398
	  %2400 = fsub float %2397, %2399
	  %2401 = fpext float %2400 to double
	  %2403 = call double @exp(double %2401) #4
	  %2411 = getelementptr inbounds float, float* %2410, i64 %2409
	  %2410 = load float*, float** %2, align 8
	  %2406 = load i32, i32* %z, align 4
	  %2405 = fptrunc double %2403 to float
	  %2407 = add nsw i32 0, %2406
	  %2408 = srem i32 %2407, 128
	  %2409 = sext i32 %2408 to i64
	  store float %2405, float* %2411, align 4
