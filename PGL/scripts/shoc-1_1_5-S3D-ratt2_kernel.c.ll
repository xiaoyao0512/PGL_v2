	  %a = alloca [16384 x float], align 16
	  %b = alloca [16384 x float], align 16
	  %c = alloca [16384 x float], align 16
	  %d = alloca [16384 x float], align 16
	  %e = alloca float, align 4
	  %1 = bitcast [16384 x float]* %a to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* bitcast ([16384 x float]* @main.a to i8*), i64 65536, i32 16, i1 false)
	  %4 = bitcast [16384 x float]* %b to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %4, i8* bitcast ([16384 x float]* @main.b to i8*), i64 65536, i32 16, i1 false)
	  %7 = bitcast [16384 x float]* %c to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %7, i8* bitcast ([16384 x float]* @main.c to i8*), i64 65536, i32 16, i1 false)
	  %10 = bitcast [16384 x float]* %d to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %10, i8* bitcast ([16384 x float]* @main.d to i8*), i64 65536, i32 16, i1 false)
	  %17 = load float, float* %e, align 4
	  %16 = getelementptr inbounds [16384 x float], [16384 x float]* %d, i32 0, i32 0
	  %15 = getelementptr inbounds [16384 x float], [16384 x float]* %c, i32 0, i32 0
	  %14 = getelementptr inbounds [16384 x float], [16384 x float]* %b, i32 0, i32 0
	  %13 = getelementptr inbounds [16384 x float], [16384 x float]* %a, i32 0, i32 0
	store float* %13, float** %a, align 8
	store  float* %14, float** %b, align 8
	store  float* %15, float** %c, align 8
	store  float* %16, float** %d, align 8
	store  float %17, float* %e, align 8
	  store float 1.000000e+00, float* %e, align 4
	  call void @A(float* %13, float* %14, float* %15, float* %16, float %17)
	  %13 = load float, float* %f, align 4
	  %11 = load float, float* %5, align 4
	  %10 = load float, float* %9, align 4
	  %9 = getelementptr inbounds float, float* %8, i64 %7
	  %8 = load float*, float** %1, align 8
	  %6 = load i32, i32* %z, align 4
	  %1 = alloca float*, align 8
	  %2 = alloca float*, align 8
	  %3 = alloca float*, align 8
	  %4 = alloca float*, align 8
	  %5 = alloca float, align 4
	  %z = alloca i32, align 4
	  %f = alloca float, align 4
	  %g = alloca float, align 4
	  %h = alloca float, align 4
	  %i = alloca float, align 4
	  %j = alloca float, align 4
	  %k = alloca float, align 4
	  %l = alloca float, align 4
	  store float* %a, float** %1, align 8
	  store float* %b, float** %2, align 8
	  store float* %c, float** %3, align 8
	  store float* %d, float** %4, align 8
	  store float %e, float* %5, align 4
	  store i32 0, i32* %z, align 4
	  %7 = sext i32 %6 to i64
	  %12 = fmul float %10, %11
	  store float %12, float* %f, align 4
	  %14 = fpext float %13 to double
	  %16 = call double @log(double %14) #4
	  %67 = load float, float* %h, align 4
	  %65 = load float, float* %l, align 4
	  %63 = load float, float* %62, align 4
	  %62 = getelementptr inbounds float, float* %61, i64 %60
	  %61 = load float*, float** %2, align 8
	  %57 = load i32, i32* %z, align 4
	  %53 = load float, float* %52, align 4
	  %52 = getelementptr inbounds float, float* %51, i64 %50
	  %51 = load float*, float** %4, align 8
	  %47 = load i32, i32* %z, align 4
	  %46 = load float, float* %45, align 4
	  %45 = getelementptr inbounds float, float* %44, i64 %43
	  %44 = load float*, float** %4, align 8
	  %40 = load i32, i32* %z, align 4
	  %38 = load float, float* %37, align 4
	  %37 = getelementptr inbounds float, float* %36, i64 %35
	  %36 = load float*, float** %4, align 8
	  %32 = load i32, i32* %z, align 4
	  %31 = load float, float* %30, align 4
	  %30 = getelementptr inbounds float, float* %29, i64 %28
	  %29 = load float*, float** %4, align 8
	  %25 = load i32, i32* %z, align 4
	  %21 = load float, float* %f, align 4
	  %20 = load float, float* %i, align 4
	  %19 = load float, float* %j, align 4
	  %18 = fptrunc double %16 to float
	  store float %18, float* %g, align 4
	  store float 0x4415AF1D80000000, float* %h, align 4
	  store float 0x4193D2C640000000, float* %i, align 4
	  store float 1.013250e+06, float* %j, align 4
	  %22 = fmul float %20, %21
	  %23 = fdiv float 1.000000e+00, %22
	  %24 = fmul float %19, %23
	  store float %24, float* %k, align 4
	  %26 = add nsw i32 8, %25
	  %27 = srem i32 %26, 128
	  %28 = sext i32 %27 to i64
	  %33 = add nsw i32 24, %32
	  %34 = srem i32 %33, 128
	  %35 = sext i32 %34 to i64
	  %39 = fmul float %31, %38
	  %41 = add nsw i32 16, %40
	  %42 = srem i32 %41, 128
	  %43 = sext i32 %42 to i64
	  %48 = add nsw i32 32, %47
	  %49 = srem i32 %48, 128
	  %50 = sext i32 %49 to i64
	  %54 = fmul float %46, %53
	  %55 = fdiv float 1.000000e+00, %54
	  %56 = fmul float %39, %55
	  store float %56, float* %l, align 4
	  %58 = add nsw i32 0, %57
	  %59 = srem i32 %58, 128
	  %60 = sext i32 %59 to i64
	  %64 = fpext float %63 to double
	  %66 = fpext float %65 to double
	  %68 = fpext float %67 to double
	  %70 = call double @fmin(double %66, double %68) #6
	  %122 = load float, float* %h, align 4
	  %120 = load float, float* %l, align 4
	  %118 = load float, float* %117, align 4
	  %117 = getelementptr inbounds float, float* %116, i64 %115
	  %116 = load float*, float** %2, align 8
	  %112 = load i32, i32* %z, align 4
	  %108 = load float, float* %107, align 4
	  %107 = getelementptr inbounds float, float* %106, i64 %105
	  %106 = load float*, float** %4, align 8
	  %102 = load i32, i32* %z, align 4
	  %101 = load float, float* %100, align 4
	  %100 = getelementptr inbounds float, float* %99, i64 %98
	  %99 = load float*, float** %4, align 8
	  %95 = load i32, i32* %z, align 4
	  %93 = load float, float* %92, align 4
	  %92 = getelementptr inbounds float, float* %91, i64 %90
	  %91 = load float*, float** %4, align 8
	  %87 = load i32, i32* %z, align 4
	  %86 = load float, float* %85, align 4
	  %85 = getelementptr inbounds float, float* %84, i64 %83
	  %84 = load float*, float** %4, align 8
	  %80 = load i32, i32* %z, align 4
	  %79 = getelementptr inbounds float, float* %78, i64 %77
	  %78 = load float*, float** %3, align 8
	  %74 = load i32, i32* %z, align 4
	  %72 = fmul double %64, %70
	  %73 = fptrunc double %72 to float
	  %75 = add nsw i32 0, %74
	  %76 = srem i32 %75, 128
	  %77 = sext i32 %76 to i64
	  store float %73, float* %79, align 4
	  %81 = add nsw i32 0, %80
	  %82 = srem i32 %81, 128
	  %83 = sext i32 %82 to i64
	  %88 = add nsw i32 16, %87
	  %89 = srem i32 %88, 128
	  %90 = sext i32 %89 to i64
	  %94 = fmul float %86, %93
	  %96 = add nsw i32 8, %95
	  %97 = srem i32 %96, 128
	  %98 = sext i32 %97 to i64
	  %103 = add nsw i32 32, %102
	  %104 = srem i32 %103, 128
	  %105 = sext i32 %104 to i64
	  %109 = fmul float %101, %108
	  %110 = fdiv float 1.000000e+00, %109
	  %111 = fmul float %94, %110
	  store float %111, float* %l, align 4
	  %113 = add nsw i32 8, %112
	  %114 = srem i32 %113, 128
	  %115 = sext i32 %114 to i64
	  %119 = fpext float %118 to double
	  %121 = fpext float %120 to double
	  %123 = fpext float %122 to double
	  %125 = call double @fmin(double %121, double %123) #6
	  %177 = load float, float* %h, align 4
	  %175 = load float, float* %l, align 4
	  %173 = load float, float* %172, align 4
	  %172 = getelementptr inbounds float, float* %171, i64 %170
	  %171 = load float*, float** %2, align 8
	  %167 = load i32, i32* %z, align 4
	  %163 = load float, float* %162, align 4
	  %162 = getelementptr inbounds float, float* %161, i64 %160
	  %161 = load float*, float** %4, align 8
	  %157 = load i32, i32* %z, align 4
	  %156 = load float, float* %155, align 4
	  %155 = getelementptr inbounds float, float* %154, i64 %153
	  %154 = load float*, float** %4, align 8
	  %150 = load i32, i32* %z, align 4
	  %148 = load float, float* %147, align 4
	  %147 = getelementptr inbounds float, float* %146, i64 %145
	  %146 = load float*, float** %4, align 8
	  %142 = load i32, i32* %z, align 4
	  %141 = load float, float* %140, align 4
	  %140 = getelementptr inbounds float, float* %139, i64 %138
	  %139 = load float*, float** %4, align 8
	  %135 = load i32, i32* %z, align 4
	  %134 = getelementptr inbounds float, float* %133, i64 %132
	  %133 = load float*, float** %3, align 8
	  %129 = load i32, i32* %z, align 4
	  %127 = fmul double %119, %125
	  %128 = fptrunc double %127 to float
	  %130 = add nsw i32 8, %129
	  %131 = srem i32 %130, 128
	  %132 = sext i32 %131 to i64
	  store float %128, float* %134, align 4
	  %136 = add nsw i32 0, %135
	  %137 = srem i32 %136, 128
	  %138 = sext i32 %137 to i64
	  %143 = add nsw i32 32, %142
	  %144 = srem i32 %143, 128
	  %145 = sext i32 %144 to i64
	  %149 = fmul float %141, %148
	  %151 = add nsw i32 8, %150
	  %152 = srem i32 %151, 128
	  %153 = sext i32 %152 to i64
	  %158 = add nsw i32 40, %157
	  %159 = srem i32 %158, 128
	  %160 = sext i32 %159 to i64
	  %164 = fmul float %156, %163
	  %165 = fdiv float 1.000000e+00, %164
	  %166 = fmul float %149, %165
	  store float %166, float* %l, align 4
	  %168 = add nsw i32 16, %167
	  %169 = srem i32 %168, 128
	  %170 = sext i32 %169 to i64
	  %174 = fpext float %173 to double
	  %176 = fpext float %175 to double
	  %178 = fpext float %177 to double
	  %180 = call double @fmin(double %176, double %178) #6
	  %232 = load float, float* %h, align 4
	  %230 = load float, float* %l, align 4
	  %228 = load float, float* %227, align 4
	  %227 = getelementptr inbounds float, float* %226, i64 %225
	  %226 = load float*, float** %2, align 8
	  %222 = load i32, i32* %z, align 4
	  %218 = load float, float* %217, align 4
	  %217 = getelementptr inbounds float, float* %216, i64 %215
	  %216 = load float*, float** %4, align 8
	  %212 = load i32, i32* %z, align 4
	  %211 = load float, float* %210, align 4
	  %210 = getelementptr inbounds float, float* %209, i64 %208
	  %209 = load float*, float** %4, align 8
	  %205 = load i32, i32* %z, align 4
	  %203 = load float, float* %202, align 4
	  %202 = getelementptr inbounds float, float* %201, i64 %200
	  %201 = load float*, float** %4, align 8
	  %197 = load i32, i32* %z, align 4
	  %196 = load float, float* %195, align 4
	  %195 = getelementptr inbounds float, float* %194, i64 %193
	  %194 = load float*, float** %4, align 8
	  %190 = load i32, i32* %z, align 4
	  %189 = getelementptr inbounds float, float* %188, i64 %187
	  %188 = load float*, float** %3, align 8
	  %184 = load i32, i32* %z, align 4
	  %182 = fmul double %174, %180
	  %183 = fptrunc double %182 to float
	  %185 = add nsw i32 16, %184
	  %186 = srem i32 %185, 128
	  %187 = sext i32 %186 to i64
	  store float %183, float* %189, align 4
	  %191 = add nsw i32 32, %190
	  %192 = srem i32 %191, 128
	  %193 = sext i32 %192 to i64
	  %198 = add nsw i32 32, %197
	  %199 = srem i32 %198, 128
	  %200 = sext i32 %199 to i64
	  %204 = fmul float %196, %203
	  %206 = add nsw i32 16, %205
	  %207 = srem i32 %206, 128
	  %208 = sext i32 %207 to i64
	  %213 = add nsw i32 40, %212
	  %214 = srem i32 %213, 128
	  %215 = sext i32 %214 to i64
	  %219 = fmul float %211, %218
	  %220 = fdiv float 1.000000e+00, %219
	  %221 = fmul float %204, %220
	  store float %221, float* %l, align 4
	  %223 = add nsw i32 24, %222
	  %224 = srem i32 %223, 128
	  %225 = sext i32 %224 to i64
	  %229 = fpext float %228 to double
	  %231 = fpext float %230 to double
	  %233 = fpext float %232 to double
	  %235 = call double @fmin(double %231, double %233) #6
	  %281 = load float, float* %h, align 4
	  %279 = load float, float* %l, align 4
	  %277 = load float, float* %276, align 4
	  %276 = getelementptr inbounds float, float* %275, i64 %274
	  %275 = load float*, float** %2, align 8
	  %271 = load i32, i32* %z, align 4
	  %268 = load float, float* %267, align 4
	  %267 = getelementptr inbounds float, float* %266, i64 %265
	  %266 = load float*, float** %4, align 8
	  %262 = load i32, i32* %z, align 4
	  %260 = load float, float* %k, align 4
	  %258 = load float, float* %257, align 4
	  %257 = getelementptr inbounds float, float* %256, i64 %255
	  %256 = load float*, float** %4, align 8
	  %252 = load i32, i32* %z, align 4
	  %251 = load float, float* %250, align 4
	  %250 = getelementptr inbounds float, float* %249, i64 %248
	  %249 = load float*, float** %4, align 8
	  %245 = load i32, i32* %z, align 4
	  %244 = getelementptr inbounds float, float* %243, i64 %242
	  %243 = load float*, float** %3, align 8
	  %239 = load i32, i32* %z, align 4
	  %237 = fmul double %229, %235
	  %238 = fptrunc double %237 to float
	  %240 = add nsw i32 24, %239
	  %241 = srem i32 %240, 128
	  %242 = sext i32 %241 to i64
	  store float %238, float* %244, align 4
	  %246 = add nsw i32 8, %245
	  %247 = srem i32 %246, 128
	  %248 = sext i32 %247 to i64
	  %253 = add nsw i32 8, %252
	  %254 = srem i32 %253, 128
	  %255 = sext i32 %254 to i64
	  %259 = fmul float %251, %258
	  %261 = fmul float %259, %260
	  %263 = add nsw i32 0, %262
	  %264 = srem i32 %263, 128
	  %265 = sext i32 %264 to i64
	  %269 = fdiv float 1.000000e+00, %268
	  %270 = fmul float %261, %269
	  store float %270, float* %l, align 4
	  %272 = add nsw i32 32, %271
	  %273 = srem i32 %272, 128
	  %274 = sext i32 %273 to i64
	  %278 = fpext float %277 to double
	  %280 = fpext float %279 to double
	  %282 = fpext float %281 to double
	  %284 = call double @fmin(double %280, double %282) #6
	  %330 = load float, float* %h, align 4
	  %328 = load float, float* %l, align 4
	  %326 = load float, float* %325, align 4
	  %325 = getelementptr inbounds float, float* %324, i64 %323
	  %324 = load float*, float** %2, align 8
	  %320 = load i32, i32* %z, align 4
	  %317 = load float, float* %316, align 4
	  %316 = getelementptr inbounds float, float* %315, i64 %314
	  %315 = load float*, float** %4, align 8
	  %311 = load i32, i32* %z, align 4
	  %309 = load float, float* %k, align 4
	  %307 = load float, float* %306, align 4
	  %306 = getelementptr inbounds float, float* %305, i64 %304
	  %305 = load float*, float** %4, align 8
	  %301 = load i32, i32* %z, align 4
	  %300 = load float, float* %299, align 4
	  %299 = getelementptr inbounds float, float* %298, i64 %297
	  %298 = load float*, float** %4, align 8
	  %294 = load i32, i32* %z, align 4
	  %293 = getelementptr inbounds float, float* %292, i64 %291
	  %292 = load float*, float** %3, align 8
	  %288 = load i32, i32* %z, align 4
	  %286 = fmul double %278, %284
	  %287 = fptrunc double %286 to float
	  %289 = add nsw i32 32, %288
	  %290 = srem i32 %289, 128
	  %291 = sext i32 %290 to i64
	  store float %287, float* %293, align 4
	  %295 = add nsw i32 8, %294
	  %296 = srem i32 %295, 128
	  %297 = sext i32 %296 to i64
	  %302 = add nsw i32 8, %301
	  %303 = srem i32 %302, 128
	  %304 = sext i32 %303 to i64
	  %308 = fmul float %300, %307
	  %310 = fmul float %308, %309
	  %312 = add nsw i32 0, %311
	  %313 = srem i32 %312, 128
	  %314 = sext i32 %313 to i64
	  %318 = fdiv float 1.000000e+00, %317
	  %319 = fmul float %310, %318
	  store float %319, float* %l, align 4
	  %321 = add nsw i32 40, %320
	  %322 = srem i32 %321, 128
	  %323 = sext i32 %322 to i64
	  %327 = fpext float %326 to double
	  %329 = fpext float %328 to double
	  %331 = fpext float %330 to double
	  %333 = call double @fmin(double %329, double %331) #6
	  %379 = load float, float* %h, align 4
	  %377 = load float, float* %l, align 4
	  %375 = load float, float* %374, align 4
	  %374 = getelementptr inbounds float, float* %373, i64 %372
	  %373 = load float*, float** %2, align 8
	  %369 = load i32, i32* %z, align 4
	  %366 = load float, float* %365, align 4
	  %365 = getelementptr inbounds float, float* %364, i64 %363
	  %364 = load float*, float** %4, align 8
	  %360 = load i32, i32* %z, align 4
	  %358 = load float, float* %k, align 4
	  %356 = load float, float* %355, align 4
	  %355 = getelementptr inbounds float, float* %354, i64 %353
	  %354 = load float*, float** %4, align 8
	  %350 = load i32, i32* %z, align 4
	  %349 = load float, float* %348, align 4
	  %348 = getelementptr inbounds float, float* %347, i64 %346
	  %347 = load float*, float** %4, align 8
	  %343 = load i32, i32* %z, align 4
	  %342 = getelementptr inbounds float, float* %341, i64 %340
	  %341 = load float*, float** %3, align 8
	  %337 = load i32, i32* %z, align 4
	  %335 = fmul double %327, %333
	  %336 = fptrunc double %335 to float
	  %338 = add nsw i32 40, %337
	  %339 = srem i32 %338, 128
	  %340 = sext i32 %339 to i64
	  store float %336, float* %342, align 4
	  %344 = add nsw i32 8, %343
	  %345 = srem i32 %344, 128
	  %346 = sext i32 %345 to i64
	  %351 = add nsw i32 8, %350
	  %352 = srem i32 %351, 128
	  %353 = sext i32 %352 to i64
	  %357 = fmul float %349, %356
	  %359 = fmul float %357, %358
	  %361 = add nsw i32 0, %360
	  %362 = srem i32 %361, 128
	  %363 = sext i32 %362 to i64
	  %367 = fdiv float 1.000000e+00, %366
	  %368 = fmul float %359, %367
	  store float %368, float* %l, align 4
	  %370 = add nsw i32 48, %369
	  %371 = srem i32 %370, 128
	  %372 = sext i32 %371 to i64
	  %376 = fpext float %375 to double
	  %378 = fpext float %377 to double
	  %380 = fpext float %379 to double
	  %382 = call double @fmin(double %378, double %380) #6
	  %428 = load float, float* %h, align 4
	  %426 = load float, float* %l, align 4
	  %424 = load float, float* %423, align 4
	  %423 = getelementptr inbounds float, float* %422, i64 %421
	  %422 = load float*, float** %2, align 8
	  %418 = load i32, i32* %z, align 4
	  %415 = load float, float* %414, align 4
	  %414 = getelementptr inbounds float, float* %413, i64 %412
	  %413 = load float*, float** %4, align 8
	  %409 = load i32, i32* %z, align 4
	  %407 = load float, float* %k, align 4
	  %405 = load float, float* %404, align 4
	  %404 = getelementptr inbounds float, float* %403, i64 %402
	  %403 = load float*, float** %4, align 8
	  %399 = load i32, i32* %z, align 4
	  %398 = load float, float* %397, align 4
	  %397 = getelementptr inbounds float, float* %396, i64 %395
	  %396 = load float*, float** %4, align 8
	  %392 = load i32, i32* %z, align 4
	  %391 = getelementptr inbounds float, float* %390, i64 %389
	  %390 = load float*, float** %3, align 8
	  %386 = load i32, i32* %z, align 4
	  %384 = fmul double %376, %382
	  %385 = fptrunc double %384 to float
	  %387 = add nsw i32 48, %386
	  %388 = srem i32 %387, 128
	  %389 = sext i32 %388 to i64
	  store float %385, float* %391, align 4
	  %393 = add nsw i32 8, %392
	  %394 = srem i32 %393, 128
	  %395 = sext i32 %394 to i64
	  %400 = add nsw i32 8, %399
	  %401 = srem i32 %400, 128
	  %402 = sext i32 %401 to i64
	  %406 = fmul float %398, %405
	  %408 = fmul float %406, %407
	  %410 = add nsw i32 0, %409
	  %411 = srem i32 %410, 128
	  %412 = sext i32 %411 to i64
	  %416 = fdiv float 1.000000e+00, %415
	  %417 = fmul float %408, %416
	  store float %417, float* %l, align 4
	  %419 = add nsw i32 56, %418
	  %420 = srem i32 %419, 128
	  %421 = sext i32 %420 to i64
	  %425 = fpext float %424 to double
	  %427 = fpext float %426 to double
	  %429 = fpext float %428 to double
	  %431 = call double @fmin(double %427, double %429) #6
	  %477 = load float, float* %h, align 4
	  %475 = load float, float* %l, align 4
	  %473 = load float, float* %472, align 4
	  %472 = getelementptr inbounds float, float* %471, i64 %470
	  %471 = load float*, float** %2, align 8
	  %467 = load i32, i32* %z, align 4
	  %464 = load float, float* %463, align 4
	  %463 = getelementptr inbounds float, float* %462, i64 %461
	  %462 = load float*, float** %4, align 8
	  %458 = load i32, i32* %z, align 4
	  %456 = load float, float* %k, align 4
	  %454 = load float, float* %453, align 4
	  %453 = getelementptr inbounds float, float* %452, i64 %451
	  %452 = load float*, float** %4, align 8
	  %448 = load i32, i32* %z, align 4
	  %447 = load float, float* %446, align 4
	  %446 = getelementptr inbounds float, float* %445, i64 %444
	  %445 = load float*, float** %4, align 8
	  %441 = load i32, i32* %z, align 4
	  %440 = getelementptr inbounds float, float* %439, i64 %438
	  %439 = load float*, float** %3, align 8
	  %435 = load i32, i32* %z, align 4
	  %433 = fmul double %425, %431
	  %434 = fptrunc double %433 to float
	  %436 = add nsw i32 56, %435
	  %437 = srem i32 %436, 128
	  %438 = sext i32 %437 to i64
	  store float %434, float* %440, align 4
	  %442 = add nsw i32 8, %441
	  %443 = srem i32 %442, 128
	  %444 = sext i32 %443 to i64
	  %449 = add nsw i32 32, %448
	  %450 = srem i32 %449, 128
	  %451 = sext i32 %450 to i64
	  %455 = fmul float %447, %454
	  %457 = fmul float %455, %456
	  %459 = add nsw i32 40, %458
	  %460 = srem i32 %459, 128
	  %461 = sext i32 %460 to i64
	  %465 = fdiv float 1.000000e+00, %464
	  %466 = fmul float %457, %465
	  store float %466, float* %l, align 4
	  %468 = add nsw i32 64, %467
	  %469 = srem i32 %468, 128
	  %470 = sext i32 %469 to i64
	  %474 = fpext float %473 to double
	  %476 = fpext float %475 to double
	  %478 = fpext float %477 to double
	  %480 = call double @fmin(double %476, double %478) #6
	  %526 = load float, float* %h, align 4
	  %524 = load float, float* %l, align 4
	  %522 = load float, float* %521, align 4
	  %521 = getelementptr inbounds float, float* %520, i64 %519
	  %520 = load float*, float** %2, align 8
	  %516 = load i32, i32* %z, align 4
	  %513 = load float, float* %512, align 4
	  %512 = getelementptr inbounds float, float* %511, i64 %510
	  %511 = load float*, float** %4, align 8
	  %507 = load i32, i32* %z, align 4
	  %505 = load float, float* %k, align 4
	  %503 = load float, float* %502, align 4
	  %502 = getelementptr inbounds float, float* %501, i64 %500
	  %501 = load float*, float** %4, align 8
	  %497 = load i32, i32* %z, align 4
	  %496 = load float, float* %495, align 4
	  %495 = getelementptr inbounds float, float* %494, i64 %493
	  %494 = load float*, float** %4, align 8
	  %490 = load i32, i32* %z, align 4
	  %489 = getelementptr inbounds float, float* %488, i64 %487
	  %488 = load float*, float** %3, align 8
	  %484 = load i32, i32* %z, align 4
	  %482 = fmul double %474, %480
	  %483 = fptrunc double %482 to float
	  %485 = add nsw i32 64, %484
	  %486 = srem i32 %485, 128
	  %487 = sext i32 %486 to i64
	  store float %483, float* %489, align 4
	  %491 = add nsw i32 8, %490
	  %492 = srem i32 %491, 128
	  %493 = sext i32 %492 to i64
	  %498 = add nsw i32 16, %497
	  %499 = srem i32 %498, 128
	  %500 = sext i32 %499 to i64
	  %504 = fmul float %496, %503
	  %506 = fmul float %504, %505
	  %508 = add nsw i32 32, %507
	  %509 = srem i32 %508, 128
	  %510 = sext i32 %509 to i64
	  %514 = fdiv float 1.000000e+00, %513
	  %515 = fmul float %506, %514
	  store float %515, float* %l, align 4
	  %517 = add nsw i32 72, %516
	  %518 = srem i32 %517, 128
	  %519 = sext i32 %518 to i64
	  %523 = fpext float %522 to double
	  %525 = fpext float %524 to double
	  %527 = fpext float %526 to double
	  %529 = call double @fmin(double %525, double %527) #6
	  %575 = load float, float* %h, align 4
	  %573 = load float, float* %l, align 4
	  %571 = load float, float* %570, align 4
	  %570 = getelementptr inbounds float, float* %569, i64 %568
	  %569 = load float*, float** %2, align 8
	  %565 = load i32, i32* %z, align 4
	  %562 = load float, float* %561, align 4
	  %561 = getelementptr inbounds float, float* %560, i64 %559
	  %560 = load float*, float** %4, align 8
	  %556 = load i32, i32* %z, align 4
	  %554 = load float, float* %k, align 4
	  %552 = load float, float* %551, align 4
	  %551 = getelementptr inbounds float, float* %550, i64 %549
	  %550 = load float*, float** %4, align 8
	  %546 = load i32, i32* %z, align 4
	  %545 = load float, float* %544, align 4
	  %544 = getelementptr inbounds float, float* %543, i64 %542
	  %543 = load float*, float** %4, align 8
	  %539 = load i32, i32* %z, align 4
	  %538 = getelementptr inbounds float, float* %537, i64 %536
	  %537 = load float*, float** %3, align 8
	  %533 = load i32, i32* %z, align 4
	  %531 = fmul double %523, %529
	  %532 = fptrunc double %531 to float
	  %534 = add nsw i32 72, %533
	  %535 = srem i32 %534, 128
	  %536 = sext i32 %535 to i64
	  store float %532, float* %538, align 4
	  %540 = add nsw i32 16, %539
	  %541 = srem i32 %540, 128
	  %542 = sext i32 %541 to i64
	  %547 = add nsw i32 16, %546
	  %548 = srem i32 %547, 128
	  %549 = sext i32 %548 to i64
	  %553 = fmul float %545, %552
	  %555 = fmul float %553, %554
	  %557 = add nsw i32 24, %556
	  %558 = srem i32 %557, 128
	  %559 = sext i32 %558 to i64
	  %563 = fdiv float 1.000000e+00, %562
	  %564 = fmul float %555, %563
	  store float %564, float* %l, align 4
	  %566 = add nsw i32 80, %565
	  %567 = srem i32 %566, 128
	  %568 = sext i32 %567 to i64
	  %572 = fpext float %571 to double
	  %574 = fpext float %573 to double
	  %576 = fpext float %575 to double
	  %578 = call double @fmin(double %574, double %576) #6
	  %624 = load float, float* %h, align 4
	  %622 = load float, float* %l, align 4
	  %620 = load float, float* %619, align 4
	  %619 = getelementptr inbounds float, float* %618, i64 %617
	  %618 = load float*, float** %2, align 8
	  %614 = load i32, i32* %z, align 4
	  %611 = load float, float* %610, align 4
	  %610 = getelementptr inbounds float, float* %609, i64 %608
	  %609 = load float*, float** %4, align 8
	  %605 = load i32, i32* %z, align 4
	  %603 = load float, float* %k, align 4
	  %601 = load float, float* %600, align 4
	  %600 = getelementptr inbounds float, float* %599, i64 %598
	  %599 = load float*, float** %4, align 8
	  %595 = load i32, i32* %z, align 4
	  %594 = load float, float* %593, align 4
	  %593 = getelementptr inbounds float, float* %592, i64 %591
	  %592 = load float*, float** %4, align 8
	  %588 = load i32, i32* %z, align 4
	  %587 = getelementptr inbounds float, float* %586, i64 %585
	  %586 = load float*, float** %3, align 8
	  %582 = load i32, i32* %z, align 4
	  %580 = fmul double %572, %578
	  %581 = fptrunc double %580 to float
	  %583 = add nsw i32 80, %582
	  %584 = srem i32 %583, 128
	  %585 = sext i32 %584 to i64
	  store float %581, float* %587, align 4
	  %589 = add nsw i32 8, %588
	  %590 = srem i32 %589, 128
	  %591 = sext i32 %590 to i64
	  %596 = add nsw i32 24, %595
	  %597 = srem i32 %596, 128
	  %598 = sext i32 %597 to i64
	  %602 = fmul float %594, %601
	  %604 = fmul float %602, %603
	  %606 = add nsw i32 48, %605
	  %607 = srem i32 %606, 128
	  %608 = sext i32 %607 to i64
	  %612 = fdiv float 1.000000e+00, %611
	  %613 = fmul float %604, %612
	  store float %613, float* %l, align 4
	  %615 = add nsw i32 88, %614
	  %616 = srem i32 %615, 128
	  %617 = sext i32 %616 to i64
	  %621 = fpext float %620 to double
	  %623 = fpext float %622 to double
	  %625 = fpext float %624 to double
	  %627 = call double @fmin(double %623, double %625) #6
	  %673 = load float, float* %h, align 4
	  %671 = load float, float* %l, align 4
	  %669 = load float, float* %668, align 4
	  %668 = getelementptr inbounds float, float* %667, i64 %666
	  %667 = load float*, float** %2, align 8
	  %663 = load i32, i32* %z, align 4
	  %660 = load float, float* %659, align 4
	  %659 = getelementptr inbounds float, float* %658, i64 %657
	  %658 = load float*, float** %4, align 8
	  %654 = load i32, i32* %z, align 4
	  %652 = load float, float* %k, align 4
	  %650 = load float, float* %649, align 4
	  %649 = getelementptr inbounds float, float* %648, i64 %647
	  %648 = load float*, float** %4, align 8
	  %644 = load i32, i32* %z, align 4
	  %643 = load float, float* %642, align 4
	  %642 = getelementptr inbounds float, float* %641, i64 %640
	  %641 = load float*, float** %4, align 8
	  %637 = load i32, i32* %z, align 4
	  %636 = getelementptr inbounds float, float* %635, i64 %634
	  %635 = load float*, float** %3, align 8
	  %631 = load i32, i32* %z, align 4
	  %629 = fmul double %621, %627
	  %630 = fptrunc double %629 to float
	  %632 = add nsw i32 88, %631
	  %633 = srem i32 %632, 128
	  %634 = sext i32 %633 to i64
	  store float %630, float* %636, align 4
	  %638 = add nsw i32 8, %637
	  %639 = srem i32 %638, 128
	  %640 = sext i32 %639 to i64
	  %645 = add nsw i32 24, %644
	  %646 = srem i32 %645, 128
	  %647 = sext i32 %646 to i64
	  %651 = fmul float %643, %650
	  %653 = fmul float %651, %652
	  %655 = add nsw i32 48, %654
	  %656 = srem i32 %655, 128
	  %657 = sext i32 %656 to i64
	  %661 = fdiv float 1.000000e+00, %660
	  %662 = fmul float %653, %661
	  store float %662, float* %l, align 4
	  %664 = add nsw i32 96, %663
	  %665 = srem i32 %664, 128
	  %666 = sext i32 %665 to i64
	  %670 = fpext float %669 to double
	  %672 = fpext float %671 to double
	  %674 = fpext float %673 to double
	  %676 = call double @fmin(double %672, double %674) #6
	  %722 = load float, float* %h, align 4
	  %720 = load float, float* %l, align 4
	  %718 = load float, float* %717, align 4
	  %717 = getelementptr inbounds float, float* %716, i64 %715
	  %716 = load float*, float** %2, align 8
	  %712 = load i32, i32* %z, align 4
	  %709 = load float, float* %708, align 4
	  %708 = getelementptr inbounds float, float* %707, i64 %706
	  %707 = load float*, float** %4, align 8
	  %703 = load i32, i32* %z, align 4
	  %701 = load float, float* %k, align 4
	  %699 = load float, float* %698, align 4
	  %698 = getelementptr inbounds float, float* %697, i64 %696
	  %697 = load float*, float** %4, align 8
	  %693 = load i32, i32* %z, align 4
	  %692 = load float, float* %691, align 4
	  %691 = getelementptr inbounds float, float* %690, i64 %689
	  %690 = load float*, float** %4, align 8
	  %686 = load i32, i32* %z, align 4
	  %685 = getelementptr inbounds float, float* %684, i64 %683
	  %684 = load float*, float** %3, align 8
	  %680 = load i32, i32* %z, align 4
	  %678 = fmul double %670, %676
	  %679 = fptrunc double %678 to float
	  %681 = add nsw i32 96, %680
	  %682 = srem i32 %681, 128
	  %683 = sext i32 %682 to i64
	  store float %679, float* %685, align 4
	  %687 = add nsw i32 8, %686
	  %688 = srem i32 %687, 128
	  %689 = sext i32 %688 to i64
	  %694 = add nsw i32 24, %693
	  %695 = srem i32 %694, 128
	  %696 = sext i32 %695 to i64
	  %700 = fmul float %692, %699
	  %702 = fmul float %700, %701
	  %704 = add nsw i32 48, %703
	  %705 = srem i32 %704, 128
	  %706 = sext i32 %705 to i64
	  %710 = fdiv float 1.000000e+00, %709
	  %711 = fmul float %702, %710
	  store float %711, float* %l, align 4
	  %713 = add nsw i32 104, %712
	  %714 = srem i32 %713, 128
	  %715 = sext i32 %714 to i64
	  %719 = fpext float %718 to double
	  %721 = fpext float %720 to double
	  %723 = fpext float %722 to double
	  %725 = call double @fmin(double %721, double %723) #6
	  %771 = load float, float* %h, align 4
	  %769 = load float, float* %l, align 4
	  %767 = load float, float* %766, align 4
	  %766 = getelementptr inbounds float, float* %765, i64 %764
	  %765 = load float*, float** %2, align 8
	  %761 = load i32, i32* %z, align 4
	  %758 = load float, float* %757, align 4
	  %757 = getelementptr inbounds float, float* %756, i64 %755
	  %756 = load float*, float** %4, align 8
	  %752 = load i32, i32* %z, align 4
	  %750 = load float, float* %k, align 4
	  %748 = load float, float* %747, align 4
	  %747 = getelementptr inbounds float, float* %746, i64 %745
	  %746 = load float*, float** %4, align 8
	  %742 = load i32, i32* %z, align 4
	  %741 = load float, float* %740, align 4
	  %740 = getelementptr inbounds float, float* %739, i64 %738
	  %739 = load float*, float** %4, align 8
	  %735 = load i32, i32* %z, align 4
	  %734 = getelementptr inbounds float, float* %733, i64 %732
	  %733 = load float*, float** %3, align 8
	  %729 = load i32, i32* %z, align 4
	  %727 = fmul double %719, %725
	  %728 = fptrunc double %727 to float
	  %730 = add nsw i32 104, %729
	  %731 = srem i32 %730, 128
	  %732 = sext i32 %731 to i64
	  store float %728, float* %734, align 4
	  %736 = add nsw i32 8, %735
	  %737 = srem i32 %736, 128
	  %738 = sext i32 %737 to i64
	  %743 = add nsw i32 24, %742
	  %744 = srem i32 %743, 128
	  %745 = sext i32 %744 to i64
	  %749 = fmul float %741, %748
	  %751 = fmul float %749, %750
	  %753 = add nsw i32 48, %752
	  %754 = srem i32 %753, 128
	  %755 = sext i32 %754 to i64
	  %759 = fdiv float 1.000000e+00, %758
	  %760 = fmul float %751, %759
	  store float %760, float* %l, align 4
	  %762 = add nsw i32 112, %761
	  %763 = srem i32 %762, 128
	  %764 = sext i32 %763 to i64
	  %768 = fpext float %767 to double
	  %770 = fpext float %769 to double
	  %772 = fpext float %771 to double
	  %774 = call double @fmin(double %770, double %772) #6
	  %820 = load float, float* %h, align 4
	  %818 = load float, float* %l, align 4
	  %816 = load float, float* %815, align 4
	  %815 = getelementptr inbounds float, float* %814, i64 %813
	  %814 = load float*, float** %2, align 8
	  %810 = load i32, i32* %z, align 4
	  %807 = load float, float* %806, align 4
	  %806 = getelementptr inbounds float, float* %805, i64 %804
	  %805 = load float*, float** %4, align 8
	  %801 = load i32, i32* %z, align 4
	  %799 = load float, float* %k, align 4
	  %797 = load float, float* %796, align 4
	  %796 = getelementptr inbounds float, float* %795, i64 %794
	  %795 = load float*, float** %4, align 8
	  %791 = load i32, i32* %z, align 4
	  %790 = load float, float* %789, align 4
	  %789 = getelementptr inbounds float, float* %788, i64 %787
	  %788 = load float*, float** %4, align 8
	  %784 = load i32, i32* %z, align 4
	  %783 = getelementptr inbounds float, float* %782, i64 %781
	  %782 = load float*, float** %3, align 8
	  %778 = load i32, i32* %z, align 4
	  %776 = fmul double %768, %774
	  %777 = fptrunc double %776 to float
	  %779 = add nsw i32 112, %778
	  %780 = srem i32 %779, 128
	  %781 = sext i32 %780 to i64
	  store float %777, float* %783, align 4
	  %785 = add nsw i32 32, %784
	  %786 = srem i32 %785, 128
	  %787 = sext i32 %786 to i64
	  %792 = add nsw i32 32, %791
	  %793 = srem i32 %792, 128
	  %794 = sext i32 %793 to i64
	  %798 = fmul float %790, %797
	  %800 = fmul float %798, %799
	  %802 = add nsw i32 56, %801
	  %803 = srem i32 %802, 128
	  %804 = sext i32 %803 to i64
	  %808 = fdiv float 1.000000e+00, %807
	  %809 = fmul float %800, %808
	  store float %809, float* %l, align 4
	  %811 = add nsw i32 120, %810
	  %812 = srem i32 %811, 128
	  %813 = sext i32 %812 to i64
	  %817 = fpext float %816 to double
	  %819 = fpext float %818 to double
	  %821 = fpext float %820 to double
	  %823 = call double @fmin(double %819, double %821) #6
	  %875 = load float, float* %h, align 4
	  %873 = load float, float* %l, align 4
	  %871 = load float, float* %870, align 4
	  %870 = getelementptr inbounds float, float* %869, i64 %868
	  %869 = load float*, float** %2, align 8
	  %865 = load i32, i32* %z, align 4
	  %861 = load float, float* %860, align 4
	  %860 = getelementptr inbounds float, float* %859, i64 %858
	  %859 = load float*, float** %4, align 8
	  %855 = load i32, i32* %z, align 4
	  %854 = load float, float* %853, align 4
	  %853 = getelementptr inbounds float, float* %852, i64 %851
	  %852 = load float*, float** %4, align 8
	  %848 = load i32, i32* %z, align 4
	  %846 = load float, float* %845, align 4
	  %845 = getelementptr inbounds float, float* %844, i64 %843
	  %844 = load float*, float** %4, align 8
	  %840 = load i32, i32* %z, align 4
	  %839 = load float, float* %838, align 4
	  %838 = getelementptr inbounds float, float* %837, i64 %836
	  %837 = load float*, float** %4, align 8
	  %833 = load i32, i32* %z, align 4
	  %832 = getelementptr inbounds float, float* %831, i64 %830
	  %831 = load float*, float** %3, align 8
	  %827 = load i32, i32* %z, align 4
	  %825 = fmul double %817, %823
	  %826 = fptrunc double %825 to float
	  %828 = add nsw i32 120, %827
	  %829 = srem i32 %828, 128
	  %830 = sext i32 %829 to i64
	  store float %826, float* %832, align 4
	  %834 = add nsw i32 8, %833
	  %835 = srem i32 %834, 128
	  %836 = sext i32 %835 to i64
	  %841 = add nsw i32 48, %840
	  %842 = srem i32 %841, 128
	  %843 = sext i32 %842 to i64
	  %847 = fmul float %839, %846
	  %849 = add nsw i32 16, %848
	  %850 = srem i32 %849, 128
	  %851 = sext i32 %850 to i64
	  %856 = add nsw i32 40, %855
	  %857 = srem i32 %856, 128
	  %858 = sext i32 %857 to i64
	  %862 = fmul float %854, %861
	  %863 = fdiv float 1.000000e+00, %862
	  %864 = fmul float %847, %863
	  store float %864, float* %l, align 4
	  %866 = add nsw i32 128, %865
	  %867 = srem i32 %866, 128
	  %868 = sext i32 %867 to i64
	  %872 = fpext float %871 to double
	  %874 = fpext float %873 to double
	  %876 = fpext float %875 to double
	  %878 = call double @fmin(double %874, double %876) #6
	  %930 = load float, float* %h, align 4
	  %928 = load float, float* %l, align 4
	  %926 = load float, float* %925, align 4
	  %925 = getelementptr inbounds float, float* %924, i64 %923
	  %924 = load float*, float** %2, align 8
	  %920 = load i32, i32* %z, align 4
	  %916 = load float, float* %915, align 4
	  %915 = getelementptr inbounds float, float* %914, i64 %913
	  %914 = load float*, float** %4, align 8
	  %910 = load i32, i32* %z, align 4
	  %909 = load float, float* %908, align 4
	  %908 = getelementptr inbounds float, float* %907, i64 %906
	  %907 = load float*, float** %4, align 8
	  %903 = load i32, i32* %z, align 4
	  %901 = load float, float* %900, align 4
	  %900 = getelementptr inbounds float, float* %899, i64 %898
	  %899 = load float*, float** %4, align 8
	  %895 = load i32, i32* %z, align 4
	  %894 = load float, float* %893, align 4
	  %893 = getelementptr inbounds float, float* %892, i64 %891
	  %892 = load float*, float** %4, align 8
	  %888 = load i32, i32* %z, align 4
	  %887 = getelementptr inbounds float, float* %886, i64 %885
	  %886 = load float*, float** %3, align 8
	  %882 = load i32, i32* %z, align 4
	  %880 = fmul double %872, %878
	  %881 = fptrunc double %880 to float
	  %883 = add nsw i32 128, %882
	  %884 = srem i32 %883, 128
	  %885 = sext i32 %884 to i64
	  store float %881, float* %887, align 4
	  %889 = add nsw i32 8, %888
	  %890 = srem i32 %889, 128
	  %891 = sext i32 %890 to i64
	  %896 = add nsw i32 48, %895
	  %897 = srem i32 %896, 128
	  %898 = sext i32 %897 to i64
	  %902 = fmul float %894, %901
	  %904 = add nsw i32 0, %903
	  %905 = srem i32 %904, 128
	  %906 = sext i32 %905 to i64
	  %911 = add nsw i32 24, %910
	  %912 = srem i32 %911, 128
	  %913 = sext i32 %912 to i64
	  %917 = fmul float %909, %916
	  %918 = fdiv float 1.000000e+00, %917
	  %919 = fmul float %902, %918
	  store float %919, float* %l, align 4
	  %921 = add nsw i32 136, %920
	  %922 = srem i32 %921, 128
	  %923 = sext i32 %922 to i64
	  %927 = fpext float %926 to double
	  %929 = fpext float %928 to double
	  %931 = fpext float %930 to double
	  %933 = call double @fmin(double %929, double %931) #6
	  %985 = load float, float* %h, align 4
	  %983 = load float, float* %l, align 4
	  %981 = load float, float* %980, align 4
	  %980 = getelementptr inbounds float, float* %979, i64 %978
	  %979 = load float*, float** %2, align 8
	  %975 = load i32, i32* %z, align 4
	  %971 = load float, float* %970, align 4
	  %970 = getelementptr inbounds float, float* %969, i64 %968
	  %969 = load float*, float** %4, align 8
	  %965 = load i32, i32* %z, align 4
	  %964 = load float, float* %963, align 4
	  %963 = getelementptr inbounds float, float* %962, i64 %961
	  %962 = load float*, float** %4, align 8
	  %958 = load i32, i32* %z, align 4
	  %956 = load float, float* %955, align 4
	  %955 = getelementptr inbounds float, float* %954, i64 %953
	  %954 = load float*, float** %4, align 8
	  %950 = load i32, i32* %z, align 4
	  %949 = load float, float* %948, align 4
	  %948 = getelementptr inbounds float, float* %947, i64 %946
	  %947 = load float*, float** %4, align 8
	  %943 = load i32, i32* %z, align 4
	  %942 = getelementptr inbounds float, float* %941, i64 %940
	  %941 = load float*, float** %3, align 8
	  %937 = load i32, i32* %z, align 4
	  %935 = fmul double %927, %933
	  %936 = fptrunc double %935 to float
	  %938 = add nsw i32 136, %937
	  %939 = srem i32 %938, 128
	  %940 = sext i32 %939 to i64
	  store float %936, float* %942, align 4
	  %944 = add nsw i32 8, %943
	  %945 = srem i32 %944, 128
	  %946 = sext i32 %945 to i64
	  %951 = add nsw i32 48, %950
	  %952 = srem i32 %951, 128
	  %953 = sext i32 %952 to i64
	  %957 = fmul float %949, %956
	  %959 = add nsw i32 32, %958
	  %960 = srem i32 %959, 128
	  %961 = sext i32 %960 to i64
	  %966 = add nsw i32 32, %965
	  %967 = srem i32 %966, 128
	  %968 = sext i32 %967 to i64
	  %972 = fmul float %964, %971
	  %973 = fdiv float 1.000000e+00, %972
	  %974 = fmul float %957, %973
	  store float %974, float* %l, align 4
	  %976 = add nsw i32 144, %975
	  %977 = srem i32 %976, 128
	  %978 = sext i32 %977 to i64
	  %982 = fpext float %981 to double
	  %984 = fpext float %983 to double
	  %986 = fpext float %985 to double
	  %988 = call double @fmin(double %984, double %986) #6
	  %1040 = load float, float* %h, align 4
	  %1038 = load float, float* %l, align 4
	  %1036 = load float, float* %1035, align 4
	  %1035 = getelementptr inbounds float, float* %1034, i64 %1033
	  %1034 = load float*, float** %2, align 8
	  %1030 = load i32, i32* %z, align 4
	  %1026 = load float, float* %1025, align 4
	  %1025 = getelementptr inbounds float, float* %1024, i64 %1023
	  %1024 = load float*, float** %4, align 8
	  %1020 = load i32, i32* %z, align 4
	  %1019 = load float, float* %1018, align 4
	  %1018 = getelementptr inbounds float, float* %1017, i64 %1016
	  %1017 = load float*, float** %4, align 8
	  %1013 = load i32, i32* %z, align 4
	  %1011 = load float, float* %1010, align 4
	  %1010 = getelementptr inbounds float, float* %1009, i64 %1008
	  %1009 = load float*, float** %4, align 8
	  %1005 = load i32, i32* %z, align 4
	  %1004 = load float, float* %1003, align 4
	  %1003 = getelementptr inbounds float, float* %1002, i64 %1001
	  %1002 = load float*, float** %4, align 8
	  %998 = load i32, i32* %z, align 4
	  %997 = getelementptr inbounds float, float* %996, i64 %995
	  %996 = load float*, float** %3, align 8
	  %992 = load i32, i32* %z, align 4
	  %990 = fmul double %982, %988
	  %991 = fptrunc double %990 to float
	  %993 = add nsw i32 144, %992
	  %994 = srem i32 %993, 128
	  %995 = sext i32 %994 to i64
	  store float %991, float* %997, align 4
	  %999 = add nsw i32 16, %998
	  %1000 = srem i32 %999, 128
	  %1001 = sext i32 %1000 to i64
	  %1006 = add nsw i32 48, %1005
	  %1007 = srem i32 %1006, 128
	  %1008 = sext i32 %1007 to i64
	  %1012 = fmul float %1004, %1011
	  %1014 = add nsw i32 24, %1013
	  %1015 = srem i32 %1014, 128
	  %1016 = sext i32 %1015 to i64
	  %1021 = add nsw i32 32, %1020
	  %1022 = srem i32 %1021, 128
	  %1023 = sext i32 %1022 to i64
	  %1027 = fmul float %1019, %1026
	  %1028 = fdiv float 1.000000e+00, %1027
	  %1029 = fmul float %1012, %1028
	  store float %1029, float* %l, align 4
	  %1031 = add nsw i32 152, %1030
	  %1032 = srem i32 %1031, 128
	  %1033 = sext i32 %1032 to i64
	  %1037 = fpext float %1036 to double
	  %1039 = fpext float %1038 to double
	  %1041 = fpext float %1040 to double
	  %1043 = call double @fmin(double %1039, double %1041) #6
	  %1095 = load float, float* %h, align 4
	  %1093 = load float, float* %l, align 4
	  %1091 = load float, float* %1090, align 4
	  %1090 = getelementptr inbounds float, float* %1089, i64 %1088
	  %1089 = load float*, float** %2, align 8
	  %1085 = load i32, i32* %z, align 4
	  %1081 = load float, float* %1080, align 4
	  %1080 = getelementptr inbounds float, float* %1079, i64 %1078
	  %1079 = load float*, float** %4, align 8
	  %1075 = load i32, i32* %z, align 4
	  %1074 = load float, float* %1073, align 4
	  %1073 = getelementptr inbounds float, float* %1072, i64 %1071
	  %1072 = load float*, float** %4, align 8
	  %1068 = load i32, i32* %z, align 4
	  %1066 = load float, float* %1065, align 4
	  %1065 = getelementptr inbounds float, float* %1064, i64 %1063
	  %1064 = load float*, float** %4, align 8
	  %1060 = load i32, i32* %z, align 4
	  %1059 = load float, float* %1058, align 4
	  %1058 = getelementptr inbounds float, float* %1057, i64 %1056
	  %1057 = load float*, float** %4, align 8
	  %1053 = load i32, i32* %z, align 4
	  %1052 = getelementptr inbounds float, float* %1051, i64 %1050
	  %1051 = load float*, float** %3, align 8
	  %1047 = load i32, i32* %z, align 4
	  %1045 = fmul double %1037, %1043
	  %1046 = fptrunc double %1045 to float
	  %1048 = add nsw i32 152, %1047
	  %1049 = srem i32 %1048, 128
	  %1050 = sext i32 %1049 to i64
	  store float %1046, float* %1052, align 4
	  %1054 = add nsw i32 32, %1053
	  %1055 = srem i32 %1054, 128
	  %1056 = sext i32 %1055 to i64
	  %1061 = add nsw i32 48, %1060
	  %1062 = srem i32 %1061, 128
	  %1063 = sext i32 %1062 to i64
	  %1067 = fmul float %1059, %1066
	  %1069 = add nsw i32 24, %1068
	  %1070 = srem i32 %1069, 128
	  %1071 = sext i32 %1070 to i64
	  %1076 = add nsw i32 40, %1075
	  %1077 = srem i32 %1076, 128
	  %1078 = sext i32 %1077 to i64
	  %1082 = fmul float %1074, %1081
	  %1083 = fdiv float 1.000000e+00, %1082
	  %1084 = fmul float %1067, %1083
	  store float %1084, float* %l, align 4
	  %1086 = add nsw i32 160, %1085
	  %1087 = srem i32 %1086, 128
	  %1088 = sext i32 %1087 to i64
	  %1092 = fpext float %1091 to double
	  %1094 = fpext float %1093 to double
	  %1096 = fpext float %1095 to double
	  %1098 = call double @fmin(double %1094, double %1096) #6
	  %1150 = load float, float* %h, align 4
	  %1148 = load float, float* %l, align 4
	  %1146 = load float, float* %1145, align 4
	  %1145 = getelementptr inbounds float, float* %1144, i64 %1143
	  %1144 = load float*, float** %2, align 8
	  %1140 = load i32, i32* %z, align 4
	  %1136 = load float, float* %1135, align 4
	  %1135 = getelementptr inbounds float, float* %1134, i64 %1133
	  %1134 = load float*, float** %4, align 8
	  %1130 = load i32, i32* %z, align 4
	  %1129 = load float, float* %1128, align 4
	  %1128 = getelementptr inbounds float, float* %1127, i64 %1126
	  %1127 = load float*, float** %4, align 8
	  %1123 = load i32, i32* %z, align 4
	  %1121 = load float, float* %1120, align 4
	  %1120 = getelementptr inbounds float, float* %1119, i64 %1118
	  %1119 = load float*, float** %4, align 8
	  %1115 = load i32, i32* %z, align 4
	  %1114 = load float, float* %1113, align 4
	  %1113 = getelementptr inbounds float, float* %1112, i64 %1111
	  %1112 = load float*, float** %4, align 8
	  %1108 = load i32, i32* %z, align 4
	  %1107 = getelementptr inbounds float, float* %1106, i64 %1105
	  %1106 = load float*, float** %3, align 8
	  %1102 = load i32, i32* %z, align 4
	  %1100 = fmul double %1092, %1098
	  %1101 = fptrunc double %1100 to float
	  %1103 = add nsw i32 160, %1102
	  %1104 = srem i32 %1103, 128
	  %1105 = sext i32 %1104 to i64
	  store float %1101, float* %1107, align 4
	  %1109 = add nsw i32 48, %1108
	  %1110 = srem i32 %1109, 128
	  %1111 = sext i32 %1110 to i64
	  %1116 = add nsw i32 48, %1115
	  %1117 = srem i32 %1116, 128
	  %1118 = sext i32 %1117 to i64
	  %1122 = fmul float %1114, %1121
	  %1124 = add nsw i32 24, %1123
	  %1125 = srem i32 %1124, 128
	  %1126 = sext i32 %1125 to i64
	  %1131 = add nsw i32 56, %1130
	  %1132 = srem i32 %1131, 128
	  %1133 = sext i32 %1132 to i64
	  %1137 = fmul float %1129, %1136
	  %1138 = fdiv float 1.000000e+00, %1137
	  %1139 = fmul float %1122, %1138
	  store float %1139, float* %l, align 4
	  %1141 = add nsw i32 168, %1140
	  %1142 = srem i32 %1141, 128
	  %1143 = sext i32 %1142 to i64
	  %1147 = fpext float %1146 to double
	  %1149 = fpext float %1148 to double
	  %1151 = fpext float %1150 to double
	  %1153 = call double @fmin(double %1149, double %1151) #6
	  %1205 = load float, float* %h, align 4
	  %1203 = load float, float* %l, align 4
	  %1201 = load float, float* %1200, align 4
	  %1200 = getelementptr inbounds float, float* %1199, i64 %1198
	  %1199 = load float*, float** %2, align 8
	  %1195 = load i32, i32* %z, align 4
	  %1191 = load float, float* %1190, align 4
	  %1190 = getelementptr inbounds float, float* %1189, i64 %1188
	  %1189 = load float*, float** %4, align 8
	  %1185 = load i32, i32* %z, align 4
	  %1184 = load float, float* %1183, align 4
	  %1183 = getelementptr inbounds float, float* %1182, i64 %1181
	  %1182 = load float*, float** %4, align 8
	  %1178 = load i32, i32* %z, align 4
	  %1176 = load float, float* %1175, align 4
	  %1175 = getelementptr inbounds float, float* %1174, i64 %1173
	  %1174 = load float*, float** %4, align 8
	  %1170 = load i32, i32* %z, align 4
	  %1169 = load float, float* %1168, align 4
	  %1168 = getelementptr inbounds float, float* %1167, i64 %1166
	  %1167 = load float*, float** %4, align 8
	  %1163 = load i32, i32* %z, align 4
	  %1162 = getelementptr inbounds float, float* %1161, i64 %1160
	  %1161 = load float*, float** %3, align 8
	  %1157 = load i32, i32* %z, align 4
	  %1155 = fmul double %1147, %1153
	  %1156 = fptrunc double %1155 to float
	  %1158 = add nsw i32 168, %1157
	  %1159 = srem i32 %1158, 128
	  %1160 = sext i32 %1159 to i64
	  store float %1156, float* %1162, align 4
	  %1164 = add nsw i32 48, %1163
	  %1165 = srem i32 %1164, 128
	  %1166 = sext i32 %1165 to i64
	  %1171 = add nsw i32 48, %1170
	  %1172 = srem i32 %1171, 128
	  %1173 = sext i32 %1172 to i64
	  %1177 = fmul float %1169, %1176
	  %1179 = add nsw i32 24, %1178
	  %1180 = srem i32 %1179, 128
	  %1181 = sext i32 %1180 to i64
	  %1186 = add nsw i32 56, %1185
	  %1187 = srem i32 %1186, 128
	  %1188 = sext i32 %1187 to i64
	  %1192 = fmul float %1184, %1191
	  %1193 = fdiv float 1.000000e+00, %1192
	  %1194 = fmul float %1177, %1193
	  store float %1194, float* %l, align 4
	  %1196 = add nsw i32 176, %1195
	  %1197 = srem i32 %1196, 128
	  %1198 = sext i32 %1197 to i64
	  %1202 = fpext float %1201 to double
	  %1204 = fpext float %1203 to double
	  %1206 = fpext float %1205 to double
	  %1208 = call double @fmin(double %1204, double %1206) #6
	  %1260 = load float, float* %h, align 4
	  %1258 = load float, float* %l, align 4
	  %1256 = load float, float* %1255, align 4
	  %1255 = getelementptr inbounds float, float* %1254, i64 %1253
	  %1254 = load float*, float** %2, align 8
	  %1250 = load i32, i32* %z, align 4
	  %1246 = load float, float* %1245, align 4
	  %1245 = getelementptr inbounds float, float* %1244, i64 %1243
	  %1244 = load float*, float** %4, align 8
	  %1240 = load i32, i32* %z, align 4
	  %1239 = load float, float* %1238, align 4
	  %1238 = getelementptr inbounds float, float* %1237, i64 %1236
	  %1237 = load float*, float** %4, align 8
	  %1233 = load i32, i32* %z, align 4
	  %1231 = load float, float* %1230, align 4
	  %1230 = getelementptr inbounds float, float* %1229, i64 %1228
	  %1229 = load float*, float** %4, align 8
	  %1225 = load i32, i32* %z, align 4
	  %1224 = load float, float* %1223, align 4
	  %1223 = getelementptr inbounds float, float* %1222, i64 %1221
	  %1222 = load float*, float** %4, align 8
	  %1218 = load i32, i32* %z, align 4
	  %1217 = getelementptr inbounds float, float* %1216, i64 %1215
	  %1216 = load float*, float** %3, align 8
	  %1212 = load i32, i32* %z, align 4
	  %1210 = fmul double %1202, %1208
	  %1211 = fptrunc double %1210 to float
	  %1213 = add nsw i32 176, %1212
	  %1214 = srem i32 %1213, 128
	  %1215 = sext i32 %1214 to i64
	  store float %1211, float* %1217, align 4
	  %1219 = add nsw i32 8, %1218
	  %1220 = srem i32 %1219, 128
	  %1221 = sext i32 %1220 to i64
	  %1226 = add nsw i32 56, %1225
	  %1227 = srem i32 %1226, 128
	  %1228 = sext i32 %1227 to i64
	  %1232 = fmul float %1224, %1231
	  %1234 = add nsw i32 0, %1233
	  %1235 = srem i32 %1234, 128
	  %1236 = sext i32 %1235 to i64
	  %1241 = add nsw i32 48, %1240
	  %1242 = srem i32 %1241, 128
	  %1243 = sext i32 %1242 to i64
	  %1247 = fmul float %1239, %1246
	  %1248 = fdiv float 1.000000e+00, %1247
	  %1249 = fmul float %1232, %1248
	  store float %1249, float* %l, align 4
	  %1251 = add nsw i32 184, %1250
	  %1252 = srem i32 %1251, 128
	  %1253 = sext i32 %1252 to i64
	  %1257 = fpext float %1256 to double
	  %1259 = fpext float %1258 to double
	  %1261 = fpext float %1260 to double
	  %1263 = call double @fmin(double %1259, double %1261) #6
	  %1315 = load float, float* %h, align 4
	  %1313 = load float, float* %l, align 4
	  %1311 = load float, float* %1310, align 4
	  %1310 = getelementptr inbounds float, float* %1309, i64 %1308
	  %1309 = load float*, float** %2, align 8
	  %1305 = load i32, i32* %z, align 4
	  %1301 = load float, float* %1300, align 4
	  %1300 = getelementptr inbounds float, float* %1299, i64 %1298
	  %1299 = load float*, float** %4, align 8
	  %1295 = load i32, i32* %z, align 4
	  %1294 = load float, float* %1293, align 4
	  %1293 = getelementptr inbounds float, float* %1292, i64 %1291
	  %1292 = load float*, float** %4, align 8
	  %1288 = load i32, i32* %z, align 4
	  %1286 = load float, float* %1285, align 4
	  %1285 = getelementptr inbounds float, float* %1284, i64 %1283
	  %1284 = load float*, float** %4, align 8
	  %1280 = load i32, i32* %z, align 4
	  %1279 = load float, float* %1278, align 4
	  %1278 = getelementptr inbounds float, float* %1277, i64 %1276
	  %1277 = load float*, float** %4, align 8
	  %1273 = load i32, i32* %z, align 4
	  %1272 = getelementptr inbounds float, float* %1271, i64 %1270
	  %1271 = load float*, float** %3, align 8
	  %1267 = load i32, i32* %z, align 4
	  %1265 = fmul double %1257, %1263
	  %1266 = fptrunc double %1265 to float
	  %1268 = add nsw i32 184, %1267
	  %1269 = srem i32 %1268, 128
	  %1270 = sext i32 %1269 to i64
	  store float %1266, float* %1272, align 4
	  %1274 = add nsw i32 8, %1273
	  %1275 = srem i32 %1274, 128
	  %1276 = sext i32 %1275 to i64
	  %1281 = add nsw i32 56, %1280
	  %1282 = srem i32 %1281, 128
	  %1283 = sext i32 %1282 to i64
	  %1287 = fmul float %1279, %1286
	  %1289 = add nsw i32 32, %1288
	  %1290 = srem i32 %1289, 128
	  %1291 = sext i32 %1290 to i64
	  %1296 = add nsw i32 40, %1295
	  %1297 = srem i32 %1296, 128
	  %1298 = sext i32 %1297 to i64
	  %1302 = fmul float %1294, %1301
	  %1303 = fdiv float 1.000000e+00, %1302
	  %1304 = fmul float %1287, %1303
	  store float %1304, float* %l, align 4
	  %1306 = add nsw i32 192, %1305
	  %1307 = srem i32 %1306, 128
	  %1308 = sext i32 %1307 to i64
	  %1312 = fpext float %1311 to double
	  %1314 = fpext float %1313 to double
	  %1316 = fpext float %1315 to double
	  %1318 = call double @fmin(double %1314, double %1316) #6
	  %1327 = getelementptr inbounds float, float* %1326, i64 %1325
	  %1326 = load float*, float** %3, align 8
	  %1322 = load i32, i32* %z, align 4
	  %1320 = fmul double %1312, %1318
	  %1321 = fptrunc double %1320 to float
	  %1323 = add nsw i32 192, %1322
	  %1324 = srem i32 %1323, 128
	  %1325 = sext i32 %1324 to i64
	  store float %1321, float* %1327, align 4
