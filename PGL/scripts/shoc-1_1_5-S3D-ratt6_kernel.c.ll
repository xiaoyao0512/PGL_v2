	  %a = alloca [16384 x float], align 16
	  %b = alloca [16384 x float], align 16
	  %c = alloca [16384 x float], align 16
	  %d = alloca [16384 x float], align 16
	  %e = alloca float, align 4
	  %1 = bitcast [16384 x float]* %a to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* bitcast ([16384 x float]* @main.a to i8*), i64 65536, i32 16, i1 false)
	  %4 = bitcast [16384 x float]* %b to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %4, i8* bitcast ([16384 x float]* @main.b to i8*), i64 65536, i32 16, i1 false)
	  %7 = bitcast [16384 x float]* %c to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %7, i8* bitcast ([16384 x float]* @main.c to i8*), i64 65536, i32 16, i1 false)
	  %10 = bitcast [16384 x float]* %d to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %10, i8* bitcast ([16384 x float]* @main.d to i8*), i64 65536, i32 16, i1 false)
	  %17 = load float, float* %e, align 4
	  %16 = getelementptr inbounds [16384 x float], [16384 x float]* %d, i32 0, i32 0
	  %15 = getelementptr inbounds [16384 x float], [16384 x float]* %c, i32 0, i32 0
	  %14 = getelementptr inbounds [16384 x float], [16384 x float]* %b, i32 0, i32 0
	  %13 = getelementptr inbounds [16384 x float], [16384 x float]* %a, i32 0, i32 0
	store float* %13, float** %a, align 8
	store  float* %14, float** %b, align 8
	store  float* %15, float** %c, align 8
	store  float* %16, float** %d, align 8
	store  float %17, float* %e, align 8
	  store float 1.000000e+00, float* %e, align 4
	  call void @A(float* %13, float* %14, float* %15, float* %16, float %17)
	  %13 = load float, float* %f, align 4
	  %11 = load float, float* %5, align 4
	  %10 = load float, float* %9, align 4
	  %9 = getelementptr inbounds float, float* %8, i64 %7
	  %8 = load float*, float** %1, align 8
	  %6 = load i32, i32* %z, align 4
	  %1 = alloca float*, align 8
	  %2 = alloca float*, align 8
	  %3 = alloca float*, align 8
	  %4 = alloca float*, align 8
	  %5 = alloca float, align 4
	  %z = alloca i32, align 4
	  %f = alloca float, align 4
	  %g = alloca float, align 4
	  %h = alloca float, align 4
	  %i = alloca float, align 4
	  %j = alloca float, align 4
	  %k = alloca float, align 4
	  %l = alloca float, align 4
	  store float* %a, float** %1, align 8
	  store float* %b, float** %2, align 8
	  store float* %c, float** %3, align 8
	  store float* %d, float** %4, align 8
	  store float %e, float* %5, align 4
	  store i32 0, i32* %z, align 4
	  %7 = sext i32 %6 to i64
	  %12 = fmul float %10, %11
	  store float %12, float* %f, align 4
	  %14 = fpext float %13 to double
	  %16 = call double @log(double %14) #4
	  %67 = load float, float* %h, align 4
	  %65 = load float, float* %l, align 4
	  %63 = load float, float* %62, align 4
	  %62 = getelementptr inbounds float, float* %61, i64 %60
	  %61 = load float*, float** %2, align 8
	  %57 = load i32, i32* %z, align 4
	  %53 = load float, float* %52, align 4
	  %52 = getelementptr inbounds float, float* %51, i64 %50
	  %51 = load float*, float** %4, align 8
	  %47 = load i32, i32* %z, align 4
	  %46 = load float, float* %45, align 4
	  %45 = getelementptr inbounds float, float* %44, i64 %43
	  %44 = load float*, float** %4, align 8
	  %40 = load i32, i32* %z, align 4
	  %38 = load float, float* %37, align 4
	  %37 = getelementptr inbounds float, float* %36, i64 %35
	  %36 = load float*, float** %4, align 8
	  %32 = load i32, i32* %z, align 4
	  %31 = load float, float* %30, align 4
	  %30 = getelementptr inbounds float, float* %29, i64 %28
	  %29 = load float*, float** %4, align 8
	  %25 = load i32, i32* %z, align 4
	  %21 = load float, float* %f, align 4
	  %20 = load float, float* %i, align 4
	  %19 = load float, float* %j, align 4
	  %18 = fptrunc double %16 to float
	  store float %18, float* %g, align 4
	  store float 0x4415AF1D80000000, float* %h, align 4
	  store float 0x4193D2C640000000, float* %i, align 4
	  store float 1.013250e+06, float* %j, align 4
	  %22 = fmul float %20, %21
	  %23 = fdiv float 1.000000e+00, %22
	  %24 = fmul float %19, %23
	  store float %24, float* %k, align 4
	  %26 = add nsw i32 24, %25
	  %27 = srem i32 %26, 128
	  %28 = sext i32 %27 to i64
	  %33 = add nsw i32 136, %32
	  %34 = srem i32 %33, 128
	  %35 = sext i32 %34 to i64
	  %39 = fmul float %31, %38
	  %41 = add nsw i32 48, %40
	  %42 = srem i32 %41, 128
	  %43 = sext i32 %42 to i64
	  %48 = add nsw i32 128, %47
	  %49 = srem i32 %48, 128
	  %50 = sext i32 %49 to i64
	  %54 = fmul float %46, %53
	  %55 = fdiv float 1.000000e+00, %54
	  %56 = fmul float %39, %55
	  store float %56, float* %l, align 4
	  %58 = add nsw i32 800, %57
	  %59 = srem i32 %58, 128
	  %60 = sext i32 %59 to i64
	  %64 = fpext float %63 to double
	  %66 = fpext float %65 to double
	  %68 = fpext float %67 to double
	  %70 = call double @fmin(double %66, double %68) #6
	  %122 = load float, float* %h, align 4
	  %120 = load float, float* %l, align 4
	  %118 = load float, float* %117, align 4
	  %117 = getelementptr inbounds float, float* %116, i64 %115
	  %116 = load float*, float** %2, align 8
	  %112 = load i32, i32* %z, align 4
	  %108 = load float, float* %107, align 4
	  %107 = getelementptr inbounds float, float* %106, i64 %105
	  %106 = load float*, float** %4, align 8
	  %102 = load i32, i32* %z, align 4
	  %101 = load float, float* %100, align 4
	  %100 = getelementptr inbounds float, float* %99, i64 %98
	  %99 = load float*, float** %4, align 8
	  %95 = load i32, i32* %z, align 4
	  %93 = load float, float* %92, align 4
	  %92 = getelementptr inbounds float, float* %91, i64 %90
	  %91 = load float*, float** %4, align 8
	  %87 = load i32, i32* %z, align 4
	  %86 = load float, float* %85, align 4
	  %85 = getelementptr inbounds float, float* %84, i64 %83
	  %84 = load float*, float** %4, align 8
	  %80 = load i32, i32* %z, align 4
	  %79 = getelementptr inbounds float, float* %78, i64 %77
	  %78 = load float*, float** %3, align 8
	  %74 = load i32, i32* %z, align 4
	  %72 = fmul double %64, %70
	  %73 = fptrunc double %72 to float
	  %75 = add nsw i32 800, %74
	  %76 = srem i32 %75, 128
	  %77 = sext i32 %76 to i64
	  store float %73, float* %79, align 4
	  %81 = add nsw i32 8, %80
	  %82 = srem i32 %81, 128
	  %83 = sext i32 %82 to i64
	  %88 = add nsw i32 96, %87
	  %89 = srem i32 %88, 128
	  %90 = sext i32 %89 to i64
	  %94 = fmul float %86, %93
	  %96 = add nsw i32 0, %95
	  %97 = srem i32 %96, 128
	  %98 = sext i32 %97 to i64
	  %103 = add nsw i32 88, %102
	  %104 = srem i32 %103, 128
	  %105 = sext i32 %104 to i64
	  %109 = fmul float %101, %108
	  %110 = fdiv float 1.000000e+00, %109
	  %111 = fmul float %94, %110
	  store float %111, float* %l, align 4
	  %113 = add nsw i32 808, %112
	  %114 = srem i32 %113, 128
	  %115 = sext i32 %114 to i64
	  %119 = fpext float %118 to double
	  %121 = fpext float %120 to double
	  %123 = fpext float %122 to double
	  %125 = call double @fmin(double %121, double %123) #6
	  %177 = load float, float* %h, align 4
	  %175 = load float, float* %l, align 4
	  %173 = load float, float* %172, align 4
	  %172 = getelementptr inbounds float, float* %171, i64 %170
	  %171 = load float*, float** %2, align 8
	  %167 = load i32, i32* %z, align 4
	  %163 = load float, float* %162, align 4
	  %162 = getelementptr inbounds float, float* %161, i64 %160
	  %161 = load float*, float** %4, align 8
	  %157 = load i32, i32* %z, align 4
	  %156 = load float, float* %155, align 4
	  %155 = getelementptr inbounds float, float* %154, i64 %153
	  %154 = load float*, float** %4, align 8
	  %150 = load i32, i32* %z, align 4
	  %148 = load float, float* %147, align 4
	  %147 = getelementptr inbounds float, float* %146, i64 %145
	  %146 = load float*, float** %4, align 8
	  %142 = load i32, i32* %z, align 4
	  %141 = load float, float* %140, align 4
	  %140 = getelementptr inbounds float, float* %139, i64 %138
	  %139 = load float*, float** %4, align 8
	  %135 = load i32, i32* %z, align 4
	  %134 = getelementptr inbounds float, float* %133, i64 %132
	  %133 = load float*, float** %3, align 8
	  %129 = load i32, i32* %z, align 4
	  %127 = fmul double %119, %125
	  %128 = fptrunc double %127 to float
	  %130 = add nsw i32 808, %129
	  %131 = srem i32 %130, 128
	  %132 = sext i32 %131 to i64
	  store float %128, float* %134, align 4
	  %136 = add nsw i32 16, %135
	  %137 = srem i32 %136, 128
	  %138 = sext i32 %137 to i64
	  %143 = add nsw i32 96, %142
	  %144 = srem i32 %143, 128
	  %145 = sext i32 %144 to i64
	  %149 = fmul float %141, %148
	  %151 = add nsw i32 32, %150
	  %152 = srem i32 %151, 128
	  %153 = sext i32 %152 to i64
	  %158 = add nsw i32 88, %157
	  %159 = srem i32 %158, 128
	  %160 = sext i32 %159 to i64
	  %164 = fmul float %156, %163
	  %165 = fdiv float 1.000000e+00, %164
	  %166 = fmul float %149, %165
	  store float %166, float* %l, align 4
	  %168 = add nsw i32 816, %167
	  %169 = srem i32 %168, 128
	  %170 = sext i32 %169 to i64
	  %174 = fpext float %173 to double
	  %176 = fpext float %175 to double
	  %178 = fpext float %177 to double
	  %180 = call double @fmin(double %176, double %178) #6
	  %232 = load float, float* %h, align 4
	  %230 = load float, float* %l, align 4
	  %228 = load float, float* %227, align 4
	  %227 = getelementptr inbounds float, float* %226, i64 %225
	  %226 = load float*, float** %2, align 8
	  %222 = load i32, i32* %z, align 4
	  %218 = load float, float* %217, align 4
	  %217 = getelementptr inbounds float, float* %216, i64 %215
	  %216 = load float*, float** %4, align 8
	  %212 = load i32, i32* %z, align 4
	  %211 = load float, float* %210, align 4
	  %210 = getelementptr inbounds float, float* %209, i64 %208
	  %209 = load float*, float** %4, align 8
	  %205 = load i32, i32* %z, align 4
	  %203 = load float, float* %202, align 4
	  %202 = getelementptr inbounds float, float* %201, i64 %200
	  %201 = load float*, float** %4, align 8
	  %197 = load i32, i32* %z, align 4
	  %196 = load float, float* %195, align 4
	  %195 = getelementptr inbounds float, float* %194, i64 %193
	  %194 = load float*, float** %4, align 8
	  %190 = load i32, i32* %z, align 4
	  %189 = getelementptr inbounds float, float* %188, i64 %187
	  %188 = load float*, float** %3, align 8
	  %184 = load i32, i32* %z, align 4
	  %182 = fmul double %174, %180
	  %183 = fptrunc double %182 to float
	  %185 = add nsw i32 816, %184
	  %186 = srem i32 %185, 128
	  %187 = sext i32 %186 to i64
	  store float %183, float* %189, align 4
	  %191 = add nsw i32 32, %190
	  %192 = srem i32 %191, 128
	  %193 = sext i32 %192 to i64
	  %198 = add nsw i32 96, %197
	  %199 = srem i32 %198, 128
	  %200 = sext i32 %199 to i64
	  %204 = fmul float %196, %203
	  %206 = add nsw i32 40, %205
	  %207 = srem i32 %206, 128
	  %208 = sext i32 %207 to i64
	  %213 = add nsw i32 88, %212
	  %214 = srem i32 %213, 128
	  %215 = sext i32 %214 to i64
	  %219 = fmul float %211, %218
	  %220 = fdiv float 1.000000e+00, %219
	  %221 = fmul float %204, %220
	  store float %221, float* %l, align 4
	  %223 = add nsw i32 824, %222
	  %224 = srem i32 %223, 128
	  %225 = sext i32 %224 to i64
	  %229 = fpext float %228 to double
	  %231 = fpext float %230 to double
	  %233 = fpext float %232 to double
	  %235 = call double @fmin(double %231, double %233) #6
	  %287 = load float, float* %h, align 4
	  %285 = load float, float* %l, align 4
	  %283 = load float, float* %282, align 4
	  %282 = getelementptr inbounds float, float* %281, i64 %280
	  %281 = load float*, float** %2, align 8
	  %277 = load i32, i32* %z, align 4
	  %273 = load float, float* %272, align 4
	  %272 = getelementptr inbounds float, float* %271, i64 %270
	  %271 = load float*, float** %4, align 8
	  %267 = load i32, i32* %z, align 4
	  %266 = load float, float* %265, align 4
	  %265 = getelementptr inbounds float, float* %264, i64 %263
	  %264 = load float*, float** %4, align 8
	  %260 = load i32, i32* %z, align 4
	  %258 = load float, float* %257, align 4
	  %257 = getelementptr inbounds float, float* %256, i64 %255
	  %256 = load float*, float** %4, align 8
	  %252 = load i32, i32* %z, align 4
	  %251 = load float, float* %250, align 4
	  %250 = getelementptr inbounds float, float* %249, i64 %248
	  %249 = load float*, float** %4, align 8
	  %245 = load i32, i32* %z, align 4
	  %244 = getelementptr inbounds float, float* %243, i64 %242
	  %243 = load float*, float** %3, align 8
	  %239 = load i32, i32* %z, align 4
	  %237 = fmul double %229, %235
	  %238 = fptrunc double %237 to float
	  %240 = add nsw i32 824, %239
	  %241 = srem i32 %240, 128
	  %242 = sext i32 %241 to i64
	  store float %238, float* %244, align 4
	  %246 = add nsw i32 64, %245
	  %247 = srem i32 %246, 128
	  %248 = sext i32 %247 to i64
	  %253 = add nsw i32 96, %252
	  %254 = srem i32 %253, 128
	  %255 = sext i32 %254 to i64
	  %259 = fmul float %251, %258
	  %261 = add nsw i32 8, %260
	  %262 = srem i32 %261, 128
	  %263 = sext i32 %262 to i64
	  %268 = add nsw i32 168, %267
	  %269 = srem i32 %268, 128
	  %270 = sext i32 %269 to i64
	  %274 = fmul float %266, %273
	  %275 = fdiv float 1.000000e+00, %274
	  %276 = fmul float %259, %275
	  store float %276, float* %l, align 4
	  %278 = add nsw i32 832, %277
	  %279 = srem i32 %278, 128
	  %280 = sext i32 %279 to i64
	  %284 = fpext float %283 to double
	  %286 = fpext float %285 to double
	  %288 = fpext float %287 to double
	  %290 = call double @fmin(double %286, double %288) #6
	  %342 = load float, float* %h, align 4
	  %340 = load float, float* %l, align 4
	  %338 = load float, float* %337, align 4
	  %337 = getelementptr inbounds float, float* %336, i64 %335
	  %336 = load float*, float** %2, align 8
	  %332 = load i32, i32* %z, align 4
	  %328 = load float, float* %327, align 4
	  %327 = getelementptr inbounds float, float* %326, i64 %325
	  %326 = load float*, float** %4, align 8
	  %322 = load i32, i32* %z, align 4
	  %321 = load float, float* %320, align 4
	  %320 = getelementptr inbounds float, float* %319, i64 %318
	  %319 = load float*, float** %4, align 8
	  %315 = load i32, i32* %z, align 4
	  %313 = load float, float* %312, align 4
	  %312 = getelementptr inbounds float, float* %311, i64 %310
	  %311 = load float*, float** %4, align 8
	  %307 = load i32, i32* %z, align 4
	  %306 = load float, float* %305, align 4
	  %305 = getelementptr inbounds float, float* %304, i64 %303
	  %304 = load float*, float** %4, align 8
	  %300 = load i32, i32* %z, align 4
	  %299 = getelementptr inbounds float, float* %298, i64 %297
	  %298 = load float*, float** %3, align 8
	  %294 = load i32, i32* %z, align 4
	  %292 = fmul double %284, %290
	  %293 = fptrunc double %292 to float
	  %295 = add nsw i32 832, %294
	  %296 = srem i32 %295, 128
	  %297 = sext i32 %296 to i64
	  store float %293, float* %299, align 4
	  %301 = add nsw i32 72, %300
	  %302 = srem i32 %301, 128
	  %303 = sext i32 %302 to i64
	  %308 = add nsw i32 96, %307
	  %309 = srem i32 %308, 128
	  %310 = sext i32 %309 to i64
	  %314 = fmul float %306, %313
	  %316 = add nsw i32 88, %315
	  %317 = srem i32 %316, 128
	  %318 = sext i32 %317 to i64
	  %323 = add nsw i32 88, %322
	  %324 = srem i32 %323, 128
	  %325 = sext i32 %324 to i64
	  %329 = fmul float %321, %328
	  %330 = fdiv float 1.000000e+00, %329
	  %331 = fmul float %314, %330
	  store float %331, float* %l, align 4
	  %333 = add nsw i32 840, %332
	  %334 = srem i32 %333, 128
	  %335 = sext i32 %334 to i64
	  %339 = fpext float %338 to double
	  %341 = fpext float %340 to double
	  %343 = fpext float %342 to double
	  %345 = call double @fmin(double %341, double %343) #6
	  %397 = load float, float* %h, align 4
	  %395 = load float, float* %l, align 4
	  %393 = load float, float* %392, align 4
	  %392 = getelementptr inbounds float, float* %391, i64 %390
	  %391 = load float*, float** %2, align 8
	  %387 = load i32, i32* %z, align 4
	  %383 = load float, float* %382, align 4
	  %382 = getelementptr inbounds float, float* %381, i64 %380
	  %381 = load float*, float** %4, align 8
	  %377 = load i32, i32* %z, align 4
	  %376 = load float, float* %375, align 4
	  %375 = getelementptr inbounds float, float* %374, i64 %373
	  %374 = load float*, float** %4, align 8
	  %370 = load i32, i32* %z, align 4
	  %368 = load float, float* %367, align 4
	  %367 = getelementptr inbounds float, float* %366, i64 %365
	  %366 = load float*, float** %4, align 8
	  %362 = load i32, i32* %z, align 4
	  %361 = load float, float* %360, align 4
	  %360 = getelementptr inbounds float, float* %359, i64 %358
	  %359 = load float*, float** %4, align 8
	  %355 = load i32, i32* %z, align 4
	  %354 = getelementptr inbounds float, float* %353, i64 %352
	  %353 = load float*, float** %3, align 8
	  %349 = load i32, i32* %z, align 4
	  %347 = fmul double %339, %345
	  %348 = fptrunc double %347 to float
	  %350 = add nsw i32 840, %349
	  %351 = srem i32 %350, 128
	  %352 = sext i32 %351 to i64
	  store float %348, float* %354, align 4
	  %356 = add nsw i32 80, %355
	  %357 = srem i32 %356, 128
	  %358 = sext i32 %357 to i64
	  %363 = add nsw i32 96, %362
	  %364 = srem i32 %363, 128
	  %365 = sext i32 %364 to i64
	  %369 = fmul float %361, %368
	  %371 = add nsw i32 88, %370
	  %372 = srem i32 %371, 128
	  %373 = sext i32 %372 to i64
	  %378 = add nsw i32 88, %377
	  %379 = srem i32 %378, 128
	  %380 = sext i32 %379 to i64
	  %384 = fmul float %376, %383
	  %385 = fdiv float 1.000000e+00, %384
	  %386 = fmul float %369, %385
	  store float %386, float* %l, align 4
	  %388 = add nsw i32 848, %387
	  %389 = srem i32 %388, 128
	  %390 = sext i32 %389 to i64
	  %394 = fpext float %393 to double
	  %396 = fpext float %395 to double
	  %398 = fpext float %397 to double
	  %400 = call double @fmin(double %396, double %398) #6
	  %452 = load float, float* %h, align 4
	  %450 = load float, float* %l, align 4
	  %448 = load float, float* %447, align 4
	  %447 = getelementptr inbounds float, float* %446, i64 %445
	  %446 = load float*, float** %2, align 8
	  %442 = load i32, i32* %z, align 4
	  %438 = load float, float* %437, align 4
	  %437 = getelementptr inbounds float, float* %436, i64 %435
	  %436 = load float*, float** %4, align 8
	  %432 = load i32, i32* %z, align 4
	  %431 = load float, float* %430, align 4
	  %430 = getelementptr inbounds float, float* %429, i64 %428
	  %429 = load float*, float** %4, align 8
	  %425 = load i32, i32* %z, align 4
	  %423 = load float, float* %422, align 4
	  %422 = getelementptr inbounds float, float* %421, i64 %420
	  %421 = load float*, float** %4, align 8
	  %417 = load i32, i32* %z, align 4
	  %416 = load float, float* %415, align 4
	  %415 = getelementptr inbounds float, float* %414, i64 %413
	  %414 = load float*, float** %4, align 8
	  %410 = load i32, i32* %z, align 4
	  %409 = getelementptr inbounds float, float* %408, i64 %407
	  %408 = load float*, float** %3, align 8
	  %404 = load i32, i32* %z, align 4
	  %402 = fmul double %394, %400
	  %403 = fptrunc double %402 to float
	  %405 = add nsw i32 848, %404
	  %406 = srem i32 %405, 128
	  %407 = sext i32 %406 to i64
	  store float %403, float* %409, align 4
	  %411 = add nsw i32 8, %410
	  %412 = srem i32 %411, 128
	  %413 = sext i32 %412 to i64
	  %418 = add nsw i32 192, %417
	  %419 = srem i32 %418, 128
	  %420 = sext i32 %419 to i64
	  %424 = fmul float %416, %423
	  %426 = add nsw i32 80, %425
	  %427 = srem i32 %426, 128
	  %428 = sext i32 %427 to i64
	  %433 = add nsw i32 104, %432
	  %434 = srem i32 %433, 128
	  %435 = sext i32 %434 to i64
	  %439 = fmul float %431, %438
	  %440 = fdiv float 1.000000e+00, %439
	  %441 = fmul float %424, %440
	  store float %441, float* %l, align 4
	  %443 = add nsw i32 856, %442
	  %444 = srem i32 %443, 128
	  %445 = sext i32 %444 to i64
	  %449 = fpext float %448 to double
	  %451 = fpext float %450 to double
	  %453 = fpext float %452 to double
	  %455 = call double @fmin(double %451, double %453) #6
	  %517 = load float, float* %h, align 4
	  %515 = load float, float* %l, align 4
	  %513 = load float, float* %512, align 4
	  %512 = getelementptr inbounds float, float* %511, i64 %510
	  %511 = load float*, float** %2, align 8
	  %507 = load i32, i32* %z, align 4
	  %503 = load float, float* %k, align 4
	  %501 = load float, float* %500, align 4
	  %500 = getelementptr inbounds float, float* %499, i64 %498
	  %499 = load float*, float** %4, align 8
	  %495 = load i32, i32* %z, align 4
	  %493 = load float, float* %492, align 4
	  %492 = getelementptr inbounds float, float* %491, i64 %490
	  %491 = load float*, float** %4, align 8
	  %487 = load i32, i32* %z, align 4
	  %486 = load float, float* %485, align 4
	  %485 = getelementptr inbounds float, float* %484, i64 %483
	  %484 = load float*, float** %4, align 8
	  %480 = load i32, i32* %z, align 4
	  %478 = load float, float* %477, align 4
	  %477 = getelementptr inbounds float, float* %476, i64 %475
	  %476 = load float*, float** %4, align 8
	  %472 = load i32, i32* %z, align 4
	  %471 = load float, float* %470, align 4
	  %470 = getelementptr inbounds float, float* %469, i64 %468
	  %469 = load float*, float** %4, align 8
	  %465 = load i32, i32* %z, align 4
	  %464 = getelementptr inbounds float, float* %463, i64 %462
	  %463 = load float*, float** %3, align 8
	  %459 = load i32, i32* %z, align 4
	  %457 = fmul double %449, %455
	  %458 = fptrunc double %457 to float
	  %460 = add nsw i32 856, %459
	  %461 = srem i32 %460, 128
	  %462 = sext i32 %461 to i64
	  store float %458, float* %464, align 4
	  %466 = add nsw i32 16, %465
	  %467 = srem i32 %466, 128
	  %468 = sext i32 %467 to i64
	  %473 = add nsw i32 192, %472
	  %474 = srem i32 %473, 128
	  %475 = sext i32 %474 to i64
	  %479 = fmul float %471, %478
	  %481 = add nsw i32 8, %480
	  %482 = srem i32 %481, 128
	  %483 = sext i32 %482 to i64
	  %488 = add nsw i32 104, %487
	  %489 = srem i32 %488, 128
	  %490 = sext i32 %489 to i64
	  %494 = fmul float %486, %493
	  %496 = add nsw i32 104, %495
	  %497 = srem i32 %496, 128
	  %498 = sext i32 %497 to i64
	  %502 = fmul float %494, %501
	  %504 = fmul float %502, %503
	  %505 = fdiv float 1.000000e+00, %504
	  %506 = fmul float %479, %505
	  store float %506, float* %l, align 4
	  %508 = add nsw i32 864, %507
	  %509 = srem i32 %508, 128
	  %510 = sext i32 %509 to i64
	  %514 = fpext float %513 to double
	  %516 = fpext float %515 to double
	  %518 = fpext float %517 to double
	  %520 = call double @fmin(double %516, double %518) #6
	  %582 = load float, float* %h, align 4
	  %580 = load float, float* %l, align 4
	  %578 = load float, float* %577, align 4
	  %577 = getelementptr inbounds float, float* %576, i64 %575
	  %576 = load float*, float** %2, align 8
	  %572 = load i32, i32* %z, align 4
	  %568 = load float, float* %k, align 4
	  %566 = load float, float* %565, align 4
	  %565 = getelementptr inbounds float, float* %564, i64 %563
	  %564 = load float*, float** %4, align 8
	  %560 = load i32, i32* %z, align 4
	  %558 = load float, float* %557, align 4
	  %557 = getelementptr inbounds float, float* %556, i64 %555
	  %556 = load float*, float** %4, align 8
	  %552 = load i32, i32* %z, align 4
	  %551 = load float, float* %550, align 4
	  %550 = getelementptr inbounds float, float* %549, i64 %548
	  %549 = load float*, float** %4, align 8
	  %545 = load i32, i32* %z, align 4
	  %543 = load float, float* %542, align 4
	  %542 = getelementptr inbounds float, float* %541, i64 %540
	  %541 = load float*, float** %4, align 8
	  %537 = load i32, i32* %z, align 4
	  %536 = load float, float* %535, align 4
	  %535 = getelementptr inbounds float, float* %534, i64 %533
	  %534 = load float*, float** %4, align 8
	  %530 = load i32, i32* %z, align 4
	  %529 = getelementptr inbounds float, float* %528, i64 %527
	  %528 = load float*, float** %3, align 8
	  %524 = load i32, i32* %z, align 4
	  %522 = fmul double %514, %520
	  %523 = fptrunc double %522 to float
	  %525 = add nsw i32 864, %524
	  %526 = srem i32 %525, 128
	  %527 = sext i32 %526 to i64
	  store float %523, float* %529, align 4
	  %531 = add nsw i32 24, %530
	  %532 = srem i32 %531, 128
	  %533 = sext i32 %532 to i64
	  %538 = add nsw i32 192, %537
	  %539 = srem i32 %538, 128
	  %540 = sext i32 %539 to i64
	  %544 = fmul float %536, %543
	  %546 = add nsw i32 32, %545
	  %547 = srem i32 %546, 128
	  %548 = sext i32 %547 to i64
	  %553 = add nsw i32 104, %552
	  %554 = srem i32 %553, 128
	  %555 = sext i32 %554 to i64
	  %559 = fmul float %551, %558
	  %561 = add nsw i32 104, %560
	  %562 = srem i32 %561, 128
	  %563 = sext i32 %562 to i64
	  %567 = fmul float %559, %566
	  %569 = fmul float %567, %568
	  %570 = fdiv float 1.000000e+00, %569
	  %571 = fmul float %544, %570
	  store float %571, float* %l, align 4
	  %573 = add nsw i32 872, %572
	  %574 = srem i32 %573, 128
	  %575 = sext i32 %574 to i64
	  %579 = fpext float %578 to double
	  %581 = fpext float %580 to double
	  %583 = fpext float %582 to double
	  %585 = call double @fmin(double %581, double %583) #6
	  %637 = load float, float* %h, align 4
	  %635 = load float, float* %l, align 4
	  %633 = load float, float* %632, align 4
	  %632 = getelementptr inbounds float, float* %631, i64 %630
	  %631 = load float*, float** %2, align 8
	  %627 = load i32, i32* %z, align 4
	  %623 = load float, float* %622, align 4
	  %622 = getelementptr inbounds float, float* %621, i64 %620
	  %621 = load float*, float** %4, align 8
	  %617 = load i32, i32* %z, align 4
	  %616 = load float, float* %615, align 4
	  %615 = getelementptr inbounds float, float* %614, i64 %613
	  %614 = load float*, float** %4, align 8
	  %610 = load i32, i32* %z, align 4
	  %608 = load float, float* %607, align 4
	  %607 = getelementptr inbounds float, float* %606, i64 %605
	  %606 = load float*, float** %4, align 8
	  %602 = load i32, i32* %z, align 4
	  %601 = load float, float* %600, align 4
	  %600 = getelementptr inbounds float, float* %599, i64 %598
	  %599 = load float*, float** %4, align 8
	  %595 = load i32, i32* %z, align 4
	  %594 = getelementptr inbounds float, float* %593, i64 %592
	  %593 = load float*, float** %3, align 8
	  %589 = load i32, i32* %z, align 4
	  %587 = fmul double %579, %585
	  %588 = fptrunc double %587 to float
	  %590 = add nsw i32 872, %589
	  %591 = srem i32 %590, 128
	  %592 = sext i32 %591 to i64
	  store float %588, float* %594, align 4
	  %596 = add nsw i32 64, %595
	  %597 = srem i32 %596, 128
	  %598 = sext i32 %597 to i64
	  %603 = add nsw i32 192, %602
	  %604 = srem i32 %603, 128
	  %605 = sext i32 %604 to i64
	  %609 = fmul float %601, %608
	  %611 = add nsw i32 104, %610
	  %612 = srem i32 %611, 128
	  %613 = sext i32 %612 to i64
	  %618 = add nsw i32 144, %617
	  %619 = srem i32 %618, 128
	  %620 = sext i32 %619 to i64
	  %624 = fmul float %616, %623
	  %625 = fdiv float 1.000000e+00, %624
	  %626 = fmul float %609, %625
	  store float %626, float* %l, align 4
	  %628 = add nsw i32 880, %627
	  %629 = srem i32 %628, 128
	  %630 = sext i32 %629 to i64
	  %634 = fpext float %633 to double
	  %636 = fpext float %635 to double
	  %638 = fpext float %637 to double
	  %640 = call double @fmin(double %636, double %638) #6
	  %692 = load float, float* %h, align 4
	  %690 = load float, float* %l, align 4
	  %688 = load float, float* %687, align 4
	  %687 = getelementptr inbounds float, float* %686, i64 %685
	  %686 = load float*, float** %2, align 8
	  %682 = load i32, i32* %z, align 4
	  %678 = load float, float* %677, align 4
	  %677 = getelementptr inbounds float, float* %676, i64 %675
	  %676 = load float*, float** %4, align 8
	  %672 = load i32, i32* %z, align 4
	  %671 = load float, float* %670, align 4
	  %670 = getelementptr inbounds float, float* %669, i64 %668
	  %669 = load float*, float** %4, align 8
	  %665 = load i32, i32* %z, align 4
	  %663 = load float, float* %662, align 4
	  %662 = getelementptr inbounds float, float* %661, i64 %660
	  %661 = load float*, float** %4, align 8
	  %657 = load i32, i32* %z, align 4
	  %656 = load float, float* %655, align 4
	  %655 = getelementptr inbounds float, float* %654, i64 %653
	  %654 = load float*, float** %4, align 8
	  %650 = load i32, i32* %z, align 4
	  %649 = getelementptr inbounds float, float* %648, i64 %647
	  %648 = load float*, float** %3, align 8
	  %644 = load i32, i32* %z, align 4
	  %642 = fmul double %634, %640
	  %643 = fptrunc double %642 to float
	  %645 = add nsw i32 880, %644
	  %646 = srem i32 %645, 128
	  %647 = sext i32 %646 to i64
	  store float %643, float* %649, align 4
	  %651 = add nsw i32 72, %650
	  %652 = srem i32 %651, 128
	  %653 = sext i32 %652 to i64
	  %658 = add nsw i32 192, %657
	  %659 = srem i32 %658, 128
	  %660 = sext i32 %659 to i64
	  %664 = fmul float %656, %663
	  %666 = add nsw i32 104, %665
	  %667 = srem i32 %666, 128
	  %668 = sext i32 %667 to i64
	  %673 = add nsw i32 160, %672
	  %674 = srem i32 %673, 128
	  %675 = sext i32 %674 to i64
	  %679 = fmul float %671, %678
	  %680 = fdiv float 1.000000e+00, %679
	  %681 = fmul float %664, %680
	  store float %681, float* %l, align 4
	  %683 = add nsw i32 888, %682
	  %684 = srem i32 %683, 128
	  %685 = sext i32 %684 to i64
	  %689 = fpext float %688 to double
	  %691 = fpext float %690 to double
	  %693 = fpext float %692 to double
	  %695 = call double @fmin(double %691, double %693) #6
	  %757 = load float, float* %h, align 4
	  %755 = load float, float* %l, align 4
	  %753 = load float, float* %752, align 4
	  %752 = getelementptr inbounds float, float* %751, i64 %750
	  %751 = load float*, float** %2, align 8
	  %747 = load i32, i32* %z, align 4
	  %743 = load float, float* %k, align 4
	  %741 = load float, float* %740, align 4
	  %740 = getelementptr inbounds float, float* %739, i64 %738
	  %739 = load float*, float** %4, align 8
	  %735 = load i32, i32* %z, align 4
	  %733 = load float, float* %732, align 4
	  %732 = getelementptr inbounds float, float* %731, i64 %730
	  %731 = load float*, float** %4, align 8
	  %727 = load i32, i32* %z, align 4
	  %726 = load float, float* %725, align 4
	  %725 = getelementptr inbounds float, float* %724, i64 %723
	  %724 = load float*, float** %4, align 8
	  %720 = load i32, i32* %z, align 4
	  %718 = load float, float* %717, align 4
	  %717 = getelementptr inbounds float, float* %716, i64 %715
	  %716 = load float*, float** %4, align 8
	  %712 = load i32, i32* %z, align 4
	  %711 = load float, float* %710, align 4
	  %710 = getelementptr inbounds float, float* %709, i64 %708
	  %709 = load float*, float** %4, align 8
	  %705 = load i32, i32* %z, align 4
	  %704 = getelementptr inbounds float, float* %703, i64 %702
	  %703 = load float*, float** %3, align 8
	  %699 = load i32, i32* %z, align 4
	  %697 = fmul double %689, %695
	  %698 = fptrunc double %697 to float
	  %700 = add nsw i32 888, %699
	  %701 = srem i32 %700, 128
	  %702 = sext i32 %701 to i64
	  store float %698, float* %704, align 4
	  %706 = add nsw i32 192, %705
	  %707 = srem i32 %706, 128
	  %708 = sext i32 %707 to i64
	  %713 = add nsw i32 192, %712
	  %714 = srem i32 %713, 128
	  %715 = sext i32 %714 to i64
	  %719 = fmul float %711, %718
	  %721 = add nsw i32 104, %720
	  %722 = srem i32 %721, 128
	  %723 = sext i32 %722 to i64
	  %728 = add nsw i32 104, %727
	  %729 = srem i32 %728, 128
	  %730 = sext i32 %729 to i64
	  %734 = fmul float %726, %733
	  %736 = add nsw i32 144, %735
	  %737 = srem i32 %736, 128
	  %738 = sext i32 %737 to i64
	  %742 = fmul float %734, %741
	  %744 = fmul float %742, %743
	  %745 = fdiv float 1.000000e+00, %744
	  %746 = fmul float %719, %745
	  store float %746, float* %l, align 4
	  %748 = add nsw i32 896, %747
	  %749 = srem i32 %748, 128
	  %750 = sext i32 %749 to i64
	  %754 = fpext float %753 to double
	  %756 = fpext float %755 to double
	  %758 = fpext float %757 to double
	  %760 = call double @fmin(double %756, double %758) #6
	  %796 = load float, float* %h, align 4
	  %794 = load float, float* %l, align 4
	  %792 = load float, float* %791, align 4
	  %791 = getelementptr inbounds float, float* %790, i64 %789
	  %790 = load float*, float** %2, align 8
	  %786 = load i32, i32* %z, align 4
	  %783 = load float, float* %782, align 4
	  %782 = getelementptr inbounds float, float* %781, i64 %780
	  %781 = load float*, float** %4, align 8
	  %777 = load i32, i32* %z, align 4
	  %776 = load float, float* %775, align 4
	  %775 = getelementptr inbounds float, float* %774, i64 %773
	  %774 = load float*, float** %4, align 8
	  %770 = load i32, i32* %z, align 4
	  %769 = getelementptr inbounds float, float* %768, i64 %767
	  %768 = load float*, float** %3, align 8
	  %764 = load i32, i32* %z, align 4
	  %762 = fmul double %754, %760
	  %763 = fptrunc double %762 to float
	  %765 = add nsw i32 896, %764
	  %766 = srem i32 %765, 128
	  %767 = sext i32 %766 to i64
	  store float %763, float* %769, align 4
	  %771 = add nsw i32 144, %770
	  %772 = srem i32 %771, 128
	  %773 = sext i32 %772 to i64
	  %778 = add nsw i32 152, %777
	  %779 = srem i32 %778, 128
	  %780 = sext i32 %779 to i64
	  %784 = fdiv float 1.000000e+00, %783
	  %785 = fmul float %776, %784
	  store float %785, float* %l, align 4
	  %787 = add nsw i32 904, %786
	  %788 = srem i32 %787, 128
	  %789 = sext i32 %788 to i64
	  %793 = fpext float %792 to double
	  %795 = fpext float %794 to double
	  %797 = fpext float %796 to double
	  %799 = call double @fmin(double %795, double %797) #6
	  %845 = load float, float* %h, align 4
	  %843 = load float, float* %l, align 4
	  %841 = load float, float* %840, align 4
	  %840 = getelementptr inbounds float, float* %839, i64 %838
	  %839 = load float*, float** %2, align 8
	  %835 = load i32, i32* %z, align 4
	  %831 = load float, float* %k, align 4
	  %829 = load float, float* %828, align 4
	  %828 = getelementptr inbounds float, float* %827, i64 %826
	  %827 = load float*, float** %4, align 8
	  %823 = load i32, i32* %z, align 4
	  %822 = load float, float* %821, align 4
	  %821 = getelementptr inbounds float, float* %820, i64 %819
	  %820 = load float*, float** %4, align 8
	  %816 = load i32, i32* %z, align 4
	  %815 = load float, float* %814, align 4
	  %814 = getelementptr inbounds float, float* %813, i64 %812
	  %813 = load float*, float** %4, align 8
	  %809 = load i32, i32* %z, align 4
	  %808 = getelementptr inbounds float, float* %807, i64 %806
	  %807 = load float*, float** %3, align 8
	  %803 = load i32, i32* %z, align 4
	  %801 = fmul double %793, %799
	  %802 = fptrunc double %801 to float
	  %804 = add nsw i32 904, %803
	  %805 = srem i32 %804, 128
	  %806 = sext i32 %805 to i64
	  store float %802, float* %808, align 4
	  %810 = add nsw i32 160, %809
	  %811 = srem i32 %810, 128
	  %812 = sext i32 %811 to i64
	  %817 = add nsw i32 8, %816
	  %818 = srem i32 %817, 128
	  %819 = sext i32 %818 to i64
	  %824 = add nsw i32 144, %823
	  %825 = srem i32 %824, 128
	  %826 = sext i32 %825 to i64
	  %830 = fmul float %822, %829
	  %832 = fmul float %830, %831
	  %833 = fdiv float 1.000000e+00, %832
	  %834 = fmul float %815, %833
	  store float %834, float* %l, align 4
	  %836 = add nsw i32 912, %835
	  %837 = srem i32 %836, 128
	  %838 = sext i32 %837 to i64
	  %842 = fpext float %841 to double
	  %844 = fpext float %843 to double
	  %846 = fpext float %845 to double
	  %848 = call double @fmin(double %844, double %846) #6
	  %900 = load float, float* %h, align 4
	  %898 = load float, float* %l, align 4
	  %896 = load float, float* %895, align 4
	  %895 = getelementptr inbounds float, float* %894, i64 %893
	  %894 = load float*, float** %2, align 8
	  %890 = load i32, i32* %z, align 4
	  %886 = load float, float* %885, align 4
	  %885 = getelementptr inbounds float, float* %884, i64 %883
	  %884 = load float*, float** %4, align 8
	  %880 = load i32, i32* %z, align 4
	  %879 = load float, float* %878, align 4
	  %878 = getelementptr inbounds float, float* %877, i64 %876
	  %877 = load float*, float** %4, align 8
	  %873 = load i32, i32* %z, align 4
	  %871 = load float, float* %870, align 4
	  %870 = getelementptr inbounds float, float* %869, i64 %868
	  %869 = load float*, float** %4, align 8
	  %865 = load i32, i32* %z, align 4
	  %864 = load float, float* %863, align 4
	  %863 = getelementptr inbounds float, float* %862, i64 %861
	  %862 = load float*, float** %4, align 8
	  %858 = load i32, i32* %z, align 4
	  %857 = getelementptr inbounds float, float* %856, i64 %855
	  %856 = load float*, float** %3, align 8
	  %852 = load i32, i32* %z, align 4
	  %850 = fmul double %842, %848
	  %851 = fptrunc double %850 to float
	  %853 = add nsw i32 912, %852
	  %854 = srem i32 %853, 128
	  %855 = sext i32 %854 to i64
	  store float %851, float* %857, align 4
	  %859 = add nsw i32 16, %858
	  %860 = srem i32 %859, 128
	  %861 = sext i32 %860 to i64
	  %866 = add nsw i32 144, %865
	  %867 = srem i32 %866, 128
	  %868 = sext i32 %867 to i64
	  %872 = fmul float %864, %871
	  %874 = add nsw i32 8, %873
	  %875 = srem i32 %874, 128
	  %876 = sext i32 %875 to i64
	  %881 = add nsw i32 192, %880
	  %882 = srem i32 %881, 128
	  %883 = sext i32 %882 to i64
	  %887 = fmul float %879, %886
	  %888 = fdiv float 1.000000e+00, %887
	  %889 = fmul float %872, %888
	  store float %889, float* %l, align 4
	  %891 = add nsw i32 920, %890
	  %892 = srem i32 %891, 128
	  %893 = sext i32 %892 to i64
	  %897 = fpext float %896 to double
	  %899 = fpext float %898 to double
	  %901 = fpext float %900 to double
	  %903 = call double @fmin(double %899, double %901) #6
	  %955 = load float, float* %h, align 4
	  %953 = load float, float* %l, align 4
	  %951 = load float, float* %950, align 4
	  %950 = getelementptr inbounds float, float* %949, i64 %948
	  %949 = load float*, float** %2, align 8
	  %945 = load i32, i32* %z, align 4
	  %941 = load float, float* %940, align 4
	  %940 = getelementptr inbounds float, float* %939, i64 %938
	  %939 = load float*, float** %4, align 8
	  %935 = load i32, i32* %z, align 4
	  %934 = load float, float* %933, align 4
	  %933 = getelementptr inbounds float, float* %932, i64 %931
	  %932 = load float*, float** %4, align 8
	  %928 = load i32, i32* %z, align 4
	  %926 = load float, float* %925, align 4
	  %925 = getelementptr inbounds float, float* %924, i64 %923
	  %924 = load float*, float** %4, align 8
	  %920 = load i32, i32* %z, align 4
	  %919 = load float, float* %918, align 4
	  %918 = getelementptr inbounds float, float* %917, i64 %916
	  %917 = load float*, float** %4, align 8
	  %913 = load i32, i32* %z, align 4
	  %912 = getelementptr inbounds float, float* %911, i64 %910
	  %911 = load float*, float** %3, align 8
	  %907 = load i32, i32* %z, align 4
	  %905 = fmul double %897, %903
	  %906 = fptrunc double %905 to float
	  %908 = add nsw i32 920, %907
	  %909 = srem i32 %908, 128
	  %910 = sext i32 %909 to i64
	  store float %906, float* %912, align 4
	  %914 = add nsw i32 16, %913
	  %915 = srem i32 %914, 128
	  %916 = sext i32 %915 to i64
	  %921 = add nsw i32 144, %920
	  %922 = srem i32 %921, 128
	  %923 = sext i32 %922 to i64
	  %927 = fmul float %919, %926
	  %929 = add nsw i32 72, %928
	  %930 = srem i32 %929, 128
	  %931 = sext i32 %930 to i64
	  %936 = add nsw i32 104, %935
	  %937 = srem i32 %936, 128
	  %938 = sext i32 %937 to i64
	  %942 = fmul float %934, %941
	  %943 = fdiv float 1.000000e+00, %942
	  %944 = fmul float %927, %943
	  store float %944, float* %l, align 4
	  %946 = add nsw i32 928, %945
	  %947 = srem i32 %946, 128
	  %948 = sext i32 %947 to i64
	  %952 = fpext float %951 to double
	  %954 = fpext float %953 to double
	  %956 = fpext float %955 to double
	  %958 = call double @fmin(double %954, double %956) #6
	  %1010 = load float, float* %h, align 4
	  %1008 = load float, float* %l, align 4
	  %1006 = load float, float* %1005, align 4
	  %1005 = getelementptr inbounds float, float* %1004, i64 %1003
	  %1004 = load float*, float** %2, align 8
	  %1000 = load i32, i32* %z, align 4
	  %996 = load float, float* %995, align 4
	  %995 = getelementptr inbounds float, float* %994, i64 %993
	  %994 = load float*, float** %4, align 8
	  %990 = load i32, i32* %z, align 4
	  %989 = load float, float* %988, align 4
	  %988 = getelementptr inbounds float, float* %987, i64 %986
	  %987 = load float*, float** %4, align 8
	  %983 = load i32, i32* %z, align 4
	  %981 = load float, float* %980, align 4
	  %980 = getelementptr inbounds float, float* %979, i64 %978
	  %979 = load float*, float** %4, align 8
	  %975 = load i32, i32* %z, align 4
	  %974 = load float, float* %973, align 4
	  %973 = getelementptr inbounds float, float* %972, i64 %971
	  %972 = load float*, float** %4, align 8
	  %968 = load i32, i32* %z, align 4
	  %967 = getelementptr inbounds float, float* %966, i64 %965
	  %966 = load float*, float** %3, align 8
	  %962 = load i32, i32* %z, align 4
	  %960 = fmul double %952, %958
	  %961 = fptrunc double %960 to float
	  %963 = add nsw i32 928, %962
	  %964 = srem i32 %963, 128
	  %965 = sext i32 %964 to i64
	  store float %961, float* %967, align 4
	  %969 = add nsw i32 32, %968
	  %970 = srem i32 %969, 128
	  %971 = sext i32 %970 to i64
	  %976 = add nsw i32 144, %975
	  %977 = srem i32 %976, 128
	  %978 = sext i32 %977 to i64
	  %982 = fmul float %974, %981
	  %984 = add nsw i32 8, %983
	  %985 = srem i32 %984, 128
	  %986 = sext i32 %985 to i64
	  %991 = add nsw i32 200, %990
	  %992 = srem i32 %991, 128
	  %993 = sext i32 %992 to i64
	  %997 = fmul float %989, %996
	  %998 = fdiv float 1.000000e+00, %997
	  %999 = fmul float %982, %998
	  store float %999, float* %l, align 4
	  %1001 = add nsw i32 936, %1000
	  %1002 = srem i32 %1001, 128
	  %1003 = sext i32 %1002 to i64
	  %1007 = fpext float %1006 to double
	  %1009 = fpext float %1008 to double
	  %1011 = fpext float %1010 to double
	  %1013 = call double @fmin(double %1009, double %1011) #6
	  %1065 = load float, float* %h, align 4
	  %1063 = load float, float* %l, align 4
	  %1061 = load float, float* %1060, align 4
	  %1060 = getelementptr inbounds float, float* %1059, i64 %1058
	  %1059 = load float*, float** %2, align 8
	  %1055 = load i32, i32* %z, align 4
	  %1051 = load float, float* %1050, align 4
	  %1050 = getelementptr inbounds float, float* %1049, i64 %1048
	  %1049 = load float*, float** %4, align 8
	  %1045 = load i32, i32* %z, align 4
	  %1044 = load float, float* %1043, align 4
	  %1043 = getelementptr inbounds float, float* %1042, i64 %1041
	  %1042 = load float*, float** %4, align 8
	  %1038 = load i32, i32* %z, align 4
	  %1036 = load float, float* %1035, align 4
	  %1035 = getelementptr inbounds float, float* %1034, i64 %1033
	  %1034 = load float*, float** %4, align 8
	  %1030 = load i32, i32* %z, align 4
	  %1029 = load float, float* %1028, align 4
	  %1028 = getelementptr inbounds float, float* %1027, i64 %1026
	  %1027 = load float*, float** %4, align 8
	  %1023 = load i32, i32* %z, align 4
	  %1022 = getelementptr inbounds float, float* %1021, i64 %1020
	  %1021 = load float*, float** %3, align 8
	  %1017 = load i32, i32* %z, align 4
	  %1015 = fmul double %1007, %1013
	  %1016 = fptrunc double %1015 to float
	  %1018 = add nsw i32 936, %1017
	  %1019 = srem i32 %1018, 128
	  %1020 = sext i32 %1019 to i64
	  store float %1016, float* %1022, align 4
	  %1024 = add nsw i32 32, %1023
	  %1025 = srem i32 %1024, 128
	  %1026 = sext i32 %1025 to i64
	  %1031 = add nsw i32 144, %1030
	  %1032 = srem i32 %1031, 128
	  %1033 = sext i32 %1032 to i64
	  %1037 = fmul float %1029, %1036
	  %1039 = add nsw i32 88, %1038
	  %1040 = srem i32 %1039, 128
	  %1041 = sext i32 %1040 to i64
	  %1046 = add nsw i32 104, %1045
	  %1047 = srem i32 %1046, 128
	  %1048 = sext i32 %1047 to i64
	  %1052 = fmul float %1044, %1051
	  %1053 = fdiv float 1.000000e+00, %1052
	  %1054 = fmul float %1037, %1053
	  store float %1054, float* %l, align 4
	  %1056 = add nsw i32 944, %1055
	  %1057 = srem i32 %1056, 128
	  %1058 = sext i32 %1057 to i64
	  %1062 = fpext float %1061 to double
	  %1064 = fpext float %1063 to double
	  %1066 = fpext float %1065 to double
	  %1068 = call double @fmin(double %1064, double %1066) #6
	  %1120 = load float, float* %h, align 4
	  %1118 = load float, float* %l, align 4
	  %1116 = load float, float* %1115, align 4
	  %1115 = getelementptr inbounds float, float* %1114, i64 %1113
	  %1114 = load float*, float** %2, align 8
	  %1110 = load i32, i32* %z, align 4
	  %1106 = load float, float* %1105, align 4
	  %1105 = getelementptr inbounds float, float* %1104, i64 %1103
	  %1104 = load float*, float** %4, align 8
	  %1100 = load i32, i32* %z, align 4
	  %1099 = load float, float* %1098, align 4
	  %1098 = getelementptr inbounds float, float* %1097, i64 %1096
	  %1097 = load float*, float** %4, align 8
	  %1093 = load i32, i32* %z, align 4
	  %1091 = load float, float* %1090, align 4
	  %1090 = getelementptr inbounds float, float* %1089, i64 %1088
	  %1089 = load float*, float** %4, align 8
	  %1085 = load i32, i32* %z, align 4
	  %1084 = load float, float* %1083, align 4
	  %1083 = getelementptr inbounds float, float* %1082, i64 %1081
	  %1082 = load float*, float** %4, align 8
	  %1078 = load i32, i32* %z, align 4
	  %1077 = getelementptr inbounds float, float* %1076, i64 %1075
	  %1076 = load float*, float** %3, align 8
	  %1072 = load i32, i32* %z, align 4
	  %1070 = fmul double %1062, %1068
	  %1071 = fptrunc double %1070 to float
	  %1073 = add nsw i32 944, %1072
	  %1074 = srem i32 %1073, 128
	  %1075 = sext i32 %1074 to i64
	  store float %1071, float* %1077, align 4
	  %1079 = add nsw i32 120, %1078
	  %1080 = srem i32 %1079, 128
	  %1081 = sext i32 %1080 to i64
	  %1086 = add nsw i32 144, %1085
	  %1087 = srem i32 %1086, 128
	  %1088 = sext i32 %1087 to i64
	  %1092 = fmul float %1084, %1091
	  %1094 = add nsw i32 104, %1093
	  %1095 = srem i32 %1094, 128
	  %1096 = sext i32 %1095 to i64
	  %1101 = add nsw i32 160, %1100
	  %1102 = srem i32 %1101, 128
	  %1103 = sext i32 %1102 to i64
	  %1107 = fmul float %1099, %1106
	  %1108 = fdiv float 1.000000e+00, %1107
	  %1109 = fmul float %1092, %1108
	  store float %1109, float* %l, align 4
	  %1111 = add nsw i32 952, %1110
	  %1112 = srem i32 %1111, 128
	  %1113 = sext i32 %1112 to i64
	  %1117 = fpext float %1116 to double
	  %1119 = fpext float %1118 to double
	  %1121 = fpext float %1120 to double
	  %1123 = call double @fmin(double %1119, double %1121) #6
	  %1169 = load float, float* %h, align 4
	  %1167 = load float, float* %l, align 4
	  %1165 = load float, float* %1164, align 4
	  %1164 = getelementptr inbounds float, float* %1163, i64 %1162
	  %1163 = load float*, float** %2, align 8
	  %1159 = load i32, i32* %z, align 4
	  %1156 = load float, float* %1155, align 4
	  %1155 = getelementptr inbounds float, float* %1154, i64 %1153
	  %1154 = load float*, float** %4, align 8
	  %1150 = load i32, i32* %z, align 4
	  %1148 = load float, float* %k, align 4
	  %1146 = load float, float* %1145, align 4
	  %1145 = getelementptr inbounds float, float* %1144, i64 %1143
	  %1144 = load float*, float** %4, align 8
	  %1140 = load i32, i32* %z, align 4
	  %1139 = load float, float* %1138, align 4
	  %1138 = getelementptr inbounds float, float* %1137, i64 %1136
	  %1137 = load float*, float** %4, align 8
	  %1133 = load i32, i32* %z, align 4
	  %1132 = getelementptr inbounds float, float* %1131, i64 %1130
	  %1131 = load float*, float** %3, align 8
	  %1127 = load i32, i32* %z, align 4
	  %1125 = fmul double %1117, %1123
	  %1126 = fptrunc double %1125 to float
	  %1128 = add nsw i32 952, %1127
	  %1129 = srem i32 %1128, 128
	  %1130 = sext i32 %1129 to i64
	  store float %1126, float* %1132, align 4
	  %1134 = add nsw i32 88, %1133
	  %1135 = srem i32 %1134, 128
	  %1136 = sext i32 %1135 to i64
	  %1141 = add nsw i32 144, %1140
	  %1142 = srem i32 %1141, 128
	  %1143 = sext i32 %1142 to i64
	  %1147 = fmul float %1139, %1146
	  %1149 = fmul float %1147, %1148
	  %1151 = add nsw i32 224, %1150
	  %1152 = srem i32 %1151, 128
	  %1153 = sext i32 %1152 to i64
	  %1157 = fdiv float 1.000000e+00, %1156
	  %1158 = fmul float %1149, %1157
	  store float %1158, float* %l, align 4
	  %1160 = add nsw i32 960, %1159
	  %1161 = srem i32 %1160, 128
	  %1162 = sext i32 %1161 to i64
	  %1166 = fpext float %1165 to double
	  %1168 = fpext float %1167 to double
	  %1170 = fpext float %1169 to double
	  %1172 = call double @fmin(double %1168, double %1170) #6
	  %1208 = load float, float* %h, align 4
	  %1206 = load float, float* %l, align 4
	  %1204 = load float, float* %1203, align 4
	  %1203 = getelementptr inbounds float, float* %1202, i64 %1201
	  %1202 = load float*, float** %2, align 8
	  %1198 = load i32, i32* %z, align 4
	  %1195 = load float, float* %1194, align 4
	  %1194 = getelementptr inbounds float, float* %1193, i64 %1192
	  %1193 = load float*, float** %4, align 8
	  %1189 = load i32, i32* %z, align 4
	  %1188 = load float, float* %1187, align 4
	  %1187 = getelementptr inbounds float, float* %1186, i64 %1185
	  %1186 = load float*, float** %4, align 8
	  %1182 = load i32, i32* %z, align 4
	  %1181 = getelementptr inbounds float, float* %1180, i64 %1179
	  %1180 = load float*, float** %3, align 8
	  %1176 = load i32, i32* %z, align 4
	  %1174 = fmul double %1166, %1172
	  %1175 = fptrunc double %1174 to float
	  %1177 = add nsw i32 960, %1176
	  %1178 = srem i32 %1177, 128
	  %1179 = sext i32 %1178 to i64
	  store float %1175, float* %1181, align 4
	  %1183 = add nsw i32 144, %1182
	  %1184 = srem i32 %1183, 128
	  %1185 = sext i32 %1184 to i64
	  %1190 = add nsw i32 152, %1189
	  %1191 = srem i32 %1190, 128
	  %1192 = sext i32 %1191 to i64
	  %1196 = fdiv float 1.000000e+00, %1195
	  %1197 = fmul float %1188, %1196
	  store float %1197, float* %l, align 4
	  %1199 = add nsw i32 968, %1198
	  %1200 = srem i32 %1199, 128
	  %1201 = sext i32 %1200 to i64
	  %1205 = fpext float %1204 to double
	  %1207 = fpext float %1206 to double
	  %1209 = fpext float %1208 to double
	  %1211 = call double @fmin(double %1207, double %1209) #6
	  %1263 = load float, float* %h, align 4
	  %1261 = load float, float* %l, align 4
	  %1259 = load float, float* %1258, align 4
	  %1258 = getelementptr inbounds float, float* %1257, i64 %1256
	  %1257 = load float*, float** %2, align 8
	  %1253 = load i32, i32* %z, align 4
	  %1249 = load float, float* %1248, align 4
	  %1248 = getelementptr inbounds float, float* %1247, i64 %1246
	  %1247 = load float*, float** %4, align 8
	  %1243 = load i32, i32* %z, align 4
	  %1242 = load float, float* %1241, align 4
	  %1241 = getelementptr inbounds float, float* %1240, i64 %1239
	  %1240 = load float*, float** %4, align 8
	  %1236 = load i32, i32* %z, align 4
	  %1234 = load float, float* %1233, align 4
	  %1233 = getelementptr inbounds float, float* %1232, i64 %1231
	  %1232 = load float*, float** %4, align 8
	  %1228 = load i32, i32* %z, align 4
	  %1227 = load float, float* %1226, align 4
	  %1226 = getelementptr inbounds float, float* %1225, i64 %1224
	  %1225 = load float*, float** %4, align 8
	  %1221 = load i32, i32* %z, align 4
	  %1220 = getelementptr inbounds float, float* %1219, i64 %1218
	  %1219 = load float*, float** %3, align 8
	  %1215 = load i32, i32* %z, align 4
	  %1213 = fmul double %1205, %1211
	  %1214 = fptrunc double %1213 to float
	  %1216 = add nsw i32 968, %1215
	  %1217 = srem i32 %1216, 128
	  %1218 = sext i32 %1217 to i64
	  store float %1214, float* %1220, align 4
	  %1222 = add nsw i32 16, %1221
	  %1223 = srem i32 %1222, 128
	  %1224 = sext i32 %1223 to i64
	  %1229 = add nsw i32 152, %1228
	  %1230 = srem i32 %1229, 128
	  %1231 = sext i32 %1230 to i64
	  %1235 = fmul float %1227, %1234
	  %1237 = add nsw i32 72, %1236
	  %1238 = srem i32 %1237, 128
	  %1239 = sext i32 %1238 to i64
	  %1244 = add nsw i32 104, %1243
	  %1245 = srem i32 %1244, 128
	  %1246 = sext i32 %1245 to i64
	  %1250 = fmul float %1242, %1249
	  %1251 = fdiv float 1.000000e+00, %1250
	  %1252 = fmul float %1235, %1251
	  store float %1252, float* %l, align 4
	  %1254 = add nsw i32 976, %1253
	  %1255 = srem i32 %1254, 128
	  %1256 = sext i32 %1255 to i64
	  %1260 = fpext float %1259 to double
	  %1262 = fpext float %1261 to double
	  %1264 = fpext float %1263 to double
	  %1266 = call double @fmin(double %1262, double %1264) #6
	  %1318 = load float, float* %h, align 4
	  %1316 = load float, float* %l, align 4
	  %1314 = load float, float* %1313, align 4
	  %1313 = getelementptr inbounds float, float* %1312, i64 %1311
	  %1312 = load float*, float** %2, align 8
	  %1308 = load i32, i32* %z, align 4
	  %1304 = load float, float* %1303, align 4
	  %1303 = getelementptr inbounds float, float* %1302, i64 %1301
	  %1302 = load float*, float** %4, align 8
	  %1298 = load i32, i32* %z, align 4
	  %1297 = load float, float* %1296, align 4
	  %1296 = getelementptr inbounds float, float* %1295, i64 %1294
	  %1295 = load float*, float** %4, align 8
	  %1291 = load i32, i32* %z, align 4
	  %1289 = load float, float* %1288, align 4
	  %1288 = getelementptr inbounds float, float* %1287, i64 %1286
	  %1287 = load float*, float** %4, align 8
	  %1283 = load i32, i32* %z, align 4
	  %1282 = load float, float* %1281, align 4
	  %1281 = getelementptr inbounds float, float* %1280, i64 %1279
	  %1280 = load float*, float** %4, align 8
	  %1276 = load i32, i32* %z, align 4
	  %1275 = getelementptr inbounds float, float* %1274, i64 %1273
	  %1274 = load float*, float** %3, align 8
	  %1270 = load i32, i32* %z, align 4
	  %1268 = fmul double %1260, %1266
	  %1269 = fptrunc double %1268 to float
	  %1271 = add nsw i32 976, %1270
	  %1272 = srem i32 %1271, 128
	  %1273 = sext i32 %1272 to i64
	  store float %1269, float* %1275, align 4
	  %1277 = add nsw i32 32, %1276
	  %1278 = srem i32 %1277, 128
	  %1279 = sext i32 %1278 to i64
	  %1284 = add nsw i32 152, %1283
	  %1285 = srem i32 %1284, 128
	  %1286 = sext i32 %1285 to i64
	  %1290 = fmul float %1282, %1289
	  %1292 = add nsw i32 8, %1291
	  %1293 = srem i32 %1292, 128
	  %1294 = sext i32 %1293 to i64
	  %1299 = add nsw i32 200, %1298
	  %1300 = srem i32 %1299, 128
	  %1301 = sext i32 %1300 to i64
	  %1305 = fmul float %1297, %1304
	  %1306 = fdiv float 1.000000e+00, %1305
	  %1307 = fmul float %1290, %1306
	  store float %1307, float* %l, align 4
	  %1309 = add nsw i32 984, %1308
	  %1310 = srem i32 %1309, 128
	  %1311 = sext i32 %1310 to i64
	  %1315 = fpext float %1314 to double
	  %1317 = fpext float %1316 to double
	  %1319 = fpext float %1318 to double
	  %1321 = call double @fmin(double %1317, double %1319) #6
	  %1373 = load float, float* %h, align 4
	  %1371 = load float, float* %l, align 4
	  %1369 = load float, float* %1368, align 4
	  %1368 = getelementptr inbounds float, float* %1367, i64 %1366
	  %1367 = load float*, float** %2, align 8
	  %1363 = load i32, i32* %z, align 4
	  %1359 = load float, float* %1358, align 4
	  %1358 = getelementptr inbounds float, float* %1357, i64 %1356
	  %1357 = load float*, float** %4, align 8
	  %1353 = load i32, i32* %z, align 4
	  %1352 = load float, float* %1351, align 4
	  %1351 = getelementptr inbounds float, float* %1350, i64 %1349
	  %1350 = load float*, float** %4, align 8
	  %1346 = load i32, i32* %z, align 4
	  %1344 = load float, float* %1343, align 4
	  %1343 = getelementptr inbounds float, float* %1342, i64 %1341
	  %1342 = load float*, float** %4, align 8
	  %1338 = load i32, i32* %z, align 4
	  %1337 = load float, float* %1336, align 4
	  %1336 = getelementptr inbounds float, float* %1335, i64 %1334
	  %1335 = load float*, float** %4, align 8
	  %1331 = load i32, i32* %z, align 4
	  %1330 = getelementptr inbounds float, float* %1329, i64 %1328
	  %1329 = load float*, float** %3, align 8
	  %1325 = load i32, i32* %z, align 4
	  %1323 = fmul double %1315, %1321
	  %1324 = fptrunc double %1323 to float
	  %1326 = add nsw i32 984, %1325
	  %1327 = srem i32 %1326, 128
	  %1328 = sext i32 %1327 to i64
	  store float %1324, float* %1330, align 4
	  %1332 = add nsw i32 24, %1331
	  %1333 = srem i32 %1332, 128
	  %1334 = sext i32 %1333 to i64
	  %1339 = add nsw i32 152, %1338
	  %1340 = srem i32 %1339, 128
	  %1341 = sext i32 %1340 to i64
	  %1345 = fmul float %1337, %1344
	  %1347 = add nsw i32 72, %1346
	  %1348 = srem i32 %1347, 128
	  %1349 = sext i32 %1348 to i64
	  %1354 = add nsw i32 112, %1353
	  %1355 = srem i32 %1354, 128
	  %1356 = sext i32 %1355 to i64
	  %1360 = fmul float %1352, %1359
	  %1361 = fdiv float 1.000000e+00, %1360
	  %1362 = fmul float %1345, %1361
	  store float %1362, float* %l, align 4
	  %1364 = add nsw i32 992, %1363
	  %1365 = srem i32 %1364, 128
	  %1366 = sext i32 %1365 to i64
	  %1370 = fpext float %1369 to double
	  %1372 = fpext float %1371 to double
	  %1374 = fpext float %1373 to double
	  %1376 = call double @fmin(double %1372, double %1374) #6
	  %1385 = getelementptr inbounds float, float* %1384, i64 %1383
	  %1384 = load float*, float** %3, align 8
	  %1380 = load i32, i32* %z, align 4
	  %1378 = fmul double %1370, %1376
	  %1379 = fptrunc double %1378 to float
	  %1381 = add nsw i32 992, %1380
	  %1382 = srem i32 %1381, 128
	  %1383 = sext i32 %1382 to i64
	  store float %1379, float* %1385, align 4
