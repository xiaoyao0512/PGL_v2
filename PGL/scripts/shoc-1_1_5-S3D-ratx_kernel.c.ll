	  %a = alloca [16384 x float], align 16
	  %b = alloca [16384 x float], align 16
	  %c = alloca [16384 x float], align 16
	  %d = alloca [16384 x float], align 16
	  %e = alloca [16384 x float], align 16
	  %f = alloca float, align 4
	  %1 = bitcast [16384 x float]* %a to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* bitcast ([16384 x float]* @main.a to i8*), i64 65536, i32 16, i1 false)
	  %4 = bitcast [16384 x float]* %b to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %4, i8* bitcast ([16384 x float]* @main.b to i8*), i64 65536, i32 16, i1 false)
	  %7 = bitcast [16384 x float]* %c to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %7, i8* bitcast ([16384 x float]* @main.c to i8*), i64 65536, i32 16, i1 false)
	  %10 = bitcast [16384 x float]* %d to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %10, i8* bitcast ([16384 x float]* @main.d to i8*), i64 65536, i32 16, i1 false)
	  %13 = bitcast [16384 x float]* %e to i8*
	  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %13, i8* bitcast ([16384 x float]* @main.e to i8*), i64 65536, i32 16, i1 false)
	  %21 = load float, float* %f, align 4
	  %20 = getelementptr inbounds [16384 x float], [16384 x float]* %e, i32 0, i32 0
	  %19 = getelementptr inbounds [16384 x float], [16384 x float]* %d, i32 0, i32 0
	  %18 = getelementptr inbounds [16384 x float], [16384 x float]* %c, i32 0, i32 0
	  %17 = getelementptr inbounds [16384 x float], [16384 x float]* %b, i32 0, i32 0
	  %16 = getelementptr inbounds [16384 x float], [16384 x float]* %a, i32 0, i32 0
	store float* %16, float** %a, align 8
	store  float* %17, float** %b, align 8
	store  float* %18, float** %c, align 8
	store  float* %19, float** %d, align 8
	store  float* %20, float** %e, align 8
	store  float %21, float* %f, align 8
	  store float 1.000000e+00, float* %f, align 4
	  call void @A(float* %16, float* %17, float* %18, float* %19, float* %20, float %21)
	  %14 = load float, float* %g, align 4
	  %12 = load float, float* %6, align 4
	  %11 = load float, float* %10, align 4
	  %10 = getelementptr inbounds float, float* %9, i64 %8
	  %9 = load float*, float** %1, align 8
	  %7 = load i32, i32* %z, align 4
	  %1 = alloca float*, align 8
	  %2 = alloca float*, align 8
	  %3 = alloca float*, align 8
	  %4 = alloca float*, align 8
	  %5 = alloca float*, align 8
	  %6 = alloca float, align 4
	  %z = alloca i32, align 4
	  %g = alloca float, align 4
	  %h = alloca float, align 4
	  %i = alloca float, align 4
	  %j = alloca float, align 4
	  %k = alloca float, align 4
	  %l = alloca float, align 4
	  %m = alloca float, align 4
	  %n = alloca float, align 4
	  %o = alloca float, align 4
	  %p = alloca float, align 4
	  %q = alloca float, align 4
	  %r = alloca float, align 4
	  %s = alloca float, align 4
	  %t = alloca float, align 4
	  %u = alloca i32, align 4
	  %v = alloca float, align 4
	  %w = alloca float, align 4
	  %x = alloca float, align 4
	  store float* %a, float** %1, align 8
	  store float* %b, float** %2, align 8
	  store float* %c, float** %3, align 8
	  store float* %d, float** %4, align 8
	  store float* %e, float** %5, align 8
	  store float %f, float* %6, align 4
	  store i32 0, i32* %z, align 4
	  %8 = sext i32 %7 to i64
	  %13 = fmul float %11, %12
	  store float %13, float* %g, align 4
	  %15 = fpext float %14 to double
	  %17 = call double @log(double %15) #4
	  %19 = fptrunc double %17 to float
	  store float %19, float* %h, align 4
	  store float 0.000000e+00, float* %i, align 4
	  store float 0x3810000000000000, float* %t, align 4
	  store i32 1, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %36 = load float, float* %i, align 4
	  %35 = load float, float* %34, align 4
	  %34 = getelementptr inbounds float, float* %33, i64 %32
	  %33 = load float*, float** %2, align 8
	  %29 = load i32, i32* %z, align 4
	  %26 = load i32, i32* %u, align 4
	  %27 = sub nsw i32 %26, 1
	  %28 = mul nsw i32 %27, 8
	  %30 = add nsw i32 %28, %29
	  %31 = srem i32 %30, 128
	  %32 = sext i32 %31 to i64
	  %37 = fadd float %36, %35
	  store float %37, float* %i, align 4
	  %40 = load i32, i32* %u, align 4
	  %41 = add nsw i32 %40, 1
	  store i32 %41, i32* %u, align 4
	  %22 = load i32, i32* %u, align 4
	  %23 = icmp sle i32 %22, 22
	  %309 = load float, float* %t, align 4
	  %307 = load float, float* %j, align 4
	  %301 = load float, float* %j, align 4
	  %299 = load float, float* %j, align 4
	  %295 = load float, float* %294, align 4
	  %294 = getelementptr inbounds float, float* %293, i64 %292
	  %293 = load float*, float** %3, align 8
	  %289 = load i32, i32* %z, align 4
	  %288 = load float, float* %x, align 4
	  %287 = load float, float* %286, align 4
	  %286 = getelementptr inbounds float, float* %285, i64 %284
	  %285 = load float*, float** %5, align 8
	  %281 = load i32, i32* %z, align 4
	  %276 = load float, float* %275, align 4
	  %275 = getelementptr inbounds float, float* %274, i64 %273
	  %274 = load float*, float** %2, align 8
	  %270 = load i32, i32* %z, align 4
	  %266 = load float, float* %265, align 4
	  %265 = getelementptr inbounds float, float* %264, i64 %263
	  %264 = load float*, float** %2, align 8
	  %260 = load i32, i32* %z, align 4
	  %256 = load float, float* %255, align 4
	  %255 = getelementptr inbounds float, float* %254, i64 %253
	  %254 = load float*, float** %2, align 8
	  %250 = load i32, i32* %z, align 4
	  %247 = load float, float* %246, align 4
	  %246 = getelementptr inbounds float, float* %245, i64 %244
	  %245 = load float*, float** %2, align 8
	  %241 = load i32, i32* %z, align 4
	  %237 = load float, float* %236, align 4
	  %236 = getelementptr inbounds float, float* %235, i64 %234
	  %235 = load float*, float** %2, align 8
	  %231 = load i32, i32* %z, align 4
	  %228 = load float, float* %227, align 4
	  %227 = getelementptr inbounds float, float* %226, i64 %225
	  %226 = load float*, float** %2, align 8
	  %222 = load i32, i32* %z, align 4
	  %218 = load float, float* %217, align 4
	  %217 = getelementptr inbounds float, float* %216, i64 %215
	  %216 = load float*, float** %2, align 8
	  %212 = load i32, i32* %z, align 4
	  %209 = load float, float* %208, align 4
	  %208 = getelementptr inbounds float, float* %207, i64 %206
	  %207 = load float*, float** %2, align 8
	  %203 = load i32, i32* %z, align 4
	  %202 = load float, float* %i, align 4
	  %197 = load float, float* %196, align 4
	  %196 = getelementptr inbounds float, float* %195, i64 %194
	  %195 = load float*, float** %2, align 8
	  %191 = load i32, i32* %z, align 4
	  %187 = load float, float* %186, align 4
	  %186 = getelementptr inbounds float, float* %185, i64 %184
	  %185 = load float*, float** %2, align 8
	  %181 = load i32, i32* %z, align 4
	  %177 = load float, float* %176, align 4
	  %176 = getelementptr inbounds float, float* %175, i64 %174
	  %175 = load float*, float** %2, align 8
	  %171 = load i32, i32* %z, align 4
	  %168 = load float, float* %167, align 4
	  %167 = getelementptr inbounds float, float* %166, i64 %165
	  %166 = load float*, float** %2, align 8
	  %162 = load i32, i32* %z, align 4
	  %158 = load float, float* %157, align 4
	  %157 = getelementptr inbounds float, float* %156, i64 %155
	  %156 = load float*, float** %2, align 8
	  %152 = load i32, i32* %z, align 4
	  %149 = load float, float* %148, align 4
	  %148 = getelementptr inbounds float, float* %147, i64 %146
	  %147 = load float*, float** %2, align 8
	  %143 = load i32, i32* %z, align 4
	  %139 = load float, float* %138, align 4
	  %138 = getelementptr inbounds float, float* %137, i64 %136
	  %137 = load float*, float** %2, align 8
	  %133 = load i32, i32* %z, align 4
	  %130 = load float, float* %129, align 4
	  %129 = getelementptr inbounds float, float* %128, i64 %127
	  %128 = load float*, float** %2, align 8
	  %124 = load i32, i32* %z, align 4
	  %123 = load float, float* %i, align 4
	  %118 = load float, float* %117, align 4
	  %117 = getelementptr inbounds float, float* %116, i64 %115
	  %116 = load float*, float** %2, align 8
	  %112 = load i32, i32* %z, align 4
	  %108 = load float, float* %107, align 4
	  %107 = getelementptr inbounds float, float* %106, i64 %105
	  %106 = load float*, float** %2, align 8
	  %102 = load i32, i32* %z, align 4
	  %98 = load float, float* %97, align 4
	  %97 = getelementptr inbounds float, float* %96, i64 %95
	  %96 = load float*, float** %2, align 8
	  %92 = load i32, i32* %z, align 4
	  %89 = load float, float* %88, align 4
	  %88 = getelementptr inbounds float, float* %87, i64 %86
	  %87 = load float*, float** %2, align 8
	  %83 = load i32, i32* %z, align 4
	  %79 = load float, float* %78, align 4
	  %78 = getelementptr inbounds float, float* %77, i64 %76
	  %77 = load float*, float** %2, align 8
	  %73 = load i32, i32* %z, align 4
	  %70 = load float, float* %69, align 4
	  %69 = getelementptr inbounds float, float* %68, i64 %67
	  %68 = load float*, float** %2, align 8
	  %64 = load i32, i32* %z, align 4
	  %60 = load float, float* %59, align 4
	  %59 = getelementptr inbounds float, float* %58, i64 %57
	  %58 = load float*, float** %2, align 8
	  %54 = load i32, i32* %z, align 4
	  %51 = load float, float* %50, align 4
	  %50 = getelementptr inbounds float, float* %49, i64 %48
	  %49 = load float*, float** %2, align 8
	  %45 = load i32, i32* %z, align 4
	  %44 = load float, float* %i, align 4
	  %46 = add nsw i32 0, %45
	  %47 = srem i32 %46, 128
	  %48 = sext i32 %47 to i64
	  %52 = fadd float %44, %51
	  %53 = fpext float %52 to double
	  %55 = add nsw i32 40, %54
	  %56 = srem i32 %55, 128
	  %57 = sext i32 %56 to i64
	  %61 = fpext float %60 to double
	  %62 = fmul double 5.000000e+00, %61
	  %63 = fadd double %53, %62
	  %65 = add nsw i32 72, %64
	  %66 = srem i32 %65, 128
	  %67 = sext i32 %66 to i64
	  %71 = fpext float %70 to double
	  %72 = fadd double %63, %71
	  %74 = add nsw i32 80, %73
	  %75 = srem i32 %74, 128
	  %76 = sext i32 %75 to i64
	  %80 = fpext float %79 to double
	  %81 = fmul double 5.000000e-01, %80
	  %82 = fadd double %72, %81
	  %84 = add nsw i32 88, %83
	  %85 = srem i32 %84, 128
	  %86 = sext i32 %85 to i64
	  %90 = fpext float %89 to double
	  %91 = fadd double %82, %90
	  %93 = add nsw i32 120, %92
	  %94 = srem i32 %93, 128
	  %95 = sext i32 %94 to i64
	  %99 = fpext float %98 to double
	  %100 = fmul double 2.000000e+00, %99
	  %101 = fadd double %91, %100
	  %103 = add nsw i32 104, %102
	  %104 = srem i32 %103, 128
	  %105 = sext i32 %104 to i64
	  %109 = fpext float %108 to double
	  %110 = fmul double 2.000000e+00, %109
	  %111 = fadd double %101, %110
	  %113 = add nsw i32 112, %112
	  %114 = srem i32 %113, 128
	  %115 = sext i32 %114 to i64
	  %119 = fpext float %118 to double
	  %120 = fmul double 2.000000e+00, %119
	  %121 = fadd double %111, %120
	  %122 = fptrunc double %121 to float
	  store float %122, float* %v, align 4
	  %125 = add nsw i32 0, %124
	  %126 = srem i32 %125, 128
	  %127 = sext i32 %126 to i64
	  %131 = fadd float %123, %130
	  %132 = fpext float %131 to double
	  %134 = add nsw i32 40, %133
	  %135 = srem i32 %134, 128
	  %136 = sext i32 %135 to i64
	  %140 = fpext float %139 to double
	  %141 = fmul double 5.000000e+00, %140
	  %142 = fadd double %132, %141
	  %144 = add nsw i32 72, %143
	  %145 = srem i32 %144, 128
	  %146 = sext i32 %145 to i64
	  %150 = fpext float %149 to double
	  %151 = fadd double %142, %150
	  %153 = add nsw i32 80, %152
	  %154 = srem i32 %153, 128
	  %155 = sext i32 %154 to i64
	  %159 = fpext float %158 to double
	  %160 = fmul double 5.000000e-01, %159
	  %161 = fadd double %151, %160
	  %163 = add nsw i32 88, %162
	  %164 = srem i32 %163, 128
	  %165 = sext i32 %164 to i64
	  %169 = fpext float %168 to double
	  %170 = fadd double %161, %169
	  %172 = add nsw i32 120, %171
	  %173 = srem i32 %172, 128
	  %174 = sext i32 %173 to i64
	  %178 = fpext float %177 to double
	  %179 = fmul double 2.000000e+00, %178
	  %180 = fadd double %170, %179
	  %182 = add nsw i32 104, %181
	  %183 = srem i32 %182, 128
	  %184 = sext i32 %183 to i64
	  %188 = fmul float 1.500000e+00, %187
	  %189 = fpext float %188 to double
	  %190 = fadd double %180, %189
	  %192 = add nsw i32 112, %191
	  %193 = srem i32 %192, 128
	  %194 = sext i32 %193 to i64
	  %198 = fmul float 1.500000e+00, %197
	  %199 = fpext float %198 to double
	  %200 = fadd double %190, %199
	  %201 = fptrunc double %200 to float
	  store float %201, float* %w, align 4
	  %204 = add nsw i32 0, %203
	  %205 = srem i32 %204, 128
	  %206 = sext i32 %205 to i64
	  %210 = fadd float %202, %209
	  %211 = fpext float %210 to double
	  %213 = add nsw i32 40, %212
	  %214 = srem i32 %213, 128
	  %215 = sext i32 %214 to i64
	  %219 = fpext float %218 to double
	  %220 = fmul double 5.000000e+00, %219
	  %221 = fadd double %211, %220
	  %223 = add nsw i32 72, %222
	  %224 = srem i32 %223, 128
	  %225 = sext i32 %224 to i64
	  %229 = fpext float %228 to double
	  %230 = fadd double %221, %229
	  %232 = add nsw i32 80, %231
	  %233 = srem i32 %232, 128
	  %234 = sext i32 %233 to i64
	  %238 = fpext float %237 to double
	  %239 = fmul double 5.000000e-01, %238
	  %240 = fadd double %230, %239
	  %242 = add nsw i32 88, %241
	  %243 = srem i32 %242, 128
	  %244 = sext i32 %243 to i64
	  %248 = fpext float %247 to double
	  %249 = fadd double %240, %248
	  %251 = add nsw i32 120, %250
	  %252 = srem i32 %251, 128
	  %253 = sext i32 %252 to i64
	  %257 = fpext float %256 to double
	  %258 = fmul double 2.000000e+00, %257
	  %259 = fadd double %249, %258
	  %261 = add nsw i32 104, %260
	  %262 = srem i32 %261, 128
	  %263 = sext i32 %262 to i64
	  %267 = fpext float %266 to double
	  %268 = fmul double 2.000000e+00, %267
	  %269 = fadd double %259, %268
	  %271 = add nsw i32 112, %270
	  %272 = srem i32 %271, 128
	  %273 = sext i32 %272 to i64
	  %277 = fpext float %276 to double
	  %278 = fmul double 2.000000e+00, %277
	  %279 = fadd double %269, %278
	  %280 = fptrunc double %279 to float
	  store float %280, float* %x, align 4
	  %282 = add nsw i32 0, %281
	  %283 = srem i32 %282, 128
	  %284 = sext i32 %283 to i64
	  %290 = add nsw i32 120, %289
	  %291 = srem i32 %290, 128
	  %292 = sext i32 %291 to i64
	  %296 = fdiv float 1.000000e+00, %295
	  %297 = fmul float %288, %296
	  %298 = fmul float %287, %297
	  store float %298, float* %j, align 4
	  %300 = fpext float %299 to double
	  %302 = fpext float %301 to double
	  %303 = fadd double 1.000000e+00, %302
	  %304 = fdiv double 1.000000e+00, %303
	  %305 = fmul double %300, %304
	  %306 = fptrunc double %305 to float
	  store float %306, float* %k, align 4
	  %308 = fpext float %307 to double
	  %310 = fpext float %309 to double
	  %312 = call double @fmax(double %308, double %310) #6
	  %315 = call double @log10(double %312) #5
	  %318 = load float, float* %g, align 4
	  %317 = fptrunc double %315 to float
	  store float %317, float* %l, align 4
	  %319 = fsub float -0.000000e+00, %318
	  %320 = fmul float %319, 0x3F85C98820000000
	  %321 = fpext float %320 to double
	  %323 = call double @exp(double %321) #5
	  %326 = load float, float* %g, align 4
	  %325 = fmul double 0x3FD0FC5040000000, %323
	  %327 = fsub float -0.000000e+00, %326
	  %328 = fmul float %327, 0x3F42A91CA0000000
	  %329 = fpext float %328 to double
	  %331 = call double @exp(double %329) #5
	  %335 = load float, float* %g, align 4
	  %333 = fmul double 0x3FE781D7E0000000, %331
	  %334 = fadd double %325, %333
	  %336 = fdiv float 1.000000e+00, %335
	  %337 = fmul float -5.182000e+03, %336
	  %338 = fpext float %337 to double
	  %340 = call double @exp(double %338) #5
	  %346 = load float, float* %t, align 4
	  %344 = load float, float* %m, align 4
	  %342 = fadd double %334, %340
	  %343 = fptrunc double %342 to float
	  store float %343, float* %m, align 4
	  %345 = fpext float %344 to double
	  %347 = fpext float %346 to double
	  %349 = call double @fmax(double %345, double %347) #6
	  %352 = call double @log10(double %349) #5
	  %389 = load float, float* %q, align 4
	  %382 = load float, float* %s, align 4
	  %381 = load float, float* %s, align 4
	  %379 = load float, float* %n, align 4
	  %372 = load float, float* %p, align 4
	  %370 = load float, float* %o, align 4
	  %368 = load float, float* %p, align 4
	  %362 = load float, float* %n, align 4
	  %360 = load float, float* %l, align 4
	  %355 = load float, float* %n, align 4
	  %354 = fptrunc double %352 to float
	  store float %354, float* %n, align 4
	  %356 = fpext float %355 to double
	  %357 = fmul double 1.270000e+00, %356
	  %358 = fsub double 7.500000e-01, %357
	  %359 = fptrunc double %358 to float
	  store float %359, float* %o, align 4
	  %361 = fpext float %360 to double
	  %363 = fpext float %362 to double
	  %364 = fmul double 6.700000e-01, %363
	  %365 = fadd double 4.000000e-01, %364
	  %366 = fsub double %361, %365
	  %367 = fptrunc double %366 to float
	  store float %367, float* %p, align 4
	  %369 = fpext float %368 to double
	  %371 = fpext float %370 to double
	  %373 = fpext float %372 to double
	  %374 = fmul double 1.400000e-01, %373
	  %375 = fsub double %371, %374
	  %376 = fdiv double 1.000000e+00, %375
	  %377 = fmul double %369, %376
	  %378 = fptrunc double %377 to float
	  store float %378, float* %s, align 4
	  %380 = fpext float %379 to double
	  %383 = fmul float %381, %382
	  %384 = fpext float %383 to double
	  %385 = fadd double 1.000000e+00, %384
	  %386 = fdiv double 1.000000e+00, %385
	  %387 = fmul double %380, %386
	  %388 = fptrunc double %387 to float
	  store float %388, float* %q, align 4
	  %390 = fpext float %389 to double
	  %392 = call double @exp(double %390) #5
	  %456 = load float, float* %t, align 4
	  %454 = load float, float* %j, align 4
	  %448 = load float, float* %j, align 4
	  %446 = load float, float* %j, align 4
	  %442 = load float, float* %441, align 4
	  %441 = getelementptr inbounds float, float* %440, i64 %439
	  %440 = load float*, float** %3, align 8
	  %436 = load i32, i32* %z, align 4
	  %435 = load float, float* %v, align 4
	  %434 = load float, float* %433, align 4
	  %433 = getelementptr inbounds float, float* %432, i64 %431
	  %432 = load float*, float** %5, align 8
	  %428 = load i32, i32* %z, align 4
	  %427 = getelementptr inbounds float, float* %426, i64 %425
	  %426 = load float*, float** %4, align 8
	  %422 = load i32, i32* %z, align 4
	  %420 = load float, float* %k, align 4
	  %419 = load float, float* %418, align 4
	  %418 = getelementptr inbounds float, float* %417, i64 %416
	  %417 = load float*, float** %4, align 8
	  %413 = load i32, i32* %z, align 4
	  %412 = getelementptr inbounds float, float* %411, i64 %410
	  %411 = load float*, float** %3, align 8
	  %407 = load i32, i32* %z, align 4
	  %405 = load float, float* %k, align 4
	  %404 = load float, float* %403, align 4
	  %403 = getelementptr inbounds float, float* %402, i64 %401
	  %402 = load float*, float** %3, align 8
	  %398 = load i32, i32* %z, align 4
	  %396 = load float, float* %k, align 4
	  %395 = load float, float* %r, align 4
	  %394 = fptrunc double %392 to float
	  store float %394, float* %r, align 4
	  %397 = fmul float %395, %396
	  store float %397, float* %k, align 4
	  %399 = add nsw i32 120, %398
	  %400 = srem i32 %399, 128
	  %401 = sext i32 %400 to i64
	  %406 = fmul float %404, %405
	  %408 = add nsw i32 120, %407
	  %409 = srem i32 %408, 128
	  %410 = sext i32 %409 to i64
	  store float %406, float* %412, align 4
	  %414 = add nsw i32 120, %413
	  %415 = srem i32 %414, 128
	  %416 = sext i32 %415 to i64
	  %421 = fmul float %419, %420
	  %423 = add nsw i32 120, %422
	  %424 = srem i32 %423, 128
	  %425 = sext i32 %424 to i64
	  store float %421, float* %427, align 4
	  %429 = add nsw i32 8, %428
	  %430 = srem i32 %429, 128
	  %431 = sext i32 %430 to i64
	  %437 = add nsw i32 240, %436
	  %438 = srem i32 %437, 128
	  %439 = sext i32 %438 to i64
	  %443 = fdiv float 1.000000e+00, %442
	  %444 = fmul float %435, %443
	  %445 = fmul float %434, %444
	  store float %445, float* %j, align 4
	  %447 = fpext float %446 to double
	  %449 = fpext float %448 to double
	  %450 = fadd double 1.000000e+00, %449
	  %451 = fdiv double 1.000000e+00, %450
	  %452 = fmul double %447, %451
	  %453 = fptrunc double %452 to float
	  store float %453, float* %k, align 4
	  %455 = fpext float %454 to double
	  %457 = fpext float %456 to double
	  %459 = call double @fmax(double %455, double %457) #6
	  %462 = call double @log10(double %459) #5
	  %465 = load float, float* %g, align 4
	  %464 = fptrunc double %462 to float
	  store float %464, float* %l, align 4
	  %466 = fsub float -0.000000e+00, %465
	  %467 = fmul float %466, 0x3F74CAB880000000
	  %468 = fpext float %467 to double
	  %470 = call double @exp(double %468) #5
	  %473 = load float, float* %g, align 4
	  %472 = fmul double 0x3FB16872C0000000, %470
	  %474 = fsub float -0.000000e+00, %473
	  %475 = fmul float %474, 0x3F454725E0000000
	  %476 = fpext float %475 to double
	  %478 = call double @exp(double %476) #5
	  %482 = load float, float* %g, align 4
	  %480 = fmul double 0x3FEDD2F1A0000000, %478
	  %481 = fadd double %472, %480
	  %483 = fdiv float 1.000000e+00, %482
	  %484 = fmul float -1.030000e+04, %483
	  %485 = fpext float %484 to double
	  %487 = call double @exp(double %485) #5
	  %493 = load float, float* %t, align 4
	  %491 = load float, float* %m, align 4
	  %489 = fadd double %481, %487
	  %490 = fptrunc double %489 to float
	  store float %490, float* %m, align 4
	  %492 = fpext float %491 to double
	  %494 = fpext float %493 to double
	  %496 = call double @fmax(double %492, double %494) #6
	  %499 = call double @log10(double %496) #5
	  %536 = load float, float* %q, align 4
	  %529 = load float, float* %s, align 4
	  %528 = load float, float* %s, align 4
	  %526 = load float, float* %n, align 4
	  %519 = load float, float* %p, align 4
	  %517 = load float, float* %o, align 4
	  %515 = load float, float* %p, align 4
	  %509 = load float, float* %n, align 4
	  %507 = load float, float* %l, align 4
	  %502 = load float, float* %n, align 4
	  %501 = fptrunc double %499 to float
	  store float %501, float* %n, align 4
	  %503 = fpext float %502 to double
	  %504 = fmul double 1.270000e+00, %503
	  %505 = fsub double 7.500000e-01, %504
	  %506 = fptrunc double %505 to float
	  store float %506, float* %o, align 4
	  %508 = fpext float %507 to double
	  %510 = fpext float %509 to double
	  %511 = fmul double 6.700000e-01, %510
	  %512 = fadd double 4.000000e-01, %511
	  %513 = fsub double %508, %512
	  %514 = fptrunc double %513 to float
	  store float %514, float* %p, align 4
	  %516 = fpext float %515 to double
	  %518 = fpext float %517 to double
	  %520 = fpext float %519 to double
	  %521 = fmul double 1.400000e-01, %520
	  %522 = fsub double %518, %521
	  %523 = fdiv double 1.000000e+00, %522
	  %524 = fmul double %516, %523
	  %525 = fptrunc double %524 to float
	  store float %525, float* %s, align 4
	  %527 = fpext float %526 to double
	  %530 = fmul float %528, %529
	  %531 = fpext float %530 to double
	  %532 = fadd double 1.000000e+00, %531
	  %533 = fdiv double 1.000000e+00, %532
	  %534 = fmul double %527, %533
	  %535 = fptrunc double %534 to float
	  store float %535, float* %q, align 4
	  %537 = fpext float %536 to double
	  %539 = call double @exp(double %537) #5
	  %603 = load float, float* %t, align 4
	  %601 = load float, float* %j, align 4
	  %595 = load float, float* %j, align 4
	  %593 = load float, float* %j, align 4
	  %589 = load float, float* %588, align 4
	  %588 = getelementptr inbounds float, float* %587, i64 %586
	  %587 = load float*, float** %3, align 8
	  %583 = load i32, i32* %z, align 4
	  %582 = load float, float* %v, align 4
	  %581 = load float, float* %580, align 4
	  %580 = getelementptr inbounds float, float* %579, i64 %578
	  %579 = load float*, float** %5, align 8
	  %575 = load i32, i32* %z, align 4
	  %574 = getelementptr inbounds float, float* %573, i64 %572
	  %573 = load float*, float** %4, align 8
	  %569 = load i32, i32* %z, align 4
	  %567 = load float, float* %k, align 4
	  %566 = load float, float* %565, align 4
	  %565 = getelementptr inbounds float, float* %564, i64 %563
	  %564 = load float*, float** %4, align 8
	  %560 = load i32, i32* %z, align 4
	  %559 = getelementptr inbounds float, float* %558, i64 %557
	  %558 = load float*, float** %3, align 8
	  %554 = load i32, i32* %z, align 4
	  %552 = load float, float* %k, align 4
	  %551 = load float, float* %550, align 4
	  %550 = getelementptr inbounds float, float* %549, i64 %548
	  %549 = load float*, float** %3, align 8
	  %545 = load i32, i32* %z, align 4
	  %543 = load float, float* %k, align 4
	  %542 = load float, float* %r, align 4
	  %541 = fptrunc double %539 to float
	  store float %541, float* %r, align 4
	  %544 = fmul float %542, %543
	  store float %544, float* %k, align 4
	  %546 = add nsw i32 240, %545
	  %547 = srem i32 %546, 128
	  %548 = sext i32 %547 to i64
	  %553 = fmul float %551, %552
	  %555 = add nsw i32 240, %554
	  %556 = srem i32 %555, 128
	  %557 = sext i32 %556 to i64
	  store float %553, float* %559, align 4
	  %561 = add nsw i32 240, %560
	  %562 = srem i32 %561, 128
	  %563 = sext i32 %562 to i64
	  %568 = fmul float %566, %567
	  %570 = add nsw i32 240, %569
	  %571 = srem i32 %570, 128
	  %572 = sext i32 %571 to i64
	  store float %568, float* %574, align 4
	  %576 = add nsw i32 16, %575
	  %577 = srem i32 %576, 128
	  %578 = sext i32 %577 to i64
	  %584 = add nsw i32 304, %583
	  %585 = srem i32 %584, 128
	  %586 = sext i32 %585 to i64
	  %590 = fdiv float 1.000000e+00, %589
	  %591 = fmul float %582, %590
	  %592 = fmul float %581, %591
	  store float %592, float* %j, align 4
	  %594 = fpext float %593 to double
	  %596 = fpext float %595 to double
	  %597 = fadd double 1.000000e+00, %596
	  %598 = fdiv double 1.000000e+00, %597
	  %599 = fmul double %594, %598
	  %600 = fptrunc double %599 to float
	  store float %600, float* %k, align 4
	  %602 = fpext float %601 to double
	  %604 = fpext float %603 to double
	  %606 = call double @fmax(double %602, double %604) #6
	  %609 = call double @log10(double %606) #5
	  %612 = load float, float* %g, align 4
	  %611 = fptrunc double %609 to float
	  store float %611, float* %l, align 4
	  %613 = fsub float -0.000000e+00, %612
	  %614 = fmul float %613, 0x3F71485F00000000
	  %615 = fpext float %614 to double
	  %617 = call double @exp(double %615) #5
	  %620 = load float, float* %g, align 4
	  %619 = fmul double 0x3FDB27BB20000000, %617
	  %621 = fsub float -0.000000e+00, %620
	  %622 = fmul float %621, 0x3F43D5D9A0000000
	  %623 = fpext float %622 to double
	  %625 = call double @exp(double %623) #5
	  %629 = load float, float* %g, align 4
	  %627 = fmul double 0x3FE26C2260000000, %625
	  %628 = fadd double %619, %627
	  %630 = fdiv float 1.000000e+00, %629
	  %631 = fmul float -5.069000e+03, %630
	  %632 = fpext float %631 to double
	  %634 = call double @exp(double %632) #5
	  %640 = load float, float* %t, align 4
	  %638 = load float, float* %m, align 4
	  %636 = fadd double %628, %634
	  %637 = fptrunc double %636 to float
	  store float %637, float* %m, align 4
	  %639 = fpext float %638 to double
	  %641 = fpext float %640 to double
	  %643 = call double @fmax(double %639, double %641) #6
	  %646 = call double @log10(double %643) #5
	  %683 = load float, float* %q, align 4
	  %676 = load float, float* %s, align 4
	  %675 = load float, float* %s, align 4
	  %673 = load float, float* %n, align 4
	  %666 = load float, float* %p, align 4
	  %664 = load float, float* %o, align 4
	  %662 = load float, float* %p, align 4
	  %656 = load float, float* %n, align 4
	  %654 = load float, float* %l, align 4
	  %649 = load float, float* %n, align 4
	  %648 = fptrunc double %646 to float
	  store float %648, float* %n, align 4
	  %650 = fpext float %649 to double
	  %651 = fmul double 1.270000e+00, %650
	  %652 = fsub double 7.500000e-01, %651
	  %653 = fptrunc double %652 to float
	  store float %653, float* %o, align 4
	  %655 = fpext float %654 to double
	  %657 = fpext float %656 to double
	  %658 = fmul double 6.700000e-01, %657
	  %659 = fadd double 4.000000e-01, %658
	  %660 = fsub double %655, %659
	  %661 = fptrunc double %660 to float
	  store float %661, float* %p, align 4
	  %663 = fpext float %662 to double
	  %665 = fpext float %664 to double
	  %667 = fpext float %666 to double
	  %668 = fmul double 1.400000e-01, %667
	  %669 = fsub double %665, %668
	  %670 = fdiv double 1.000000e+00, %669
	  %671 = fmul double %663, %670
	  %672 = fptrunc double %671 to float
	  store float %672, float* %s, align 4
	  %674 = fpext float %673 to double
	  %677 = fmul float %675, %676
	  %678 = fpext float %677 to double
	  %679 = fadd double 1.000000e+00, %678
	  %680 = fdiv double 1.000000e+00, %679
	  %681 = fmul double %674, %680
	  %682 = fptrunc double %681 to float
	  store float %682, float* %q, align 4
	  %684 = fpext float %683 to double
	  %686 = call double @exp(double %684) #5
	  %750 = load float, float* %t, align 4
	  %748 = load float, float* %j, align 4
	  %742 = load float, float* %j, align 4
	  %740 = load float, float* %j, align 4
	  %736 = load float, float* %735, align 4
	  %735 = getelementptr inbounds float, float* %734, i64 %733
	  %734 = load float*, float** %3, align 8
	  %730 = load i32, i32* %z, align 4
	  %729 = load float, float* %v, align 4
	  %728 = load float, float* %727, align 4
	  %727 = getelementptr inbounds float, float* %726, i64 %725
	  %726 = load float*, float** %5, align 8
	  %722 = load i32, i32* %z, align 4
	  %721 = getelementptr inbounds float, float* %720, i64 %719
	  %720 = load float*, float** %4, align 8
	  %716 = load i32, i32* %z, align 4
	  %714 = load float, float* %k, align 4
	  %713 = load float, float* %712, align 4
	  %712 = getelementptr inbounds float, float* %711, i64 %710
	  %711 = load float*, float** %4, align 8
	  %707 = load i32, i32* %z, align 4
	  %706 = getelementptr inbounds float, float* %705, i64 %704
	  %705 = load float*, float** %3, align 8
	  %701 = load i32, i32* %z, align 4
	  %699 = load float, float* %k, align 4
	  %698 = load float, float* %697, align 4
	  %697 = getelementptr inbounds float, float* %696, i64 %695
	  %696 = load float*, float** %3, align 8
	  %692 = load i32, i32* %z, align 4
	  %690 = load float, float* %k, align 4
	  %689 = load float, float* %r, align 4
	  %688 = fptrunc double %686 to float
	  store float %688, float* %r, align 4
	  %691 = fmul float %689, %690
	  store float %691, float* %k, align 4
	  %693 = add nsw i32 304, %692
	  %694 = srem i32 %693, 128
	  %695 = sext i32 %694 to i64
	  %700 = fmul float %698, %699
	  %702 = add nsw i32 304, %701
	  %703 = srem i32 %702, 128
	  %704 = sext i32 %703 to i64
	  store float %700, float* %706, align 4
	  %708 = add nsw i32 304, %707
	  %709 = srem i32 %708, 128
	  %710 = sext i32 %709 to i64
	  %715 = fmul float %713, %714
	  %717 = add nsw i32 304, %716
	  %718 = srem i32 %717, 128
	  %719 = sext i32 %718 to i64
	  store float %715, float* %721, align 4
	  %723 = add nsw i32 24, %722
	  %724 = srem i32 %723, 128
	  %725 = sext i32 %724 to i64
	  %731 = add nsw i32 320, %730
	  %732 = srem i32 %731, 128
	  %733 = sext i32 %732 to i64
	  %737 = fdiv float 1.000000e+00, %736
	  %738 = fmul float %729, %737
	  %739 = fmul float %728, %738
	  store float %739, float* %j, align 4
	  %741 = fpext float %740 to double
	  %743 = fpext float %742 to double
	  %744 = fadd double 1.000000e+00, %743
	  %745 = fdiv double 1.000000e+00, %744
	  %746 = fmul double %741, %745
	  %747 = fptrunc double %746 to float
	  store float %747, float* %k, align 4
	  %749 = fpext float %748 to double
	  %751 = fpext float %750 to double
	  %753 = call double @fmax(double %749, double %751) #6
	  %756 = call double @log10(double %753) #5
	  %759 = load float, float* %g, align 4
	  %758 = fptrunc double %756 to float
	  store float %758, float* %l, align 4
	  %760 = fsub float -0.000000e+00, %759
	  %761 = fmul float %760, 0x3F6E3A9180000000
	  %762 = fpext float %761 to double
	  %764 = call double @exp(double %762) #5
	  %767 = load float, float* %g, align 4
	  %766 = fmul double 0x3FCBDA5120000000, %764
	  %768 = fsub float -0.000000e+00, %767
	  %769 = fmul float %768, 0x3F37C9BBC0000000
	  %770 = fpext float %769 to double
	  %772 = call double @exp(double %770) #5
	  %776 = load float, float* %g, align 4
	  %774 = fmul double 0x3FE9096BC0000000, %772
	  %775 = fadd double %766, %774
	  %777 = fdiv float 1.000000e+00, %776
	  %778 = fmul float -6.570000e+03, %777
	  %779 = fpext float %778 to double
	  %781 = call double @exp(double %779) #5
	  %787 = load float, float* %t, align 4
	  %785 = load float, float* %m, align 4
	  %783 = fadd double %775, %781
	  %784 = fptrunc double %783 to float
	  store float %784, float* %m, align 4
	  %786 = fpext float %785 to double
	  %788 = fpext float %787 to double
	  %790 = call double @fmax(double %786, double %788) #6
	  %793 = call double @log10(double %790) #5
	  %830 = load float, float* %q, align 4
	  %823 = load float, float* %s, align 4
	  %822 = load float, float* %s, align 4
	  %820 = load float, float* %n, align 4
	  %813 = load float, float* %p, align 4
	  %811 = load float, float* %o, align 4
	  %809 = load float, float* %p, align 4
	  %803 = load float, float* %n, align 4
	  %801 = load float, float* %l, align 4
	  %796 = load float, float* %n, align 4
	  %795 = fptrunc double %793 to float
	  store float %795, float* %n, align 4
	  %797 = fpext float %796 to double
	  %798 = fmul double 1.270000e+00, %797
	  %799 = fsub double 7.500000e-01, %798
	  %800 = fptrunc double %799 to float
	  store float %800, float* %o, align 4
	  %802 = fpext float %801 to double
	  %804 = fpext float %803 to double
	  %805 = fmul double 6.700000e-01, %804
	  %806 = fadd double 4.000000e-01, %805
	  %807 = fsub double %802, %806
	  %808 = fptrunc double %807 to float
	  store float %808, float* %p, align 4
	  %810 = fpext float %809 to double
	  %812 = fpext float %811 to double
	  %814 = fpext float %813 to double
	  %815 = fmul double 1.400000e-01, %814
	  %816 = fsub double %812, %815
	  %817 = fdiv double 1.000000e+00, %816
	  %818 = fmul double %810, %817
	  %819 = fptrunc double %818 to float
	  store float %819, float* %s, align 4
	  %821 = fpext float %820 to double
	  %824 = fmul float %822, %823
	  %825 = fpext float %824 to double
	  %826 = fadd double 1.000000e+00, %825
	  %827 = fdiv double 1.000000e+00, %826
	  %828 = fmul double %821, %827
	  %829 = fptrunc double %828 to float
	  store float %829, float* %q, align 4
	  %831 = fpext float %830 to double
	  %833 = call double @exp(double %831) #5
	  %897 = load float, float* %t, align 4
	  %895 = load float, float* %j, align 4
	  %889 = load float, float* %j, align 4
	  %887 = load float, float* %j, align 4
	  %883 = load float, float* %882, align 4
	  %882 = getelementptr inbounds float, float* %881, i64 %880
	  %881 = load float*, float** %3, align 8
	  %877 = load i32, i32* %z, align 4
	  %876 = load float, float* %v, align 4
	  %875 = load float, float* %874, align 4
	  %874 = getelementptr inbounds float, float* %873, i64 %872
	  %873 = load float*, float** %5, align 8
	  %869 = load i32, i32* %z, align 4
	  %868 = getelementptr inbounds float, float* %867, i64 %866
	  %867 = load float*, float** %4, align 8
	  %863 = load i32, i32* %z, align 4
	  %861 = load float, float* %k, align 4
	  %860 = load float, float* %859, align 4
	  %859 = getelementptr inbounds float, float* %858, i64 %857
	  %858 = load float*, float** %4, align 8
	  %854 = load i32, i32* %z, align 4
	  %853 = getelementptr inbounds float, float* %852, i64 %851
	  %852 = load float*, float** %3, align 8
	  %848 = load i32, i32* %z, align 4
	  %846 = load float, float* %k, align 4
	  %845 = load float, float* %844, align 4
	  %844 = getelementptr inbounds float, float* %843, i64 %842
	  %843 = load float*, float** %3, align 8
	  %839 = load i32, i32* %z, align 4
	  %837 = load float, float* %k, align 4
	  %836 = load float, float* %r, align 4
	  %835 = fptrunc double %833 to float
	  store float %835, float* %r, align 4
	  %838 = fmul float %836, %837
	  store float %838, float* %k, align 4
	  %840 = add nsw i32 320, %839
	  %841 = srem i32 %840, 128
	  %842 = sext i32 %841 to i64
	  %847 = fmul float %845, %846
	  %849 = add nsw i32 320, %848
	  %850 = srem i32 %849, 128
	  %851 = sext i32 %850 to i64
	  store float %847, float* %853, align 4
	  %855 = add nsw i32 320, %854
	  %856 = srem i32 %855, 128
	  %857 = sext i32 %856 to i64
	  %862 = fmul float %860, %861
	  %864 = add nsw i32 320, %863
	  %865 = srem i32 %864, 128
	  %866 = sext i32 %865 to i64
	  store float %862, float* %868, align 4
	  %870 = add nsw i32 32, %869
	  %871 = srem i32 %870, 128
	  %872 = sext i32 %871 to i64
	  %878 = add nsw i32 376, %877
	  %879 = srem i32 %878, 128
	  %880 = sext i32 %879 to i64
	  %884 = fdiv float 1.000000e+00, %883
	  %885 = fmul float %876, %884
	  %886 = fmul float %875, %885
	  store float %886, float* %j, align 4
	  %888 = fpext float %887 to double
	  %890 = fpext float %889 to double
	  %891 = fadd double 1.000000e+00, %890
	  %892 = fdiv double 1.000000e+00, %891
	  %893 = fmul double %888, %892
	  %894 = fptrunc double %893 to float
	  store float %894, float* %k, align 4
	  %896 = fpext float %895 to double
	  %898 = fpext float %897 to double
	  %900 = call double @fmax(double %896, double %898) #6
	  %903 = call double @log10(double %900) #5
	  %906 = load float, float* %g, align 4
	  %905 = fptrunc double %903 to float
	  store float %905, float* %l, align 4
	  %907 = fsub float -0.000000e+00, %906
	  %908 = fmul float %907, 0x3F8A41A420000000
	  %909 = fpext float %908 to double
	  %911 = call double @exp(double %909) #5
	  %914 = load float, float* %g, align 4
	  %913 = fmul double 0x3FD47AE140000000, %911
	  %915 = fsub float -0.000000e+00, %914
	  %916 = fmul float %915, 0x3F406CD0E0000000
	  %917 = fpext float %916 to double
	  %919 = call double @exp(double %917) #5
	  %923 = load float, float* %g, align 4
	  %921 = fmul double 0x3FE5C28F60000000, %919
	  %922 = fadd double %913, %921
	  %924 = fdiv float 1.000000e+00, %923
	  %925 = fmul float -5.590000e+03, %924
	  %926 = fpext float %925 to double
	  %928 = call double @exp(double %926) #5
	  %934 = load float, float* %t, align 4
	  %932 = load float, float* %m, align 4
	  %930 = fadd double %922, %928
	  %931 = fptrunc double %930 to float
	  store float %931, float* %m, align 4
	  %933 = fpext float %932 to double
	  %935 = fpext float %934 to double
	  %937 = call double @fmax(double %933, double %935) #6
	  %940 = call double @log10(double %937) #5
	  %977 = load float, float* %q, align 4
	  %970 = load float, float* %s, align 4
	  %969 = load float, float* %s, align 4
	  %967 = load float, float* %n, align 4
	  %960 = load float, float* %p, align 4
	  %958 = load float, float* %o, align 4
	  %956 = load float, float* %p, align 4
	  %950 = load float, float* %n, align 4
	  %948 = load float, float* %l, align 4
	  %943 = load float, float* %n, align 4
	  %942 = fptrunc double %940 to float
	  store float %942, float* %n, align 4
	  %944 = fpext float %943 to double
	  %945 = fmul double 1.270000e+00, %944
	  %946 = fsub double 7.500000e-01, %945
	  %947 = fptrunc double %946 to float
	  store float %947, float* %o, align 4
	  %949 = fpext float %948 to double
	  %951 = fpext float %950 to double
	  %952 = fmul double 6.700000e-01, %951
	  %953 = fadd double 4.000000e-01, %952
	  %954 = fsub double %949, %953
	  %955 = fptrunc double %954 to float
	  store float %955, float* %p, align 4
	  %957 = fpext float %956 to double
	  %959 = fpext float %958 to double
	  %961 = fpext float %960 to double
	  %962 = fmul double 1.400000e-01, %961
	  %963 = fsub double %959, %962
	  %964 = fdiv double 1.000000e+00, %963
	  %965 = fmul double %957, %964
	  %966 = fptrunc double %965 to float
	  store float %966, float* %s, align 4
	  %968 = fpext float %967 to double
	  %971 = fmul float %969, %970
	  %972 = fpext float %971 to double
	  %973 = fadd double 1.000000e+00, %972
	  %974 = fdiv double 1.000000e+00, %973
	  %975 = fmul double %968, %974
	  %976 = fptrunc double %975 to float
	  store float %976, float* %q, align 4
	  %978 = fpext float %977 to double
	  %980 = call double @exp(double %978) #5
	  %1044 = load float, float* %t, align 4
	  %1042 = load float, float* %j, align 4
	  %1036 = load float, float* %j, align 4
	  %1034 = load float, float* %j, align 4
	  %1030 = load float, float* %1029, align 4
	  %1029 = getelementptr inbounds float, float* %1028, i64 %1027
	  %1028 = load float*, float** %3, align 8
	  %1024 = load i32, i32* %z, align 4
	  %1023 = load float, float* %v, align 4
	  %1022 = load float, float* %1021, align 4
	  %1021 = getelementptr inbounds float, float* %1020, i64 %1019
	  %1020 = load float*, float** %5, align 8
	  %1016 = load i32, i32* %z, align 4
	  %1015 = getelementptr inbounds float, float* %1014, i64 %1013
	  %1014 = load float*, float** %4, align 8
	  %1010 = load i32, i32* %z, align 4
	  %1008 = load float, float* %k, align 4
	  %1007 = load float, float* %1006, align 4
	  %1006 = getelementptr inbounds float, float* %1005, i64 %1004
	  %1005 = load float*, float** %4, align 8
	  %1001 = load i32, i32* %z, align 4
	  %1000 = getelementptr inbounds float, float* %999, i64 %998
	  %999 = load float*, float** %3, align 8
	  %995 = load i32, i32* %z, align 4
	  %993 = load float, float* %k, align 4
	  %992 = load float, float* %991, align 4
	  %991 = getelementptr inbounds float, float* %990, i64 %989
	  %990 = load float*, float** %3, align 8
	  %986 = load i32, i32* %z, align 4
	  %984 = load float, float* %k, align 4
	  %983 = load float, float* %r, align 4
	  %982 = fptrunc double %980 to float
	  store float %982, float* %r, align 4
	  %985 = fmul float %983, %984
	  store float %985, float* %k, align 4
	  %987 = add nsw i32 376, %986
	  %988 = srem i32 %987, 128
	  %989 = sext i32 %988 to i64
	  %994 = fmul float %992, %993
	  %996 = add nsw i32 376, %995
	  %997 = srem i32 %996, 128
	  %998 = sext i32 %997 to i64
	  store float %994, float* %1000, align 4
	  %1002 = add nsw i32 376, %1001
	  %1003 = srem i32 %1002, 128
	  %1004 = sext i32 %1003 to i64
	  %1009 = fmul float %1007, %1008
	  %1011 = add nsw i32 376, %1010
	  %1012 = srem i32 %1011, 128
	  %1013 = sext i32 %1012 to i64
	  store float %1009, float* %1015, align 4
	  %1017 = add nsw i32 40, %1016
	  %1018 = srem i32 %1017, 128
	  %1019 = sext i32 %1018 to i64
	  %1025 = add nsw i32 440, %1024
	  %1026 = srem i32 %1025, 128
	  %1027 = sext i32 %1026 to i64
	  %1031 = fdiv float 1.000000e+00, %1030
	  %1032 = fmul float %1023, %1031
	  %1033 = fmul float %1022, %1032
	  store float %1033, float* %j, align 4
	  %1035 = fpext float %1034 to double
	  %1037 = fpext float %1036 to double
	  %1038 = fadd double 1.000000e+00, %1037
	  %1039 = fdiv double 1.000000e+00, %1038
	  %1040 = fmul double %1035, %1039
	  %1041 = fptrunc double %1040 to float
	  store float %1041, float* %k, align 4
	  %1043 = fpext float %1042 to double
	  %1045 = fpext float %1044 to double
	  %1047 = call double @fmax(double %1043, double %1045) #6
	  %1050 = call double @log10(double %1047) #5
	  %1053 = load float, float* %g, align 4
	  %1052 = fptrunc double %1050 to float
	  store float %1052, float* %l, align 4
	  %1054 = fsub float -0.000000e+00, %1053
	  %1055 = fmul float %1054, 0x3F6DCA01E0000000
	  %1056 = fpext float %1055 to double
	  %1058 = call double @exp(double %1056) #5
	  %1061 = load float, float* %g, align 4
	  %1060 = fmul double 0x3FDA31F8A0000000, %1058
	  %1062 = fsub float -0.000000e+00, %1061
	  %1063 = fmul float %1062, 0x3F4ABA4200000000
	  %1064 = fpext float %1063 to double
	  %1066 = call double @exp(double %1064) #5
	  %1070 = load float, float* %g, align 4
	  %1068 = fmul double 0x3FE2E703A0000000, %1066
	  %1069 = fadd double %1060, %1068
	  %1071 = fdiv float 1.000000e+00, %1070
	  %1072 = fmul float -5.185000e+03, %1071
	  %1073 = fpext float %1072 to double
	  %1075 = call double @exp(double %1073) #5
	  %1081 = load float, float* %t, align 4
	  %1079 = load float, float* %m, align 4
	  %1077 = fadd double %1069, %1075
	  %1078 = fptrunc double %1077 to float
	  store float %1078, float* %m, align 4
	  %1080 = fpext float %1079 to double
	  %1082 = fpext float %1081 to double
	  %1084 = call double @fmax(double %1080, double %1082) #6
	  %1087 = call double @log10(double %1084) #5
	  %1124 = load float, float* %q, align 4
	  %1117 = load float, float* %s, align 4
	  %1116 = load float, float* %s, align 4
	  %1114 = load float, float* %n, align 4
	  %1107 = load float, float* %p, align 4
	  %1105 = load float, float* %o, align 4
	  %1103 = load float, float* %p, align 4
	  %1097 = load float, float* %n, align 4
	  %1095 = load float, float* %l, align 4
	  %1090 = load float, float* %n, align 4
	  %1089 = fptrunc double %1087 to float
	  store float %1089, float* %n, align 4
	  %1091 = fpext float %1090 to double
	  %1092 = fmul double 1.270000e+00, %1091
	  %1093 = fsub double 7.500000e-01, %1092
	  %1094 = fptrunc double %1093 to float
	  store float %1094, float* %o, align 4
	  %1096 = fpext float %1095 to double
	  %1098 = fpext float %1097 to double
	  %1099 = fmul double 6.700000e-01, %1098
	  %1100 = fadd double 4.000000e-01, %1099
	  %1101 = fsub double %1096, %1100
	  %1102 = fptrunc double %1101 to float
	  store float %1102, float* %p, align 4
	  %1104 = fpext float %1103 to double
	  %1106 = fpext float %1105 to double
	  %1108 = fpext float %1107 to double
	  %1109 = fmul double 1.400000e-01, %1108
	  %1110 = fsub double %1106, %1109
	  %1111 = fdiv double 1.000000e+00, %1110
	  %1112 = fmul double %1104, %1111
	  %1113 = fptrunc double %1112 to float
	  store float %1113, float* %s, align 4
	  %1115 = fpext float %1114 to double
	  %1118 = fmul float %1116, %1117
	  %1119 = fpext float %1118 to double
	  %1120 = fadd double 1.000000e+00, %1119
	  %1121 = fdiv double 1.000000e+00, %1120
	  %1122 = fmul double %1115, %1121
	  %1123 = fptrunc double %1122 to float
	  store float %1123, float* %q, align 4
	  %1125 = fpext float %1124 to double
	  %1127 = call double @exp(double %1125) #5
	  %1191 = load float, float* %t, align 4
	  %1189 = load float, float* %j, align 4
	  %1183 = load float, float* %j, align 4
	  %1181 = load float, float* %j, align 4
	  %1177 = load float, float* %1176, align 4
	  %1176 = getelementptr inbounds float, float* %1175, i64 %1174
	  %1175 = load float*, float** %3, align 8
	  %1171 = load i32, i32* %z, align 4
	  %1170 = load float, float* %v, align 4
	  %1169 = load float, float* %1168, align 4
	  %1168 = getelementptr inbounds float, float* %1167, i64 %1166
	  %1167 = load float*, float** %5, align 8
	  %1163 = load i32, i32* %z, align 4
	  %1162 = getelementptr inbounds float, float* %1161, i64 %1160
	  %1161 = load float*, float** %4, align 8
	  %1157 = load i32, i32* %z, align 4
	  %1155 = load float, float* %k, align 4
	  %1154 = load float, float* %1153, align 4
	  %1153 = getelementptr inbounds float, float* %1152, i64 %1151
	  %1152 = load float*, float** %4, align 8
	  %1148 = load i32, i32* %z, align 4
	  %1147 = getelementptr inbounds float, float* %1146, i64 %1145
	  %1146 = load float*, float** %3, align 8
	  %1142 = load i32, i32* %z, align 4
	  %1140 = load float, float* %k, align 4
	  %1139 = load float, float* %1138, align 4
	  %1138 = getelementptr inbounds float, float* %1137, i64 %1136
	  %1137 = load float*, float** %3, align 8
	  %1133 = load i32, i32* %z, align 4
	  %1131 = load float, float* %k, align 4
	  %1130 = load float, float* %r, align 4
	  %1129 = fptrunc double %1127 to float
	  store float %1129, float* %r, align 4
	  %1132 = fmul float %1130, %1131
	  store float %1132, float* %k, align 4
	  %1134 = add nsw i32 440, %1133
	  %1135 = srem i32 %1134, 128
	  %1136 = sext i32 %1135 to i64
	  %1141 = fmul float %1139, %1140
	  %1143 = add nsw i32 440, %1142
	  %1144 = srem i32 %1143, 128
	  %1145 = sext i32 %1144 to i64
	  store float %1141, float* %1147, align 4
	  %1149 = add nsw i32 440, %1148
	  %1150 = srem i32 %1149, 128
	  %1151 = sext i32 %1150 to i64
	  %1156 = fmul float %1154, %1155
	  %1158 = add nsw i32 440, %1157
	  %1159 = srem i32 %1158, 128
	  %1160 = sext i32 %1159 to i64
	  store float %1156, float* %1162, align 4
	  %1164 = add nsw i32 48, %1163
	  %1165 = srem i32 %1164, 128
	  %1166 = sext i32 %1165 to i64
	  %1172 = add nsw i32 560, %1171
	  %1173 = srem i32 %1172, 128
	  %1174 = sext i32 %1173 to i64
	  %1178 = fdiv float 1.000000e+00, %1177
	  %1179 = fmul float %1170, %1178
	  %1180 = fmul float %1169, %1179
	  store float %1180, float* %j, align 4
	  %1182 = fpext float %1181 to double
	  %1184 = fpext float %1183 to double
	  %1185 = fadd double 1.000000e+00, %1184
	  %1186 = fdiv double 1.000000e+00, %1185
	  %1187 = fmul double %1182, %1186
	  %1188 = fptrunc double %1187 to float
	  store float %1188, float* %k, align 4
	  %1190 = fpext float %1189 to double
	  %1192 = fpext float %1191 to double
	  %1194 = call double @fmax(double %1190, double %1192) #6
	  %1197 = call double @log10(double %1194) #5
	  %1200 = load float, float* %g, align 4
	  %1199 = fptrunc double %1197 to float
	  store float %1199, float* %l, align 4
	  %1201 = fsub float -0.000000e+00, %1200
	  %1202 = fmul float %1201, 0x3F85C98820000000
	  %1203 = fpext float %1202 to double
	  %1205 = call double @exp(double %1203) #5
	  %1208 = load float, float* %g, align 4
	  %1207 = fmul double 0x3FCEF9DB20000000, %1205
	  %1209 = fsub float -0.000000e+00, %1208
	  %1210 = fmul float %1209, 0x3F45129A60000000
	  %1211 = fpext float %1210 to double
	  %1213 = call double @exp(double %1211) #5
	  %1217 = load float, float* %g, align 4
	  %1215 = fmul double 0x3FE8418940000000, %1213
	  %1216 = fadd double %1207, %1215
	  %1218 = fdiv float 1.000000e+00, %1217
	  %1219 = fmul float -4.200000e+03, %1218
	  %1220 = fpext float %1219 to double
	  %1222 = call double @exp(double %1220) #5
	  %1228 = load float, float* %t, align 4
	  %1226 = load float, float* %m, align 4
	  %1224 = fadd double %1216, %1222
	  %1225 = fptrunc double %1224 to float
	  store float %1225, float* %m, align 4
	  %1227 = fpext float %1226 to double
	  %1229 = fpext float %1228 to double
	  %1231 = call double @fmax(double %1227, double %1229) #6
	  %1234 = call double @log10(double %1231) #5
	  %1271 = load float, float* %q, align 4
	  %1264 = load float, float* %s, align 4
	  %1263 = load float, float* %s, align 4
	  %1261 = load float, float* %n, align 4
	  %1254 = load float, float* %p, align 4
	  %1252 = load float, float* %o, align 4
	  %1250 = load float, float* %p, align 4
	  %1244 = load float, float* %n, align 4
	  %1242 = load float, float* %l, align 4
	  %1237 = load float, float* %n, align 4
	  %1236 = fptrunc double %1234 to float
	  store float %1236, float* %n, align 4
	  %1238 = fpext float %1237 to double
	  %1239 = fmul double 1.270000e+00, %1238
	  %1240 = fsub double 7.500000e-01, %1239
	  %1241 = fptrunc double %1240 to float
	  store float %1241, float* %o, align 4
	  %1243 = fpext float %1242 to double
	  %1245 = fpext float %1244 to double
	  %1246 = fmul double 6.700000e-01, %1245
	  %1247 = fadd double 4.000000e-01, %1246
	  %1248 = fsub double %1243, %1247
	  %1249 = fptrunc double %1248 to float
	  store float %1249, float* %p, align 4
	  %1251 = fpext float %1250 to double
	  %1253 = fpext float %1252 to double
	  %1255 = fpext float %1254 to double
	  %1256 = fmul double 1.400000e-01, %1255
	  %1257 = fsub double %1253, %1256
	  %1258 = fdiv double 1.000000e+00, %1257
	  %1259 = fmul double %1251, %1258
	  %1260 = fptrunc double %1259 to float
	  store float %1260, float* %s, align 4
	  %1262 = fpext float %1261 to double
	  %1265 = fmul float %1263, %1264
	  %1266 = fpext float %1265 to double
	  %1267 = fadd double 1.000000e+00, %1266
	  %1268 = fdiv double 1.000000e+00, %1267
	  %1269 = fmul double %1262, %1268
	  %1270 = fptrunc double %1269 to float
	  store float %1270, float* %q, align 4
	  %1272 = fpext float %1271 to double
	  %1274 = call double @exp(double %1272) #5
	  %1338 = load float, float* %t, align 4
	  %1336 = load float, float* %j, align 4
	  %1330 = load float, float* %j, align 4
	  %1328 = load float, float* %j, align 4
	  %1324 = load float, float* %1323, align 4
	  %1323 = getelementptr inbounds float, float* %1322, i64 %1321
	  %1322 = load float*, float** %3, align 8
	  %1318 = load i32, i32* %z, align 4
	  %1317 = load float, float* %v, align 4
	  %1316 = load float, float* %1315, align 4
	  %1315 = getelementptr inbounds float, float* %1314, i64 %1313
	  %1314 = load float*, float** %5, align 8
	  %1310 = load i32, i32* %z, align 4
	  %1309 = getelementptr inbounds float, float* %1308, i64 %1307
	  %1308 = load float*, float** %4, align 8
	  %1304 = load i32, i32* %z, align 4
	  %1302 = load float, float* %k, align 4
	  %1301 = load float, float* %1300, align 4
	  %1300 = getelementptr inbounds float, float* %1299, i64 %1298
	  %1299 = load float*, float** %4, align 8
	  %1295 = load i32, i32* %z, align 4
	  %1294 = getelementptr inbounds float, float* %1293, i64 %1292
	  %1293 = load float*, float** %3, align 8
	  %1289 = load i32, i32* %z, align 4
	  %1287 = load float, float* %k, align 4
	  %1286 = load float, float* %1285, align 4
	  %1285 = getelementptr inbounds float, float* %1284, i64 %1283
	  %1284 = load float*, float** %3, align 8
	  %1280 = load i32, i32* %z, align 4
	  %1278 = load float, float* %k, align 4
	  %1277 = load float, float* %r, align 4
	  %1276 = fptrunc double %1274 to float
	  store float %1276, float* %r, align 4
	  %1279 = fmul float %1277, %1278
	  store float %1279, float* %k, align 4
	  %1281 = add nsw i32 560, %1280
	  %1282 = srem i32 %1281, 128
	  %1283 = sext i32 %1282 to i64
	  %1288 = fmul float %1286, %1287
	  %1290 = add nsw i32 560, %1289
	  %1291 = srem i32 %1290, 128
	  %1292 = sext i32 %1291 to i64
	  store float %1288, float* %1294, align 4
	  %1296 = add nsw i32 560, %1295
	  %1297 = srem i32 %1296, 128
	  %1298 = sext i32 %1297 to i64
	  %1303 = fmul float %1301, %1302
	  %1305 = add nsw i32 560, %1304
	  %1306 = srem i32 %1305, 128
	  %1307 = sext i32 %1306 to i64
	  store float %1303, float* %1309, align 4
	  %1311 = add nsw i32 56, %1310
	  %1312 = srem i32 %1311, 128
	  %1313 = sext i32 %1312 to i64
	  %1319 = add nsw i32 616, %1318
	  %1320 = srem i32 %1319, 128
	  %1321 = sext i32 %1320 to i64
	  %1325 = fdiv float 1.000000e+00, %1324
	  %1326 = fmul float %1317, %1325
	  %1327 = fmul float %1316, %1326
	  store float %1327, float* %j, align 4
	  %1329 = fpext float %1328 to double
	  %1331 = fpext float %1330 to double
	  %1332 = fadd double 1.000000e+00, %1331
	  %1333 = fdiv double 1.000000e+00, %1332
	  %1334 = fmul double %1329, %1333
	  %1335 = fptrunc double %1334 to float
	  store float %1335, float* %k, align 4
	  %1337 = fpext float %1336 to double
	  %1339 = fpext float %1338 to double
	  %1341 = call double @fmax(double %1337, double %1339) #6
	  %1344 = call double @log10(double %1341) #5
	  %1347 = load float, float* %g, align 4
	  %1346 = fptrunc double %1344 to float
	  store float %1346, float* %l, align 4
	  %1348 = fsub float -0.000000e+00, %1347
	  %1349 = fmul float %1348, 0x3F8BACF920000000
	  %1350 = fpext float %1349 to double
	  %1352 = call double @exp(double %1350) #5
	  %1355 = load float, float* %g, align 4
	  %1354 = fmul double 0x3FCBC6A7E0000000, %1352
	  %1356 = fsub float -0.000000e+00, %1355
	  %1357 = fmul float %1356, 0x3F36489880000000
	  %1358 = fpext float %1357 to double
	  %1360 = call double @exp(double %1358) #5
	  %1364 = load float, float* %g, align 4
	  %1362 = fmul double 0x3FE90E5600000000, %1360
	  %1363 = fadd double %1354, %1362
	  %1365 = fdiv float 1.000000e+00, %1364
	  %1366 = fmul float -6.964000e+03, %1365
	  %1367 = fpext float %1366 to double
	  %1369 = call double @exp(double %1367) #5
	  %1375 = load float, float* %t, align 4
	  %1373 = load float, float* %m, align 4
	  %1371 = fadd double %1363, %1369
	  %1372 = fptrunc double %1371 to float
	  store float %1372, float* %m, align 4
	  %1374 = fpext float %1373 to double
	  %1376 = fpext float %1375 to double
	  %1378 = call double @fmax(double %1374, double %1376) #6
	  %1381 = call double @log10(double %1378) #5
	  %1418 = load float, float* %q, align 4
	  %1411 = load float, float* %s, align 4
	  %1410 = load float, float* %s, align 4
	  %1408 = load float, float* %n, align 4
	  %1401 = load float, float* %p, align 4
	  %1399 = load float, float* %o, align 4
	  %1397 = load float, float* %p, align 4
	  %1391 = load float, float* %n, align 4
	  %1389 = load float, float* %l, align 4
	  %1384 = load float, float* %n, align 4
	  %1383 = fptrunc double %1381 to float
	  store float %1383, float* %n, align 4
	  %1385 = fpext float %1384 to double
	  %1386 = fmul double 1.270000e+00, %1385
	  %1387 = fsub double 7.500000e-01, %1386
	  %1388 = fptrunc double %1387 to float
	  store float %1388, float* %o, align 4
	  %1390 = fpext float %1389 to double
	  %1392 = fpext float %1391 to double
	  %1393 = fmul double 6.700000e-01, %1392
	  %1394 = fadd double 4.000000e-01, %1393
	  %1395 = fsub double %1390, %1394
	  %1396 = fptrunc double %1395 to float
	  store float %1396, float* %p, align 4
	  %1398 = fpext float %1397 to double
	  %1400 = fpext float %1399 to double
	  %1402 = fpext float %1401 to double
	  %1403 = fmul double 1.400000e-01, %1402
	  %1404 = fsub double %1400, %1403
	  %1405 = fdiv double 1.000000e+00, %1404
	  %1406 = fmul double %1398, %1405
	  %1407 = fptrunc double %1406 to float
	  store float %1407, float* %s, align 4
	  %1409 = fpext float %1408 to double
	  %1412 = fmul float %1410, %1411
	  %1413 = fpext float %1412 to double
	  %1414 = fadd double 1.000000e+00, %1413
	  %1415 = fdiv double 1.000000e+00, %1414
	  %1416 = fmul double %1409, %1415
	  %1417 = fptrunc double %1416 to float
	  store float %1417, float* %q, align 4
	  %1419 = fpext float %1418 to double
	  %1421 = call double @exp(double %1419) #5
	  %1485 = load float, float* %t, align 4
	  %1483 = load float, float* %j, align 4
	  %1477 = load float, float* %j, align 4
	  %1475 = load float, float* %j, align 4
	  %1471 = load float, float* %1470, align 4
	  %1470 = getelementptr inbounds float, float* %1469, i64 %1468
	  %1469 = load float*, float** %3, align 8
	  %1465 = load i32, i32* %z, align 4
	  %1464 = load float, float* %v, align 4
	  %1463 = load float, float* %1462, align 4
	  %1462 = getelementptr inbounds float, float* %1461, i64 %1460
	  %1461 = load float*, float** %5, align 8
	  %1457 = load i32, i32* %z, align 4
	  %1456 = getelementptr inbounds float, float* %1455, i64 %1454
	  %1455 = load float*, float** %4, align 8
	  %1451 = load i32, i32* %z, align 4
	  %1449 = load float, float* %k, align 4
	  %1448 = load float, float* %1447, align 4
	  %1447 = getelementptr inbounds float, float* %1446, i64 %1445
	  %1446 = load float*, float** %4, align 8
	  %1442 = load i32, i32* %z, align 4
	  %1441 = getelementptr inbounds float, float* %1440, i64 %1439
	  %1440 = load float*, float** %3, align 8
	  %1436 = load i32, i32* %z, align 4
	  %1434 = load float, float* %k, align 4
	  %1433 = load float, float* %1432, align 4
	  %1432 = getelementptr inbounds float, float* %1431, i64 %1430
	  %1431 = load float*, float** %3, align 8
	  %1427 = load i32, i32* %z, align 4
	  %1425 = load float, float* %k, align 4
	  %1424 = load float, float* %r, align 4
	  %1423 = fptrunc double %1421 to float
	  store float %1423, float* %r, align 4
	  %1426 = fmul float %1424, %1425
	  store float %1426, float* %k, align 4
	  %1428 = add nsw i32 616, %1427
	  %1429 = srem i32 %1428, 128
	  %1430 = sext i32 %1429 to i64
	  %1435 = fmul float %1433, %1434
	  %1437 = add nsw i32 616, %1436
	  %1438 = srem i32 %1437, 128
	  %1439 = sext i32 %1438 to i64
	  store float %1435, float* %1441, align 4
	  %1443 = add nsw i32 616, %1442
	  %1444 = srem i32 %1443, 128
	  %1445 = sext i32 %1444 to i64
	  %1450 = fmul float %1448, %1449
	  %1452 = add nsw i32 616, %1451
	  %1453 = srem i32 %1452, 128
	  %1454 = sext i32 %1453 to i64
	  store float %1450, float* %1456, align 4
	  %1458 = add nsw i32 64, %1457
	  %1459 = srem i32 %1458, 128
	  %1460 = sext i32 %1459 to i64
	  %1466 = add nsw i32 704, %1465
	  %1467 = srem i32 %1466, 128
	  %1468 = sext i32 %1467 to i64
	  %1472 = fdiv float 1.000000e+00, %1471
	  %1473 = fmul float %1464, %1472
	  %1474 = fmul float %1463, %1473
	  store float %1474, float* %j, align 4
	  %1476 = fpext float %1475 to double
	  %1478 = fpext float %1477 to double
	  %1479 = fadd double 1.000000e+00, %1478
	  %1480 = fdiv double 1.000000e+00, %1479
	  %1481 = fmul double %1476, %1480
	  %1482 = fptrunc double %1481 to float
	  store float %1482, float* %k, align 4
	  %1484 = fpext float %1483 to double
	  %1486 = fpext float %1485 to double
	  %1488 = call double @fmax(double %1484, double %1486) #6
	  %1491 = call double @log10(double %1488) #5
	  %1494 = load float, float* %g, align 4
	  %1493 = fptrunc double %1491 to float
	  store float %1493, float* %l, align 4
	  %1495 = fsub float -0.000000e+00, %1494
	  %1496 = fmul float %1495, 0x3FB393EE20000000
	  %1497 = fpext float %1496 to double
	  %1499 = call double @exp(double %1497) #5
	  %1502 = load float, float* %g, align 4
	  %1501 = fmul double 0x3FD87E2820000000, %1499
	  %1503 = fsub float -0.000000e+00, %1502
	  %1504 = fmul float %1503, 0x3F3F89BB80000000
	  %1505 = fpext float %1504 to double
	  %1507 = call double @exp(double %1505) #5
	  %1511 = load float, float* %g, align 4
	  %1509 = fmul double 0x3FE3C0EBE0000000, %1507
	  %1510 = fadd double %1501, %1509
	  %1512 = fdiv float 1.000000e+00, %1511
	  %1513 = fmul float -5.093000e+03, %1512
	  %1514 = fpext float %1513 to double
	  %1516 = call double @exp(double %1514) #5
	  %1522 = load float, float* %t, align 4
	  %1520 = load float, float* %m, align 4
	  %1518 = fadd double %1510, %1516
	  %1519 = fptrunc double %1518 to float
	  store float %1519, float* %m, align 4
	  %1521 = fpext float %1520 to double
	  %1523 = fpext float %1522 to double
	  %1525 = call double @fmax(double %1521, double %1523) #6
	  %1528 = call double @log10(double %1525) #5
	  %1565 = load float, float* %q, align 4
	  %1558 = load float, float* %s, align 4
	  %1557 = load float, float* %s, align 4
	  %1555 = load float, float* %n, align 4
	  %1548 = load float, float* %p, align 4
	  %1546 = load float, float* %o, align 4
	  %1544 = load float, float* %p, align 4
	  %1538 = load float, float* %n, align 4
	  %1536 = load float, float* %l, align 4
	  %1531 = load float, float* %n, align 4
	  %1530 = fptrunc double %1528 to float
	  store float %1530, float* %n, align 4
	  %1532 = fpext float %1531 to double
	  %1533 = fmul double 1.270000e+00, %1532
	  %1534 = fsub double 7.500000e-01, %1533
	  %1535 = fptrunc double %1534 to float
	  store float %1535, float* %o, align 4
	  %1537 = fpext float %1536 to double
	  %1539 = fpext float %1538 to double
	  %1540 = fmul double 6.700000e-01, %1539
	  %1541 = fadd double 4.000000e-01, %1540
	  %1542 = fsub double %1537, %1541
	  %1543 = fptrunc double %1542 to float
	  store float %1543, float* %p, align 4
	  %1545 = fpext float %1544 to double
	  %1547 = fpext float %1546 to double
	  %1549 = fpext float %1548 to double
	  %1550 = fmul double 1.400000e-01, %1549
	  %1551 = fsub double %1547, %1550
	  %1552 = fdiv double 1.000000e+00, %1551
	  %1553 = fmul double %1545, %1552
	  %1554 = fptrunc double %1553 to float
	  store float %1554, float* %s, align 4
	  %1556 = fpext float %1555 to double
	  %1559 = fmul float %1557, %1558
	  %1560 = fpext float %1559 to double
	  %1561 = fadd double 1.000000e+00, %1560
	  %1562 = fdiv double 1.000000e+00, %1561
	  %1563 = fmul double %1556, %1562
	  %1564 = fptrunc double %1563 to float
	  store float %1564, float* %q, align 4
	  %1566 = fpext float %1565 to double
	  %1568 = call double @exp(double %1566) #5
	  %1632 = load float, float* %t, align 4
	  %1630 = load float, float* %j, align 4
	  %1624 = load float, float* %j, align 4
	  %1622 = load float, float* %j, align 4
	  %1618 = load float, float* %1617, align 4
	  %1617 = getelementptr inbounds float, float* %1616, i64 %1615
	  %1616 = load float*, float** %3, align 8
	  %1612 = load i32, i32* %z, align 4
	  %1611 = load float, float* %v, align 4
	  %1610 = load float, float* %1609, align 4
	  %1609 = getelementptr inbounds float, float* %1608, i64 %1607
	  %1608 = load float*, float** %5, align 8
	  %1604 = load i32, i32* %z, align 4
	  %1603 = getelementptr inbounds float, float* %1602, i64 %1601
	  %1602 = load float*, float** %4, align 8
	  %1598 = load i32, i32* %z, align 4
	  %1596 = load float, float* %k, align 4
	  %1595 = load float, float* %1594, align 4
	  %1594 = getelementptr inbounds float, float* %1593, i64 %1592
	  %1593 = load float*, float** %4, align 8
	  %1589 = load i32, i32* %z, align 4
	  %1588 = getelementptr inbounds float, float* %1587, i64 %1586
	  %1587 = load float*, float** %3, align 8
	  %1583 = load i32, i32* %z, align 4
	  %1581 = load float, float* %k, align 4
	  %1580 = load float, float* %1579, align 4
	  %1579 = getelementptr inbounds float, float* %1578, i64 %1577
	  %1578 = load float*, float** %3, align 8
	  %1574 = load i32, i32* %z, align 4
	  %1572 = load float, float* %k, align 4
	  %1571 = load float, float* %r, align 4
	  %1570 = fptrunc double %1568 to float
	  store float %1570, float* %r, align 4
	  %1573 = fmul float %1571, %1572
	  store float %1573, float* %k, align 4
	  %1575 = add nsw i32 704, %1574
	  %1576 = srem i32 %1575, 128
	  %1577 = sext i32 %1576 to i64
	  %1582 = fmul float %1580, %1581
	  %1584 = add nsw i32 704, %1583
	  %1585 = srem i32 %1584, 128
	  %1586 = sext i32 %1585 to i64
	  store float %1582, float* %1588, align 4
	  %1590 = add nsw i32 704, %1589
	  %1591 = srem i32 %1590, 128
	  %1592 = sext i32 %1591 to i64
	  %1597 = fmul float %1595, %1596
	  %1599 = add nsw i32 704, %1598
	  %1600 = srem i32 %1599, 128
	  %1601 = sext i32 %1600 to i64
	  store float %1597, float* %1603, align 4
	  %1605 = add nsw i32 72, %1604
	  %1606 = srem i32 %1605, 128
	  %1607 = sext i32 %1606 to i64
	  %1613 = add nsw i32 736, %1612
	  %1614 = srem i32 %1613, 128
	  %1615 = sext i32 %1614 to i64
	  %1619 = fdiv float 1.000000e+00, %1618
	  %1620 = fmul float %1611, %1619
	  %1621 = fmul float %1610, %1620
	  store float %1621, float* %j, align 4
	  %1623 = fpext float %1622 to double
	  %1625 = fpext float %1624 to double
	  %1626 = fadd double 1.000000e+00, %1625
	  %1627 = fdiv double 1.000000e+00, %1626
	  %1628 = fmul double %1623, %1627
	  %1629 = fptrunc double %1628 to float
	  store float %1629, float* %k, align 4
	  %1631 = fpext float %1630 to double
	  %1633 = fpext float %1632 to double
	  %1635 = call double @fmax(double %1631, double %1633) #6
	  %1638 = call double @log10(double %1635) #5
	  %1641 = load float, float* %g, align 4
	  %1640 = fptrunc double %1638 to float
	  store float %1640, float* %l, align 4
	  %1642 = fsub float -0.000000e+00, %1641
	  %1643 = fmul float %1642, 0x3F7B203640000000
	  %1644 = fpext float %1643 to double
	  %1646 = call double @exp(double %1644) #5
	  %1649 = load float, float* %g, align 4
	  %1648 = fmul double 0x3FDDEB8520000000, %1646
	  %1650 = fsub float -0.000000e+00, %1649
	  %1651 = fmul float %1650, 0x3F4F9182C0000000
	  %1652 = fpext float %1651 to double
	  %1654 = call double @exp(double %1652) #5
	  %1658 = load float, float* %g, align 4
	  %1656 = fmul double 0x3FE10A3D80000000, %1654
	  %1657 = fadd double %1648, %1656
	  %1659 = fdiv float 1.000000e+00, %1658
	  %1660 = fmul float -4.970000e+03, %1659
	  %1661 = fpext float %1660 to double
	  %1663 = call double @exp(double %1661) #5
	  %1669 = load float, float* %t, align 4
	  %1667 = load float, float* %m, align 4
	  %1665 = fadd double %1657, %1663
	  %1666 = fptrunc double %1665 to float
	  store float %1666, float* %m, align 4
	  %1668 = fpext float %1667 to double
	  %1670 = fpext float %1669 to double
	  %1672 = call double @fmax(double %1668, double %1670) #6
	  %1675 = call double @log10(double %1672) #5
	  %1712 = load float, float* %q, align 4
	  %1705 = load float, float* %s, align 4
	  %1704 = load float, float* %s, align 4
	  %1702 = load float, float* %n, align 4
	  %1695 = load float, float* %p, align 4
	  %1693 = load float, float* %o, align 4
	  %1691 = load float, float* %p, align 4
	  %1685 = load float, float* %n, align 4
	  %1683 = load float, float* %l, align 4
	  %1678 = load float, float* %n, align 4
	  %1677 = fptrunc double %1675 to float
	  store float %1677, float* %n, align 4
	  %1679 = fpext float %1678 to double
	  %1680 = fmul double 1.270000e+00, %1679
	  %1681 = fsub double 7.500000e-01, %1680
	  %1682 = fptrunc double %1681 to float
	  store float %1682, float* %o, align 4
	  %1684 = fpext float %1683 to double
	  %1686 = fpext float %1685 to double
	  %1687 = fmul double 6.700000e-01, %1686
	  %1688 = fadd double 4.000000e-01, %1687
	  %1689 = fsub double %1684, %1688
	  %1690 = fptrunc double %1689 to float
	  store float %1690, float* %p, align 4
	  %1692 = fpext float %1691 to double
	  %1694 = fpext float %1693 to double
	  %1696 = fpext float %1695 to double
	  %1697 = fmul double 1.400000e-01, %1696
	  %1698 = fsub double %1694, %1697
	  %1699 = fdiv double 1.000000e+00, %1698
	  %1700 = fmul double %1692, %1699
	  %1701 = fptrunc double %1700 to float
	  store float %1701, float* %s, align 4
	  %1703 = fpext float %1702 to double
	  %1706 = fmul float %1704, %1705
	  %1707 = fpext float %1706 to double
	  %1708 = fadd double 1.000000e+00, %1707
	  %1709 = fdiv double 1.000000e+00, %1708
	  %1710 = fmul double %1703, %1709
	  %1711 = fptrunc double %1710 to float
	  store float %1711, float* %q, align 4
	  %1713 = fpext float %1712 to double
	  %1715 = call double @exp(double %1713) #5
	  %1835 = load float, float* %t, align 4
	  %1833 = load float, float* %j, align 4
	  %1827 = load float, float* %j, align 4
	  %1825 = load float, float* %j, align 4
	  %1821 = load float, float* %1820, align 4
	  %1820 = getelementptr inbounds float, float* %1819, i64 %1818
	  %1819 = load float*, float** %3, align 8
	  %1815 = load i32, i32* %z, align 4
	  %1814 = load float, float* %v, align 4
	  %1813 = load float, float* %1812, align 4
	  %1812 = getelementptr inbounds float, float* %1811, i64 %1810
	  %1811 = load float*, float** %5, align 8
	  %1807 = load i32, i32* %z, align 4
	  %1806 = getelementptr inbounds float, float* %1805, i64 %1804
	  %1805 = load float*, float** %4, align 8
	  %1801 = load i32, i32* %z, align 4
	  %1799 = load float, float* %k, align 4
	  %1798 = load float, float* %1797, align 4
	  %1797 = getelementptr inbounds float, float* %1796, i64 %1795
	  %1796 = load float*, float** %4, align 8
	  %1792 = load i32, i32* %z, align 4
	  %1791 = getelementptr inbounds float, float* %1790, i64 %1789
	  %1790 = load float*, float** %3, align 8
	  %1786 = load i32, i32* %z, align 4
	  %1784 = load float, float* %k, align 4
	  %1783 = load float, float* %1782, align 4
	  %1782 = getelementptr inbounds float, float* %1781, i64 %1780
	  %1781 = load float*, float** %3, align 8
	  %1777 = load i32, i32* %z, align 4
	  %1771 = load float, float* %j, align 4
	  %1769 = load float, float* %j, align 4
	  %1765 = load float, float* %1764, align 4
	  %1764 = getelementptr inbounds float, float* %1763, i64 %1762
	  %1763 = load float*, float** %3, align 8
	  %1759 = load i32, i32* %z, align 4
	  %1758 = load float, float* %w, align 4
	  %1757 = load float, float* %1756, align 4
	  %1756 = getelementptr inbounds float, float* %1755, i64 %1754
	  %1755 = load float*, float** %5, align 8
	  %1751 = load i32, i32* %z, align 4
	  %1750 = getelementptr inbounds float, float* %1749, i64 %1748
	  %1749 = load float*, float** %4, align 8
	  %1745 = load i32, i32* %z, align 4
	  %1743 = load float, float* %k, align 4
	  %1742 = load float, float* %1741, align 4
	  %1741 = getelementptr inbounds float, float* %1740, i64 %1739
	  %1740 = load float*, float** %4, align 8
	  %1736 = load i32, i32* %z, align 4
	  %1735 = getelementptr inbounds float, float* %1734, i64 %1733
	  %1734 = load float*, float** %3, align 8
	  %1730 = load i32, i32* %z, align 4
	  %1728 = load float, float* %k, align 4
	  %1727 = load float, float* %1726, align 4
	  %1726 = getelementptr inbounds float, float* %1725, i64 %1724
	  %1725 = load float*, float** %3, align 8
	  %1721 = load i32, i32* %z, align 4
	  %1719 = load float, float* %k, align 4
	  %1718 = load float, float* %r, align 4
	  %1717 = fptrunc double %1715 to float
	  store float %1717, float* %r, align 4
	  %1720 = fmul float %1718, %1719
	  store float %1720, float* %k, align 4
	  %1722 = add nsw i32 736, %1721
	  %1723 = srem i32 %1722, 128
	  %1724 = sext i32 %1723 to i64
	  %1729 = fmul float %1727, %1728
	  %1731 = add nsw i32 736, %1730
	  %1732 = srem i32 %1731, 128
	  %1733 = sext i32 %1732 to i64
	  store float %1729, float* %1735, align 4
	  %1737 = add nsw i32 736, %1736
	  %1738 = srem i32 %1737, 128
	  %1739 = sext i32 %1738 to i64
	  %1744 = fmul float %1742, %1743
	  %1746 = add nsw i32 736, %1745
	  %1747 = srem i32 %1746, 128
	  %1748 = sext i32 %1747 to i64
	  store float %1744, float* %1750, align 4
	  %1752 = add nsw i32 80, %1751
	  %1753 = srem i32 %1752, 128
	  %1754 = sext i32 %1753 to i64
	  %1760 = add nsw i32 904, %1759
	  %1761 = srem i32 %1760, 128
	  %1762 = sext i32 %1761 to i64
	  %1766 = fdiv float 1.000000e+00, %1765
	  %1767 = fmul float %1758, %1766
	  %1768 = fmul float %1757, %1767
	  store float %1768, float* %j, align 4
	  %1770 = fpext float %1769 to double
	  %1772 = fpext float %1771 to double
	  %1773 = fadd double 1.000000e+00, %1772
	  %1774 = fdiv double 1.000000e+00, %1773
	  %1775 = fmul double %1770, %1774
	  %1776 = fptrunc double %1775 to float
	  store float %1776, float* %k, align 4
	  %1778 = add nsw i32 904, %1777
	  %1779 = srem i32 %1778, 128
	  %1780 = sext i32 %1779 to i64
	  %1785 = fmul float %1783, %1784
	  %1787 = add nsw i32 904, %1786
	  %1788 = srem i32 %1787, 128
	  %1789 = sext i32 %1788 to i64
	  store float %1785, float* %1791, align 4
	  %1793 = add nsw i32 904, %1792
	  %1794 = srem i32 %1793, 128
	  %1795 = sext i32 %1794 to i64
	  %1800 = fmul float %1798, %1799
	  %1802 = add nsw i32 904, %1801
	  %1803 = srem i32 %1802, 128
	  %1804 = sext i32 %1803 to i64
	  store float %1800, float* %1806, align 4
	  %1808 = add nsw i32 88, %1807
	  %1809 = srem i32 %1808, 128
	  %1810 = sext i32 %1809 to i64
	  %1816 = add nsw i32 912, %1815
	  %1817 = srem i32 %1816, 128
	  %1818 = sext i32 %1817 to i64
	  %1822 = fdiv float 1.000000e+00, %1821
	  %1823 = fmul float %1814, %1822
	  %1824 = fmul float %1813, %1823
	  store float %1824, float* %j, align 4
	  %1826 = fpext float %1825 to double
	  %1828 = fpext float %1827 to double
	  %1829 = fadd double 1.000000e+00, %1828
	  %1830 = fdiv double 1.000000e+00, %1829
	  %1831 = fmul double %1826, %1830
	  %1832 = fptrunc double %1831 to float
	  store float %1832, float* %k, align 4
	  %1834 = fpext float %1833 to double
	  %1836 = fpext float %1835 to double
	  %1838 = call double @fmax(double %1834, double %1836) #6
	  %1841 = call double @log10(double %1838) #5
	  %1844 = load float, float* %g, align 4
	  %1843 = fptrunc double %1841 to float
	  store float %1843, float* %l, align 4
	  %1845 = fsub float -0.000000e+00, %1844
	  %1846 = fmul float %1845, 0x3F285898C0000000
	  %1847 = fpext float %1846 to double
	  %1849 = call double @exp(double %1847) #5
	  %1852 = load float, float* %g, align 4
	  %1851 = fmul double 0xBFEF694460000000, %1849
	  %1853 = fsub float -0.000000e+00, %1852
	  %1854 = fmul float %1853, 0x3FCDD08900000000
	  %1855 = fpext float %1854 to double
	  %1857 = call double @exp(double %1855) #5
	  %1861 = load float, float* %g, align 4
	  %1859 = fmul double 0x3FFFB4A240000000, %1857
	  %1860 = fadd double %1851, %1859
	  %1862 = fdiv float 1.000000e+00, %1861
	  %1863 = fmul float 0x3FB45A1CA0000000, %1862
	  %1864 = fpext float %1863 to double
	  %1866 = call double @exp(double %1864) #5
	  %1872 = load float, float* %t, align 4
	  %1870 = load float, float* %m, align 4
	  %1868 = fadd double %1860, %1866
	  %1869 = fptrunc double %1868 to float
	  store float %1869, float* %m, align 4
	  %1871 = fpext float %1870 to double
	  %1873 = fpext float %1872 to double
	  %1875 = call double @fmax(double %1871, double %1873) #6
	  %1878 = call double @log10(double %1875) #5
	  %1915 = load float, float* %q, align 4
	  %1908 = load float, float* %s, align 4
	  %1907 = load float, float* %s, align 4
	  %1905 = load float, float* %n, align 4
	  %1898 = load float, float* %p, align 4
	  %1896 = load float, float* %o, align 4
	  %1894 = load float, float* %p, align 4
	  %1888 = load float, float* %n, align 4
	  %1886 = load float, float* %l, align 4
	  %1881 = load float, float* %n, align 4
	  %1880 = fptrunc double %1878 to float
	  store float %1880, float* %n, align 4
	  %1882 = fpext float %1881 to double
	  %1883 = fmul double 1.270000e+00, %1882
	  %1884 = fsub double 7.500000e-01, %1883
	  %1885 = fptrunc double %1884 to float
	  store float %1885, float* %o, align 4
	  %1887 = fpext float %1886 to double
	  %1889 = fpext float %1888 to double
	  %1890 = fmul double 6.700000e-01, %1889
	  %1891 = fadd double 4.000000e-01, %1890
	  %1892 = fsub double %1887, %1891
	  %1893 = fptrunc double %1892 to float
	  store float %1893, float* %p, align 4
	  %1895 = fpext float %1894 to double
	  %1897 = fpext float %1896 to double
	  %1899 = fpext float %1898 to double
	  %1900 = fmul double 1.400000e-01, %1899
	  %1901 = fsub double %1897, %1900
	  %1902 = fdiv double 1.000000e+00, %1901
	  %1903 = fmul double %1895, %1902
	  %1904 = fptrunc double %1903 to float
	  store float %1904, float* %s, align 4
	  %1906 = fpext float %1905 to double
	  %1909 = fmul float %1907, %1908
	  %1910 = fpext float %1909 to double
	  %1911 = fadd double 1.000000e+00, %1910
	  %1912 = fdiv double 1.000000e+00, %1911
	  %1913 = fmul double %1906, %1912
	  %1914 = fptrunc double %1913 to float
	  store float %1914, float* %q, align 4
	  %1916 = fpext float %1915 to double
	  %1918 = call double @exp(double %1916) #5
	  %1953 = getelementptr inbounds float, float* %1952, i64 %1951
	  %1952 = load float*, float** %4, align 8
	  %1948 = load i32, i32* %z, align 4
	  %1946 = load float, float* %k, align 4
	  %1945 = load float, float* %1944, align 4
	  %1944 = getelementptr inbounds float, float* %1943, i64 %1942
	  %1943 = load float*, float** %4, align 8
	  %1939 = load i32, i32* %z, align 4
	  %1938 = getelementptr inbounds float, float* %1937, i64 %1936
	  %1937 = load float*, float** %3, align 8
	  %1933 = load i32, i32* %z, align 4
	  %1931 = load float, float* %k, align 4
	  %1930 = load float, float* %1929, align 4
	  %1929 = getelementptr inbounds float, float* %1928, i64 %1927
	  %1928 = load float*, float** %3, align 8
	  %1924 = load i32, i32* %z, align 4
	  %1922 = load float, float* %k, align 4
	  %1921 = load float, float* %r, align 4
	  %1920 = fptrunc double %1918 to float
	  store float %1920, float* %r, align 4
	  %1923 = fmul float %1921, %1922
	  store float %1923, float* %k, align 4
	  %1925 = add nsw i32 912, %1924
	  %1926 = srem i32 %1925, 128
	  %1927 = sext i32 %1926 to i64
	  %1932 = fmul float %1930, %1931
	  %1934 = add nsw i32 912, %1933
	  %1935 = srem i32 %1934, 128
	  %1936 = sext i32 %1935 to i64
	  store float %1932, float* %1938, align 4
	  %1940 = add nsw i32 912, %1939
	  %1941 = srem i32 %1940, 128
	  %1942 = sext i32 %1941 to i64
	  %1947 = fmul float %1945, %1946
	  %1949 = add nsw i32 912, %1948
	  %1950 = srem i32 %1949, 128
	  %1951 = sext i32 %1950 to i64
	  store float %1947, float* %1953, align 4
